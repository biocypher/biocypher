{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"","text":""},{"location":"#overview","title":"Overview","text":"<p>Hot Topics</p> <p>BioCypher is the simplest way to create an AI-enabled knowledge graph for biomedical (or other) tasks. See BioCypher + LLMs for more information.</p> <p>We have also recently published a perspective on connecting knowledge and machine learning to enable causal reasoning in biomedicine, with a particular focus on the currently emerging \"foundation models\". You can read it here on arXiv.</p>"},{"location":"#democratising-knowledge-graphs","title":"Democratising Knowledge Graphs","text":"<p>Building a biomedical knowledge graph often takes months or even years. But what if you could do it in just weeks or days?</p> <p>We created BioCypher to revolutionize the process\u2014making it easier than ever while maintaining flexibility and transparency.</p> <p>At its core, BioCypher is designed around the principle of threefold modularity:</p> <ol> <li>Modular data sources \u2013 Seamlessly integrate diverse biomedical datasets.</li> <li>Modular ontology structures \u2013 Define flexible, structured knowledge representations.</li> <li>Modular output formats \u2013 Adapt results to various applications and tools.</li> </ol> <p>This modular approach maximizes flexibility and reusability, empowering the biomedical community to accelerate research while streamlining efforts.</p>"},{"location":"#new-to-knowledge-graphs","title":"New to Knowledge Graphs?","text":"<p>If you\u2019re new to knowledge graphs and want to familiarise with the concepts that drive BioCypher, we recommend to check out the graphical abstract below and read our paper (self-archived version here on Zenodo, online version at this https link)!</p>"},{"location":"#general-abstract","title":"General Abstract","text":"<p> BioCypher simplifies biomedical data integration with reusable adapters, minimizing redundancy and ensuring consistency through ontologies. Its intuitive low-code setup enables effortless creation and sharing of reproducible knowledge graphs.</p>"},{"location":"#additional-resources","title":"Additional Resources","text":"<ul> <li> Find us on GitHub</li> <li> Join our Zulip channel</li> </ul>"},{"location":"installation/","title":"Installation guide","text":"<p>Before diving into developing wonderful use cases with BioCypher, we strongly recommend installing a few prerequisites to ensure a smooth experience. These prerequisites are:</p> <ol> <li>Python 3 (version &gt;= 3.10)<ul> <li>Install Python 3</li> </ul> </li> <li>Poetry (Python packaging and dependency manager)<ul> <li>Install Poetry</li> </ul> </li> <li>git (version control manager)<ul> <li>Install git</li> </ul> </li> <li>Docker (containerization technology) [optional]<ul> <li>Install Docker</li> </ul> </li> </ol> <p>Tip</p> <p>If any of those pre-requisites is missing, please follow the installation guide in each resource before continue.</p>"},{"location":"installation/#checking-prerequisites","title":"Checking prerequisites","text":"<ol> <li>Ensure that your Python version is 3.10 or higher. To check your current Python version, run the following command in your terminal, Command Prompt, or PowerShell:    <pre><code>python --version\n</code></pre> 2.Ensure you have <code>poetry</code> installed in your machine:    <pre><code>poetry --version\n</code></pre></li> <li>Ensure you have <code>git</code> installed in your machine:    <pre><code>git --version\n</code></pre></li> </ol>"},{"location":"installation/#option-1-use-a-pre-configured-project-with-biocypher","title":"Option 1. Use a pre-configured project with BioCypher","text":"<p>The easiest way to start using BioCypher is with a pre-configured project that includes all the essential code, dependencies, environment settings, and the BioCypher framework. This setup allows you to focus solely on implementing your use case, with minimal modifications to a few existing files, depending on your needs. If this approach suits you, follow the instructions below to get started.</p> GitHub repo from template (recommended)Cloning directly the project template <p>Step  1: Go to Biocypher project template repository, click on \"Use this template\", then click on \"Create a new repository\".</p> <p></p> <p>Step 2: Complete the information such as owner, name, description and visibility of your repository.</p> <p></p> <p>Step 3:  Now, you can clone the repository and navigate into it. For our example, the repository is called \"my-knowledge-graph-project\". The user hhrobertkoch is a fictional user in honor to Robert Koch, replace this user with your own.</p> <pre><code>git clone https://github.com/hhrobertkoch/my-knowledge-graph-project.git\ncd my-project\n</code></pre> <p>Step 4: Open the <code>pyproject.toml</code> file, change the following sections, and do not forget save changes.</p> <ul> <li><code>name</code>: replace the default project's name (<code>biocypher-project-template</code>) with the name you have defined earlier (in our case <code>my-knowledge-graph-project</code>).</li> <li><code>description</code>: change the default description for a meaningful one based on your use case.</li> </ul> <p>Step 4: Install the dependencies using Poetry.</p> <pre><code>poetry install --no-root\n</code></pre> <p>Step 4: Run the script <code>create_knowledge_graph.py</code></p> <pre><code>poetry run python create_knowledge_graph.py\n</code></pre> <p>Step  1: Clone the project template repository, rename it, and navigate to the project folder.</p> <p>For this example, we are going to name the project as <code>my-knowledge-graph-project</code>, but you can name it as you want.</p> <pre><code>git clone https://github.com/biocypher/project-template.git\nmv project-template my-knowledge-graph-project\ncd my-project\n</code></pre> <p>Step  2: Make the repository your own repository.</p> <pre><code>rm -rf .git\ngit init\ngit add .\ngit commit -m \"Initial commit\"\n# (you can add your remote repository here)\n</code></pre> <p>Step 3: Open the <code>pyproject.toml</code> file, change the following sections, and do not forget save changes.</p> <ul> <li><code>name</code>: replace the default project's name (<code>biocypher-project-template</code>) with the name you have defined earlier (in our case <code>my-knowledge-graph-project</code>).</li> <li><code>description</code>: change the default description for a meaningful one based on your use case.</li> </ul> <p>Step 4: Install the dependencies using Poetry.</p> <pre><code>poetry install --no-root\n</code></pre> <p>Step 4: Run the script <code>create_knowledge_graph.py</code></p> <pre><code>poetry run python create_knowledge_graph.py\n</code></pre>"},{"location":"installation/#docker-all-batteries-included","title":"Docker (all batteries included!)","text":"<p>Play with your data in Neo4j with this Docker container</p> <p>The project template includes a Docker compose workflow that allows to:</p> <ol> <li>Create an example database using BioCypher.</li> <li>Load the data into a dockerized Neo4j instance automatically.</li> </ol> <p>Once you have created your project using any of the previous options, please follow the steps below:</p> <p>Step 1: Start a single detached Docker container running a Neo4j instance, which contains the knowledge graph built by BioCypher as the default Neo4j database.</p> <pre><code>docker compose up -d\n</code></pre> <p>Step 2: Open the Neo4j instance in a web browser by typing the address and port: localhost:7474.</p> <p>Authentication is deactivated by default and can be modified in the <code>docker_variables.env</code> file (in which case you need to provide the .env file to the deploy stage of the <code>docker-compose.yml</code>).</p>"},{"location":"installation/#docker-workflow","title":"Docker Workflow","text":"<p>The Docker Compose file creates three containers: build, import, and deploy. These containers share files using a Docker Volume. In the BioCypher build procedure, the <code>biocypher_docker_config.yaml</code> file is used instead of <code>biocypher_config.yaml</code>, as specified in <code>scripts/build.sh</code>.</p> <ul> <li>Containers and their functions<ul> <li>build: Installs and runs the BioCypher pipeline.</li> <li>import: installs Neo4j and executes the data import.</li> <li>deploy: deploys the Neo4j instance on localhost.</li> </ul> </li> </ul> <p>This three-stage setup strictly is not necessary for the mounting of a read-write instance of Neo4j, but is required if the purpose is to provide a read-only instance (e.g. for a web app) that is updated regularly;for an example, see the meta graph repository. The read-only setting is configured in the docker-compose.yml file(NEO4J_dbms_databases_default__to__read__only: \"false\") and is deactivated by default.</p>"},{"location":"installation/#option-2-install-from-a-package-manager","title":"Option 2. Install from a Package Manager","text":"poetry (recommended)pip <p>Note: about Poetry</p> <p>Poetry is a tool for dependency management and packaging in Python. It allows you to declare the libraries your project depends on and it will manage (install/update) them for you. Poetry offers a lockfile to ensure repeatable installs, and can build your project for distribution. For information about the installation process, you can consult here.</p> <pre><code># Create a new Poetry project, i.e. my-awesome-kg-project.\npoetry new &lt;name-of-the-project&gt;\n\n# Navigate into the recently created folder's project\ncd &lt;name-of-the-project&gt;\n\n# Install the BioCypher package with all the dependencies automatically\npoetry add biocypher\n</code></pre> <p>Note: Virtual environment and best practices</p> <p>To follow best practices in software engineering and prevent issues with your Python installation, we highly recommend installing packages in a separate virtual environment instead of directly in the base Python installation.</p> <ol> <li> <p>Create and activate a virtual environment. Replace <code>&lt;name-of-environment&gt;</code> with the name of the environment you desire, i.e. <code>biocypher_env</code></p> condavenv <pre><code># Create a conda environment with Python 3.10\nconda create --name &lt;name-of-environment&gt; python=3.10\n\n# Activate the new created environment\nconda activate &lt;name-of-environment&gt;\n</code></pre> <pre><code># Create a virtualenv environment\npython3 -m venv &lt;name-of-environment&gt;\n\n# Activate the new created environment\nsource ./&lt;name-of-environment&gt;/bin/activate\n</code></pre> </li> <li> <p>Install BioCypher package from <code>pip</code>. Type the following command to install BioCypher package. Note: do not forget to activate a virtual environment before do it.</p> <pre><code>pip install biocypher\n</code></pre> </li> </ol>"},{"location":"installation/#for-developers","title":"For Developers","text":"<p>If you want to directly install BioCypher, here are the steps (requires Poetry):</p> <p>Execute in bash<pre><code>git clone https://github.com/biocypher/biocypher\ncd BioCypher\npoetry install\n</code></pre> Poetry creates a virtual environment for you (starting with <code>biocypher-</code>; alternatively you can name it yourself) and installs all dependencies.</p> <p>If you want to run the tests that use a local Neo4j or PostgreSQL DBMS (database management system) instance:</p> <ul> <li> <p>Make sure that you have a Neo4j instance with the APOC plugin installed and a database named <code>test</code> running on standard bolt port <code>7687</code></p> </li> <li> <p>A PostgreSQL instance with the psql command line tool should be installed locally and running on standard port <code>5432</code></p> </li> <li> <p>Activate the virtual environment by running <code>poetry shell</code> and then run the tests by running % pytest in the root directory of the repository with the command line argument <code>--password=&lt;your DBMS password&gt;</code>.</p> </li> </ul> <p>Once this is set up, you can go through the tutorial or use it in your project as a local dependency.</p>"},{"location":"biocypher-project/biochatter-integration/","title":"BioChatter (LLM on Knowledge Graphs)","text":""},{"location":"biocypher-project/biochatter-integration/#connect-your-knowledge-graph-to-large-language-models","title":"Connect your Knowledge Graph to Large Language Models","text":"<p>To facilitate the use of knowledge graphs in downstream tasks, we have developed a framework to connect knowledge graphs to large language models, this framework is called BioChatter</p> <p>biochatter is a Python package implementing a generic backend library for the connection of biomedical applications to conversational AI. We describe the framework in this preprint. BioChatter is part of the BioCypher ecosystem, connecting natively to BioCypher knowledge graphs.</p>"},{"location":"biocypher-project/biochatter-integration/#biocypher-ecosystem-biocypher-biochatter","title":"BioCypher Ecosystem (BioCypher + BioChatter)","text":"<ul> <li> <p> BioChatter Light Web App</p> <p> To BioChatter Light</p> </li> <li> <p> BioChatter Repository</p> <p> To BioChatter Repository</p> </li> </ul>"},{"location":"biocypher-project/design-philosophy/","title":"BioCypher design philosophy","text":"<p>At its core, BioCypher is designed around the principle of threefold modularity:</p> <ol> <li>Modular data sources \u2013 Seamlessly integrate diverse biomedical datasets.</li> <li>Modular ontology structures \u2013 Define flexible, structured knowledge representations.</li> <li>Modular output formats \u2013 Adapt results to various applications and tools.</li> </ol>"},{"location":"biocypher-project/design-philosophy/#design-principles","title":"Design Principles","text":""},{"location":"biocypher-project/design-philosophy/#1-modular-data-sources","title":"1. Modular data sources","text":""},{"location":"biocypher-project/design-philosophy/#resources","title":"Resources","text":"<p>Resources are diverse data inputs and sources that feed into the knowledge graph through \"adaptors\". A Resource could be a file, a list of files, an API request, or a list of API requests. Biocypher can download resources from a given URL, cache them, and manage their lifecycle.</p>"},{"location":"biocypher-project/design-philosophy/#adaptors","title":"Adaptors","text":"<p>BioCypher is a modular framework, with the main purpose of avoiding redundant maintenance work for maintainers of secondary resources and end users alike. To achieve this, we use a collection of reusable \"adapters\" for the different sources of biomedical knowledge as well as for different ontologies.</p>"},{"location":"biocypher-project/design-philosophy/#2-modular-ontology-structures","title":"2. Modular ontology structures","text":""},{"location":"biocypher-project/design-philosophy/#ontologies","title":"Ontologies","text":"<p>An ontology is a formal, hierarchical representation of knowledge within a specific domain, organizing concepts and their relationships. It structures concepts into subclasses of more general categories, such as a wardrobe being a subclass of furniture. BioCypher requires a certain amount of knowledge about ontologies and how to use them. We try to make dealing with ontologies as easy as possible, but some basic understanding is required.</p>"},{"location":"biocypher-project/design-philosophy/#3-modular-output-formats","title":"3. Modular output formats","text":""},{"location":"biocypher-project/design-philosophy/#outputs","title":"Outputs","text":"<p>Initially focused on Neo4j due to OmniPath's migration, BioCypher now supports multiple output formats, including RDF, SQL, ArangoDB, CSV, PostgreSQL, SQLite, and NetworkX, specified via the dbms parameter in the <code>biocypher_config.yaml</code> file. Users can choose between online mode or offline mode.</p>"},{"location":"biocypher-project/design-philosophy/#configuration","title":"Configuration","text":"<p>Configuration in BioCypher involves setting up and customizing the system to meet specific needs. BioCypher provides default configuration parameters, which can be overridden by creating a <code>biocypher_config.yaml</code> file in your project's root or config directory,specifying the parameters you wish to change.</p>"},{"location":"biocypher-project/project/","title":"BioCypher Project","text":""},{"location":"biocypher-project/project/#our-mission","title":"Our Mission","text":"<p>We aim to enable access to versatile and powerful knowledge graphs for as many researchers as possible. Making biomedical knowledge \u201ctheir own\u201d is often a privilege of the companies and groups that can afford individuals or teams working on knowledge representation in a dedicated manner. With BioCypher, we aim to change that. Creating a knowledge graph should be \u201cas simple as possible, but not any simpler.\u201d To achieve this, we have developed a framework that facilitates the creation of knowledge graphs that are informed by the latest developments in the field of biomedical knowledge representation. However, to make this framework truly accessible and comprehensive, we need the input of the biomedical community. We are therefore inviting you to join us in this endeavour!</p>"},{"location":"biocypher-project/project/#our-vision","title":"Our Vision","text":"<p>The machine learning models we train are only as good as the data they are trained on. However, most developments today still rely on manually engineered and non-reproducible data processing. We envision a future where the creation of knowledge graphs is as easy as running a script, enabling researchers to build reliable knowledge representations with up-to-date information. We believe that making the knowledge representation process more agile and lifting it to the same level of attention as the process of algorithm development will lead to more robust and reliable machine learning models. We are convinced that this will be a crucial step towards the democratization of AI in biomedicine and beyond.</p>"},{"location":"biocypher-project/project/#timeline","title":"Timeline","text":"<p>Launch</p> 2022-Q1<p>First implementation.</p> <p>New features</p> 2022-Q2<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit.</p> <p>More features!</p> 2022-Q3<p>Lorem ipsum dolor sit amet.</p> <p>Bugs!</p> 2022-Q4<p>Lorem ipsum dolor sit amet.</p>"},{"location":"biocypher-project/publications/","title":"Publications","text":""},{"location":"biocypher-project/publications/#MCAFM_2024","title":"[2024] Molecular Causality in the Advent of Foundation Models","text":"<p>Publisher: arXiv</p> Click to read the paper! Preview <p></p>"},{"location":"biocypher-project/publications/#2023-democratizing-knowledge-representation-with-biocypher","title":"[2023] Democratizing knowledge representation with BioCypher","text":"<p>Publisher: Nature Biotechnology</p> Click to read the paper! Preview <p></p>"},{"location":"biocypher-project/sponsors/","title":"Sponsors","text":""},{"location":"biocypher-project/sponsors/#institutional-partners","title":"Institutional partners","text":""},{"location":"biocypher-project/sponsors/#become-a-sponsor","title":"Become a sponsor","text":"<p>As a free and open source project, Biocypher relies on the support of the community of users for its development. If you work for an organization that uses and benefits from BioCypher, please consider supporting BioCypher.</p>"},{"location":"biocypher-project/use-cases/","title":"Use cases","text":""},{"location":"biocypher-project/use-cases/#how-researchers-are-leveraging-biocypher","title":"How Researchers Are Leveraging BioCypher","text":"<p>BioCypher has been instrumental in advancing research around Knowledge Graphs. Teams have primarily used BioCypher for:</p> <ol> <li> <p>Creation and maintenance of knowledge repositories (\"storage\"): Ensuring structured, scalable, and easily accessible data storage.</p> </li> <li> <p>Project-specific knowledge graph creation (\"analysis\"): Facilitating streamlined data integration and insightful analysis tailored to research needs.</p> </li> </ol> <p>We are excited to showcase some real-world use cases where BioCypher has made a significant impact. If BioCypher has helped your research, we\u2019d love to hear about it\u2014your use case could be the next one featured here!</p>"},{"location":"biocypher-project/use-cases/#a-knowledge-graph-for-impact-of-genomic-variation-on-function-igvf","title":"A Knowledge Graph for Impact of Genomic Variation on Function (IGVF)","text":"<ul> <li> <p>Impact of Genomic Variation on Function (IGVF)</p> <p>Description: The impact of Genomic Variation on Function (IGVF) project aims to provide a comprenhensive and integrated view of the impact of genomic variation on human health and disease.</p> <p>Resources: The BioCypher pipeline used to build the knowledge graph uses several adapters for genetics data sources; an overview is available in our meta-graph and on the GitHub Components Board (pipelines column). The pipeline boasts a Docker Compose workflow that builds the graph and the API (using tRPC), and is available on GitHub.</p> <p> To the project</p> </li> </ul> <p>Testimonial</p> <p>\"Our project, Impact of Genomic Variation on Function (IGVF, https://igvf.org), is building a massive biological knowledge graph to attempt to link human variation and disease with genomic datasets at the single-cell level. We are creating a user-facing API (and eventually UI) that will access this graph. BioCypher, which acts as an intermediary between Biolink and graph databases (we are using ArangoDB) has been instrumental in helping us design the schema and move our project forward. Specifically, it provides a framework we can use to parse the dozens of data files and formats into a Biolink-inspired schema\".</p> <ul> <li>Ben Hitz, Director of Genomics Data Resources, Project Manager ENCODE, Stanford University</li> </ul>"},{"location":"biocypher-project/use-cases/#drug-repurposing-with-crossbar","title":"Drug Repurposing with CROssBAR","text":"<ul> <li> <p>CROssBAR</p> <p>Description: CROssBAR is a biomedical data integration and representation project. CROssBAR knowledge graphs incorporate relevant genes-proteins, molecular interactions, pathways, phenotypes, diseases, as well as known/predicted drugs and bioactive compounds, and they are constructred on-the-fly based on simple non-programmatic user queries.</p> <p>Resources: Using BioCypher, CROssBAR v2 will be a flexible property graph database comprised of single input adapters for each data source. As above, you can see its current state in the meta-graph and on the GitHub Components Board (pipelines column).</p> <p> To the project</p> </li> </ul> <p>Testimonial</p> <p>\"We built CROssBAR v1 on NoSQL since property graph databases were quite new at the time and there was no framework to help us establish the system. We used an available NoSQL solution to house different layers of biological/biomedical data as independent collections. CROssBAR\u2019s \u201csmall-scale knowledge graph (KG) construction module\u201d queries each collection separately, collects the data, and merges the data points according to their mappings (which are held in the database as well, as cross-references), eliminates redundancy, queries each and every collection again with the entries retrieved in the previous step, and repeats all subsequent steps. Given that user queries can start with a single or multiple genes/proteins, compounds/drugs, diseases, phenotypes, pathways, or any combination of those, this procedure gets extremely complicated, requiring an average of 64 NoSQL queries to construct one single user-specific KG. The total number of lines of code required for this procedure alone is around 8000. This task could have been achieved significantly faster and more efficiently if we had had BioCypher five years ago\".</p> <ul> <li>Tunca Do\u011fan, Department of Computer Engineering and Artificial Intelligence Engineering, Hacettepe University and Protein Function Development Team (UniProt database), European Molecular Biology Laboratory, European Bioinformatics Institute (EMBL-EBI)</li> </ul>"},{"location":"biocypher-project/use-cases/#builing-a-knowledge-graph-for-contextualised-metabolic-enzymatic-interactions","title":"Builing a Knowledge Graph for Contextualised Metabolic-Enzymatic Interactions","text":"<ul> <li> <p>metalinks</p> <p>Description: The metalinks project aims to build a knowledge graph for contextualised metabolic-enzymatic interactions.</p> <p>Resources: The BioCypher pipeline used to build the knowledge graph uses several adapters, some of which overlap with the CROssBAR project, which helps synergising maintenance efforts. An overview is available in our meta-graph and on the GitHub Components Board (pipelines column).</p> <p> To the project</p> </li> </ul> <p>Testimonial</p> <p>\"In the metalinks project, we build a knowledge graph (KG) that incorporates attributes of metabolites, proteins and their interactions to ultimately study cell-cell communication. We use two types of interaction between metabolites and proteins, I) production and degradation of metabolites by enzymes and II) interaction of metabolites with protein receptors. During the KG assembly we access multiple databases that provide information in diverse formats. BioCypher takes all of these inputs, gives them a reasonable, reproducible structure, and facilitates proper versioning. The KG produced by BioCypher can be easily contextualized to biological questions aiming for specific tissues, diseases or metabolite properties, which facilitates downstream analysis and interpretability. While spending 2.5 months to create a loose collection of scripts and directories for the initial project, I was able to obtain a structured result with BioCypher within 2 weeks.\".</p> <ul> <li>Elias Farr, Institute for Computational Biomedicine, University Hospital Heidelberg</li> </ul>"},{"location":"community/","title":"Index","text":""},{"location":"community/#zulip-channel-kind-of-forum-card","title":"Zulip Channel (kind of Forum card)","text":"<ul> <li><code>#announcements</code>: Stay informed about seminars, workshops, and other events.</li> <li><code>#help</code>: Ask questions and get help related to BioCypher.</li> <li><code>#general</code>: You can introducce yourselves, discuss ideas and shere your work.</li> <li><code>#development</code>:</li> </ul> <p>Why Zulip? We chose Zulip because it is a modern, open-source alternative to Slack.It is organised by topics, which helps in keeping the discussions focused. It also is free of charge for open-source and academic projects, which means that they sponsor the cloud hosting for our channel (as for many other open-source projects) - thanks!</p>"},{"location":"community/#_1","title":"Index","text":"<ul> <li> <p>Contribution guidelines</p> </li> <li> <p>Code of Conduct</p> </li> <li> <p>Developer Guide</p> </li> </ul>"},{"location":"community/biocypher-docstring-guide/","title":"BioCypher docstring guide","text":""},{"location":"community/biocypher-docstring-guide/#about-docstrings-and-standards","title":"About docstrings and standards","text":"<p>A Python docstring is a string used to document a Python module, class, function or method, so programmers can understand what it does without having to read the details of the implementation.</p> <p>Also, it is a common practice to generate online (html) documentation automatically from docstrings. <code>Sphinx &lt;https://www.sphinx-doc.org&gt;</code>_ serves this purpose.</p> <p>The next example gives an idea of what a docstring looks like:</p> <p>.. code-block:: python</p> <pre><code>def add(num1, num2):\n    \"\"\"\n    Add up two integer numbers.\n\n    This function simply wraps the ``+`` operator, and does not\n    do anything interesting, except for illustrating what\n    the docstring of a very simple function looks like.\n\n    Parameters\n    ----------\n    num1 : int\n        First number to add.\n    num2 : int\n        Second number to add.\n\n    Returns\n    -------\n    int\n        The sum of ``num1`` and ``num2``.\n\n    See Also\n    --------\n    subtract : Subtract one integer from another.\n\n    Examples\n    --------\n    &gt;&gt;&gt; add(2, 2)\n    4\n    &gt;&gt;&gt; add(25, 0)\n    25\n    &gt;&gt;&gt; add(10, -10)\n    0\n    \"\"\"\n    return num1 + num2\n</code></pre> <p>Some standards regarding docstrings exist, which make them easier to read, and allow them be easily exported to other formats such as html or pdf.</p> <p>The first conventions every Python docstring should follow are defined in <code>PEP-257 &lt;https://www.python.org/dev/peps/pep-0257/&gt;</code>_.</p> <p>As PEP-257 is quite broad, other more specific standards also exist. In the case of biocypher, the NumPy docstring convention is followed. These conventions are explained in this document:</p> <ul> <li><code>numpydoc docstring guide &lt;https://numpydoc.readthedocs.io/en/latest/format.html&gt;</code>_</li> </ul> <p>numpydoc is a Sphinx extension to support the NumPy docstring convention.</p> <p>The standard uses reStructuredText (reST). reStructuredText is a markup language that allows encoding styles in plain text files. Documentation about reStructuredText can be found in:</p> <ul> <li><code>Sphinx reStructuredText primer &lt;https://www.sphinx-doc.org/en/stable/rest.html&gt;</code>_</li> <li><code>Quick reStructuredText reference &lt;https://docutils.sourceforge.io/docs/user/rst/quickref.html&gt;</code>_</li> <li><code>Full reStructuredText specification &lt;https://docutils.sourceforge.io/docs/ref/rst/restructuredtext.html&gt;</code>_</li> </ul> <p>biocypher has some helpers for sharing docstrings between related classes, see :ref:<code>docstring.sharing</code>.</p> <p>The rest of this document will summarize all the above guidelines, and will provide additional conventions specific to the biocypher project.</p> <p>.. _docstring.tutorial:</p>"},{"location":"community/biocypher-docstring-guide/#writing-a-docstring","title":"Writing a docstring","text":"<p>.. _docstring.general:</p> <p>General rules ~~~~~~~~~~~~~</p> <p>Docstrings must be defined with three double-quotes. No blank lines should be left before or after the docstring. The text starts in the next line after the opening quotes. The closing quotes have their own line (meaning that they are not at the end of the last sentence).</p> <p>On rare occasions reST styles like bold text or italics will be used in docstrings, but is it common to have inline code, which is presented between backticks. The following are considered inline code:</p> <ul> <li>The name of a parameter</li> <li>Python code, a module, function, built-in, type, literal... (e.g. <code>os</code>,   <code>list</code>, <code>numpy.abs</code>, <code>datetime.date</code>, <code>True</code>)</li> <li>A biocypher class (in the form <code>`:class:</code>biocypher.Series```)</li> <li>A biocypher method (in the form <code>`:meth:</code>biocypher.Series.sum```)</li> <li>A biocypher function (in the form <code>`:func:</code>biocypher.to_datetime```)</li> </ul> <p>.. note::     To display only the last component of the linked class, method or     function, prefix it with <code>~</code>. For example, <code>:class:`~biocypher.Series```     will link to</code>biocypher.Series<code>but only display the last part,</code>Series<code>`     as the link text. See</code>Sphinx cross-referencing syntax     https://www.sphinx-doc.org/en/stable/domains.html#cross-referencing-syntax`_     for details.</p> <p>Good:</p> <p>.. code-block:: python</p> <pre><code>def add_values(arr):\n    \"\"\"\n    Add the values in ``arr``.\n\n    This is equivalent to Python ``sum`` of :meth:`biocypher.Series.sum`.\n\n    Some sections are omitted here for simplicity.\n    \"\"\"\n    return sum(arr)\n</code></pre> <p>Bad:</p> <p>.. code-block:: python</p> <pre><code>def func():\n\n    \"\"\"Some function.\n\n    With several mistakes in the docstring.\n\n    It has a blank like after the signature ``def func():``.\n\n    The text 'Some function' should go in the line after the\n    opening quotes of the docstring, not in the same line.\n\n    There is a blank line between the docstring and the first line\n    of code ``foo = 1``.\n\n    The closing quotes should be in the next line, not in this one.\"\"\"\n\n    foo = 1\n    bar = 2\n    return foo + bar\n</code></pre> <p>.. _docstring.short_summary:</p> <p>Section 1: short summary ~~~~~~~~~~~~~~~~~~~~~~~~</p> <p>The short summary is a single sentence that expresses what the function does in a concise way.</p> <p>The short summary must start with a capital letter, end with a dot, and fit in a single line. It needs to express what the object does without providing details. For functions and methods, the short summary must start with an infinitive verb.</p> <p>Good:</p> <p>.. code-block:: python</p> <pre><code>def astype(dtype):\n    \"\"\"\n    Cast Series type.\n\n    This section will provide further details.\n    \"\"\"\n    pass\n</code></pre> <p>Bad:</p> <p>.. code-block:: python</p> <pre><code>def astype(dtype):\n    \"\"\"\n    Casts Series type.\n\n    Verb in third-person of the present simple, should be infinitive.\n    \"\"\"\n    pass\n</code></pre> <p>.. code-block:: python</p> <pre><code>def astype(dtype):\n    \"\"\"\n    Method to cast Series type.\n\n    Does not start with verb.\n    \"\"\"\n    pass\n</code></pre> <p>.. code-block:: python</p> <pre><code>def astype(dtype):\n    \"\"\"\n    Cast Series type\n\n    Missing dot at the end.\n    \"\"\"\n    pass\n</code></pre> <p>.. code-block:: python</p> <pre><code>def astype(dtype):\n    \"\"\"\n    Cast Series type from its current type to the new type defined in\n    the parameter dtype.\n\n    Summary is too verbose and doesn't fit in a single line.\n    \"\"\"\n    pass\n</code></pre> <p>.. _docstring.extended_summary:</p> <p>Section 2: extended summary ~~~~~~~~~~~~~~~~~~~~~~~~~~~</p> <p>The extended summary provides details on what the function does. It should not go into the details of the parameters, or discuss implementation notes, which go in other sections.</p> <p>A blank line is left between the short summary and the extended summary. Every paragraph in the extended summary ends with a dot.</p> <p>The extended summary should provide details on why the function is useful and their use cases, if it is not too generic.</p> <p>.. code-block:: python</p> <pre><code>def unstack():\n    \"\"\"\n    Pivot a row index to columns.\n\n    When using a MultiIndex, a level can be pivoted so each value in\n    the index becomes a column. This is especially useful when a subindex\n    is repeated for the main index, and data is easier to visualize as a\n    pivot table.\n\n    The index level will be automatically removed from the index when added\n    as columns.\n    \"\"\"\n    pass\n</code></pre> <p>.. _docstring.parameters:</p> <p>Section 3: parameters ~~~~~~~~~~~~~~~~~~~~~</p> <p>The details of the parameters will be added in this section. This section has the title \"Parameters\", followed by a line with a hyphen under each letter of the word \"Parameters\". A blank line is left before the section title, but not after, and not between the line with the word \"Parameters\" and the one with the hyphens.</p> <p>After the title, each parameter in the signature must be documented, including <code>*args</code> and <code>**kwargs</code>, but not <code>self</code>.</p> <p>The parameters are defined by their name, followed by a space, a colon, another space, and the type (or types). Note that the space between the name and the colon is important. Types are not defined for <code>*args</code> and <code>**kwargs</code>, but must be defined for all other parameters. After the parameter definition, it is required to have a line with the parameter description, which is indented, and can have multiple lines. The description must start with a capital letter, and finish with a dot.</p> <p>For keyword arguments with a default value, the default will be listed after a comma at the end of the type. The exact form of the type in this case will be \"int, default 0\". In some cases it may be useful to explain what the default argument means, which can be added after a comma \"int, default -1, meaning all cpus\".</p> <p>In cases where the default value is <code>None</code>, meaning that the value will not be used. Instead of <code>\"str, default None\"</code>, it is preferred to write <code>\"str, optional\"</code>. When <code>None</code> is a value being used, we will keep the form \"str, default None\". For example, in <code>df.to_csv(compression=None)</code>, <code>None</code> is not a value being used, but means that compression is optional, and no compression is being used if not provided. In this case we will use <code>\"str, optional\"</code>. Only in cases like <code>func(value=None)</code> and <code>None</code> is being used in the same way as <code>0</code> or <code>foo</code> would be used, then we will specify \"str, int or None, default None\".</p> <p>Good:</p> <p>.. code-block:: python</p> <pre><code>class Series:\n    def plot(self, kind, color='blue', **kwargs):\n        \"\"\"\n        Generate a plot.\n\n        Render the data in the Series as a matplotlib plot of the\n        specified kind.\n\n        Parameters\n        ----------\n        kind : str\n            Kind of matplotlib plot.\n        color : str, default 'blue'\n            Color name or rgb code.\n        **kwargs\n            These parameters will be passed to the matplotlib plotting\n            function.\n        \"\"\"\n        pass\n</code></pre> <p>Bad:</p> <p>.. code-block:: python</p> <pre><code>class Series:\n    def plot(self, kind, **kwargs):\n        \"\"\"\n        Generate a plot.\n\n        Render the data in the Series as a matplotlib plot of the\n        specified kind.\n\n        Note the blank line between the parameters title and the first\n        parameter. Also, note that after the name of the parameter ``kind``\n        and before the colon, a space is missing.\n\n        Also, note that the parameter descriptions do not start with a\n        capital letter, and do not finish with a dot.\n\n        Finally, the ``**kwargs`` parameter is missing.\n\n        Parameters\n        ----------\n\n        kind: str\n            kind of matplotlib plot\n        \"\"\"\n        pass\n</code></pre> <p>.. _docstring.parameter_types:</p> <p>Parameter types ^^^^^^^^^^^^^^^</p> <p>When specifying the parameter types, Python built-in data types can be used directly (the Python type is preferred to the more verbose string, integer, boolean, etc):</p> <ul> <li>int</li> <li>float</li> <li>str</li> <li>bool</li> </ul> <p>For complex types, define the subtypes. For <code>dict</code> and <code>tuple</code>, as more than one type is present, we use the brackets to help read the type (curly brackets for <code>dict</code> and normal brackets for <code>tuple</code>):</p> <ul> <li>list of int</li> <li>dict of {str : int}</li> <li>tuple of (str, int, int)</li> <li>tuple of (str,)</li> <li>set of str</li> </ul> <p>In case where there are just a set of values allowed, list them in curly brackets and separated by commas (followed by a space). If the values are ordinal and they have an order, list them in this order. Otherwise, list the default value first, if there is one:</p> <ul> <li>{0, 10, 25}</li> <li>{'simple', 'advanced'}</li> <li>{'low', 'medium', 'high'}</li> <li>{'cat', 'dog', 'bird'}</li> </ul> <p>If the type is defined in a Python module, the module must be specified:</p> <ul> <li>datetime.date</li> <li>datetime.datetime</li> <li>decimal.Decimal</li> </ul> <p>If the type is in a package, the module must be also specified:</p> <ul> <li>numpy.ndarray</li> <li>scipy.sparse.coo_matrix</li> </ul> <p>If the type is a biocypher type, also specify biocypher except for Series and DataFrame:</p> <ul> <li>Series</li> <li>DataFrame</li> <li>biocypher.Index</li> <li>biocypher.Categorical</li> <li>biocypher.arrays.SparseArray</li> </ul> <p>If the exact type is not relevant, but must be compatible with a NumPy array, array-like can be specified. If Any type that can be iterated is accepted, iterable can be used:</p> <ul> <li>array-like</li> <li>iterable</li> </ul> <p>If more than one type is accepted, separate them by commas, except the last two types, that need to be separated by the word 'or':</p> <ul> <li>int or float</li> <li>float, decimal.Decimal or None</li> <li>str or list of str</li> </ul> <p>If <code>None</code> is one of the accepted values, it always needs to be the last in the list.</p> <p>For axis, the convention is to use something like:</p> <ul> <li>axis : {0 or 'index', 1 or 'columns', None}, default None</li> </ul> <p>.. _docstring.returns:</p> <p>Section 4: returns or yields ~~~~~~~~~~~~~~~~~~~~~~~~~~~~</p> <p>If the method returns a value, it will be documented in this section. Also if the method yields its output.</p> <p>The title of the section will be defined in the same way as the \"Parameters\". With the names \"Returns\" or \"Yields\" followed by a line with as many hyphens as the letters in the preceding word.</p> <p>The documentation of the return is also similar to the parameters. But in this case, no name will be provided, unless the method returns or yields more than one value (a tuple of values).</p> <p>The types for \"Returns\" and \"Yields\" are the same as the ones for the \"Parameters\". Also, the description must finish with a dot.</p> <p>For example, with a single value:</p> <p>.. code-block:: python</p> <pre><code>def sample():\n    \"\"\"\n    Generate and return a random number.\n\n    The value is sampled from a continuous uniform distribution between\n    0 and 1.\n\n    Returns\n    -------\n    float\n        Random number generated.\n    \"\"\"\n    return np.random.random()\n</code></pre> <p>With more than one value:</p> <p>.. code-block:: python</p> <pre><code>import string\n\ndef random_letters():\n    \"\"\"\n    Generate and return a sequence of random letters.\n\n    The length of the returned string is also random, and is also\n    returned.\n\n    Returns\n    -------\n    length : int\n        Length of the returned string.\n    letters : str\n        String of random letters.\n    \"\"\"\n    length = np.random.randint(1, 10)\n    letters = ''.join(np.random.choice(string.ascii_lowercase)\n                      for i in range(length))\n    return length, letters\n</code></pre> <p>If the method yields its value:</p> <p>.. code-block:: python</p> <pre><code>def sample_values():\n    \"\"\"\n    Generate an infinite sequence of random numbers.\n\n    The values are sampled from a continuous uniform distribution between\n    0 and 1.\n\n    Yields\n    ------\n    float\n        Random number generated.\n    \"\"\"\n    while True:\n        yield np.random.random()\n</code></pre> <p>.. _docstring.see_also:</p> <p>Section 5: see also ~~~~~~~~~~~~~~~~~~~</p> <p>This section is used to let users know about biocypher functionality related to the one being documented. In rare cases, if no related methods or functions can be found at all, this section can be skipped.</p> <p>An obvious example would be the <code>head()</code> and <code>tail()</code> methods. As <code>tail()</code> does the equivalent as <code>head()</code> but at the end of the <code>Series</code> or <code>DataFrame</code> instead of at the beginning, it is good to let the users know about it.</p> <p>To give an intuition on what can be considered related, here there are some examples:</p> <ul> <li><code>loc</code> and <code>iloc</code>, as they do the same, but in one case providing indices   and in the other positions</li> <li><code>max</code> and <code>min</code>, as they do the opposite</li> <li><code>iterrows</code>, <code>itertuples</code> and <code>items</code>, as it is easy that a user   looking for the method to iterate over columns ends up in the method to   iterate over rows, and vice-versa</li> <li><code>fillna</code> and <code>dropna</code>, as both methods are used to handle missing values</li> <li><code>read_csv</code> and <code>to_csv</code>, as they are complementary</li> <li><code>merge</code> and <code>join</code>, as one is a generalization of the other</li> <li><code>astype</code> and <code>biocypher.to_datetime</code>, as users may be reading the   documentation of <code>astype</code> to know how to cast as a date, and the way to do   it is with <code>biocypher.to_datetime</code></li> <li><code>where</code> is related to <code>numpy.where</code>, as its functionality is based on it</li> </ul> <p>When deciding what is related, you should mainly use your common sense and think about what can be useful for the users reading the documentation, especially the less experienced ones.</p> <p>When relating to other libraries (mainly <code>numpy</code>), use the name of the module first (not an alias like <code>np</code>). If the function is in a module which is not the main one, like <code>scipy.sparse</code>, list the full module (e.g. <code>scipy.sparse.coo_matrix</code>).</p> <p>This section has a header, \"See Also\" (note the capital S and A), followed by the line with hyphens and preceded by a blank line.</p> <p>After the header, we will add a line for each related method or function, followed by a space, a colon, another space, and a short description that illustrates what this method or function does, why is it relevant in this context, and what the key differences are between the documented function and the one being referenced. The description must also end with a dot.</p> <p>Note that in \"Returns\" and \"Yields\", the description is located on the line after the type. In this section, however, it is located on the same line, with a colon in between. If the description does not fit on the same line, it can continue onto other lines which must be further indented.</p> <p>For example:</p> <p>.. code-block:: python</p> <pre><code>class Series:\n    def head(self):\n        \"\"\"\n        Return the first 5 elements of the Series.\n\n        This function is mainly useful to preview the values of the\n        Series without displaying the whole of it.\n\n        Returns\n        -------\n        Series\n            Subset of the original series with the 5 first values.\n\n        See Also\n        --------\n        Series.tail : Return the last 5 elements of the Series.\n        Series.iloc : Return a slice of the elements in the Series,\n            which can also be used to return the first or last n.\n        \"\"\"\n        return self.iloc[:5]\n</code></pre> <p>.. _docstring.notes:</p> <p>Section 6: notes ~~~~~~~~~~~~~~~~</p> <p>This is an optional section used for notes about the implementation of the algorithm, or to document technical aspects of the function behavior.</p> <p>Feel free to skip it, unless you are familiar with the implementation of the algorithm, or you discover some counter-intuitive behavior while writing the examples for the function.</p> <p>This section follows the same format as the extended summary section.</p> <p>.. _docstring.examples:</p> <p>Section 7: examples ~~~~~~~~~~~~~~~~~~~</p> <p>This is one of the most important sections of a docstring, despite being placed in the last position, as often people understand concepts better by example than through accurate explanations.</p> <p>Examples in docstrings, besides illustrating the usage of the function or method, must be valid Python code, that returns the given output in a deterministic way, and that can be copied and run by users.</p> <p>Examples are presented as a session in the Python terminal. <code>&gt;&gt;&gt;</code> is used to present code. <code>...</code> is used for code continuing from the previous line. Output is presented immediately after the last line of code generating the output (no blank lines in between). Comments describing the examples can be added with blank lines before and after them.</p> <p>The way to present examples is as follows:</p> <ol> <li> <p>Import required libraries (except <code>numpy</code> and <code>biocypher</code>)</p> </li> <li> <p>Create the data required for the example</p> </li> <li> <p>Show a very basic example that gives an idea of the most common use case</p> </li> <li> <p>Add examples with explanations that illustrate how the parameters can be    used for extended functionality</p> </li> </ol> <p>A simple example could be:</p> <p>.. code-block:: python</p> <pre><code>class Series:\n\n    def head(self, n=5):\n        \"\"\"\n        Return the first elements of the Series.\n\n        This function is mainly useful to preview the values of the\n        Series without displaying all of it.\n\n        Parameters\n        ----------\n        n : int\n            Number of values to return.\n\n        Return\n        ------\n        biocypher.Series\n            Subset of the original series with the n first values.\n\n        See Also\n        --------\n        tail : Return the last n elements of the Series.\n\n        Examples\n        --------\n        &gt;&gt;&gt; ser = pd.Series(['Ant', 'Bear', 'Cow', 'Dog', 'Falcon',\n        ...                'Lion', 'Monkey', 'Rabbit', 'Zebra'])\n        &gt;&gt;&gt; ser.head()\n        0   Ant\n        1   Bear\n        2   Cow\n        3   Dog\n        4   Falcon\n        dtype: object\n\n        With the ``n`` parameter, we can change the number of returned rows:\n\n        &gt;&gt;&gt; ser.head(n=3)\n        0   Ant\n        1   Bear\n        2   Cow\n        dtype: object\n        \"\"\"\n        return self.iloc[:n]\n</code></pre> <p>The examples should be as concise as possible. In cases where the complexity of the function requires long examples, is recommended to use blocks with headers in bold. Use double star <code>**</code> to make a text bold, like in <code>**this example**</code>.</p> <p>.. _docstring.example_conventions:</p> <p>Conventions for the examples ^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p> <p>Code in examples is assumed to always start with these two lines which are not shown:</p> <p>.. code-block:: python</p> <pre><code>import numpy as np\nimport biocypher as pd\n</code></pre> <p>Any other module used in the examples must be explicitly imported, one per line (as recommended in :pep:<code>8#imports</code>) and avoiding aliases. Avoid excessive imports, but if needed, imports from the standard library go first, followed by third-party libraries (like matplotlib).</p> <p>When illustrating examples with a single <code>Series</code> use the name <code>ser</code>, and if illustrating with a single <code>DataFrame</code> use the name <code>df</code>. For indices, <code>idx</code> is the preferred name. If a set of homogeneous <code>Series</code> or <code>DataFrame</code> is used, name them <code>ser1</code>, <code>ser2</code>, <code>ser3</code>...  or <code>df1</code>, <code>df2</code>, <code>df3</code>... If the data is not homogeneous, and more than one structure is needed, name them with something meaningful, for example <code>df_main</code> and <code>df_to_join</code>.</p> <p>Data used in the example should be as compact as possible. The number of rows is recommended to be around 4, but make it a number that makes sense for the specific example. For example in the <code>head</code> method, it requires to be higher than 5, to show the example with the default values. If doing the <code>mean</code>, we could use something like <code>[1, 2, 3]</code>, so it is easy to see that the value returned is the mean.</p> <p>For more complex examples (grouping for example), avoid using data without interpretation, like a matrix of random numbers with columns A, B, C, D... And instead use a meaningful example, which makes it easier to understand the concept. Unless required by the example, use names of animals, to keep examples consistent. And numerical properties of them.</p> <p>When calling the method, keywords arguments <code>head(n=3)</code> are preferred to positional arguments <code>head(3)</code>.</p> <p>Good:</p> <p>.. code-block:: python</p> <pre><code>class Series:\n\n    def mean(self):\n        \"\"\"\n        Compute the mean of the input.\n\n        Examples\n        --------\n        &gt;&gt;&gt; ser = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; ser.mean()\n        2\n        \"\"\"\n        pass\n\n\n    def fillna(self, value):\n        \"\"\"\n        Replace missing values by ``value``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; ser = pd.Series([1, np.nan, 3])\n        &gt;&gt;&gt; ser.fillna(0)\n        [1, 0, 3]\n        \"\"\"\n        pass\n\n    def groupby_mean(self):\n        \"\"\"\n        Group by index and return mean.\n\n        Examples\n        --------\n        &gt;&gt;&gt; ser = pd.Series([380., 370., 24., 26],\n        ...               name='max_speed',\n        ...               index=['falcon', 'falcon', 'parrot', 'parrot'])\n        &gt;&gt;&gt; ser.groupby_mean()\n        index\n        falcon    375.0\n        parrot     25.0\n        Name: max_speed, dtype: float64\n        \"\"\"\n        pass\n\n    def contains(self, pattern, case_sensitive=True, na=numpy.nan):\n        \"\"\"\n        Return whether each value contains ``pattern``.\n\n        In this case, we are illustrating how to use sections, even\n        if the example is simple enough and does not require them.\n\n        Examples\n        --------\n        &gt;&gt;&gt; ser = pd.Series('Antelope', 'Lion', 'Zebra', np.nan)\n        &gt;&gt;&gt; ser.contains(pattern='a')\n        0    False\n        1    False\n        2     True\n        3      NaN\n        dtype: bool\n\n        **Case sensitivity**\n\n        With ``case_sensitive`` set to ``False`` we can match ``a`` with both\n        ``a`` and ``A``:\n\n        &gt;&gt;&gt; s.contains(pattern='a', case_sensitive=False)\n        0     True\n        1    False\n        2     True\n        3      NaN\n        dtype: bool\n\n        **Missing values**\n\n        We can fill missing values in the output using the ``na`` parameter:\n\n        &gt;&gt;&gt; ser.contains(pattern='a', na=False)\n        0    False\n        1    False\n        2     True\n        3    False\n        dtype: bool\n        \"\"\"\n        pass\n</code></pre> <p>Bad:</p> <p>.. code-block:: python</p> <pre><code>def method(foo=None, bar=None):\n    \"\"\"\n    A sample DataFrame method.\n\n    Do not import NumPy and biocypher.\n\n    Try to use meaningful data, when it makes the example easier\n    to understand.\n\n    Try to avoid positional arguments like in ``df.method(1)``. They\n    can be all right if previously defined with a meaningful name,\n    like in ``present_value(interest_rate)``, but avoid them otherwise.\n\n    When presenting the behavior with different parameters, do not place\n    all the calls one next to the other. Instead, add a short sentence\n    explaining what the example shows.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import biocypher as pd\n    &gt;&gt;&gt; df = pd.DataFrame(np.random.randn(3, 3),\n    ...                   columns=('a', 'b', 'c'))\n    &gt;&gt;&gt; df.method(1)\n    21\n    &gt;&gt;&gt; df.method(bar=14)\n    123\n    \"\"\"\n    pass\n</code></pre> <p>.. _docstring.doctest_tips:</p> <p>Tips for getting your examples pass the doctests ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p> <p>Getting the examples pass the doctests in the validation script can sometimes be tricky. Here are some attention points:</p> <ul> <li> <p>Import all needed libraries (except for biocypher and NumPy, those are already   imported as <code>import biocypher as pd</code> and <code>import numpy as np</code>) and define   all variables you use in the example.</p> </li> <li> <p>Try to avoid using random data. However random data might be OK in some   cases, like if the function you are documenting deals with probability   distributions, or if the amount of data needed to make the function result   meaningful is too much, such that creating it manually is very cumbersome.   In those cases, always use a fixed random seed to make the generated examples   predictable. Example::</p> <p>np.random.seed(42) df = pd.DataFrame({'normal': np.random.normal(100, 5, 20)})</p> </li> <li> <p>If you have a code snippet that wraps multiple lines, you need to use '...'   on the continued lines: ::</p> <p>df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], index=['a', 'b', 'c'], ...                   columns=['A', 'B'])</p> </li> <li> <p>If you want to show a case where an exception is raised, you can do::</p> <p>pd.to_datetime([\"712-01-01\"]) Traceback (most recent call last): OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 712-01-01 00:00:00</p> </li> </ul> <p>It is essential to include the \"Traceback (most recent call last):\", but for   the actual error only the error name is sufficient.</p> <ul> <li>If there is a small part of the result that can vary (e.g. a hash in an object   representation), you can use <code>...</code> to represent this part.</li> </ul> <p>If you want to show that <code>s.plot()</code> returns a matplotlib AxesSubplot object,   this will fail the doctest ::</p> <pre><code>&gt;&gt;&gt; s.plot()\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7efd0c0b0690&gt;\n</code></pre> <p>However, you can do (notice the comment that needs to be added) ::</p> <pre><code>&gt;&gt;&gt; s.plot()  # doctest: +ELLIPSIS\n&lt;matplotlib.axes._subplots.AxesSubplot at ...&gt;\n</code></pre> <p>.. _docstring.example_plots:</p> <p>Plots in examples ^^^^^^^^^^^^^^^^^</p> <p>There are some methods in biocypher returning plots. To render the plots generated by the examples in the documentation, the <code>.. plot::</code> directive exists.</p> <p>To use it, place the next code after the \"Examples\" header as shown below. The plot will be generated automatically when building the documentation.</p> <p>.. code-block:: python</p> <pre><code>class Series:\n    def plot(self):\n        \"\"\"\n        Generate a plot with the ``Series`` data.\n\n        Examples\n        --------\n\n        .. plot::\n            :context: close-figs\n\n            &gt;&gt;&gt; ser = pd.Series([1, 2, 3])\n            &gt;&gt;&gt; ser.plot()\n        \"\"\"\n        pass\n</code></pre> <p>.. _docstring.sharing:</p>"},{"location":"community/biocypher-docstring-guide/#sharing-docstrings","title":"Sharing docstrings","text":"<p>biocypher has a system for sharing docstrings, with slight variations, between classes. This helps us keep docstrings consistent, while keeping things clear for the user reading. It comes at the cost of some complexity when writing.</p> <p>Each shared docstring will have a base template with variables, like <code>{klass}</code>. The variables filled in later on using the <code>doc</code> decorator. Finally, docstrings can also be appended to with the <code>doc</code> decorator.</p> <p>In this example, we'll create a parent docstring normally (this is like <code>biocypher.core.generic.NDFrame</code>. Then we'll have two children (like <code>biocypher.core.series.Series</code> and <code>biocypher.core.frame.DataFrame</code>). We'll substitute the class names in this docstring.</p> <p>.. code-block:: python</p> <p>class Parent:        @doc(klass=\"Parent\")        def my_function(self):            \"\"\"Apply my function to {klass}.\"\"\"            ...</p> <p>class ChildA(Parent):        @doc(Parent.my_function, klass=\"ChildA\")        def my_function(self):            ...</p> <p>class ChildB(Parent):        @doc(Parent.my_function, klass=\"ChildB\")        def my_function(self):            ...</p> <p>The resulting docstrings are</p> <p>.. code-block:: python</p> <p>print(Parent.my_function.doc)    Apply my function to Parent. print(ChildA.my_function.doc)    Apply my function to ChildA. print(ChildB.my_function.doc)    Apply my function to ChildB.</p> <p>Notice:</p> <ol> <li>We \"append\" the parent docstring to the children docstrings, which are    initially empty.</li> </ol> <p>Our files will often contain a module-level <code>_shared_doc_kwargs</code> with some common substitution values (things like <code>klass</code>, <code>axes</code>, etc).</p> <p>You can substitute and append in one shot with something like</p> <p>.. code-block:: python</p> <p>@doc(template, **_shared_doc_kwargs)    def my_function(self):        ...</p> <p>where <code>template</code> may come from a module-level <code>_shared_docs</code> dictionary mapping function names to docstrings. Wherever possible, we prefer using <code>doc</code>, since the docstring-writing processes is slightly closer to normal.</p> <p>See <code>biocypher.core.generic.NDFrame.fillna</code> for an example template, and <code>biocypher.core.series.Series.fillna</code> and <code>biocypher.core.generic.frame.fillna</code> for the filled versions.</p>"},{"location":"community/contribute-codebase/","title":"Contribute to the Codebase","text":"<p>Thank you for considering to contribute to the project! This guide will help you to get started with the development of the project. If you have any questions, please feel free to ask them in the issue tracker.</p>"},{"location":"community/contribute-codebase/#dependency-management","title":"Dependency management","text":"<p>We use Poetry for dependency management. Please make sure that you have installed Poetry and set up the environment correctly before starting development.</p>"},{"location":"community/contribute-codebase/#setup-the-environment","title":"Setup the environment","text":"<ul> <li> <p>Install dependencies from the lock file: <code>poetry install</code></p> </li> <li> <p>Use the environment: You can either run commands directly with <code>poetry run &lt;command&gt;</code> or open a shell with <code>poetry shell</code> and then run commands directly.</p> </li> </ul>"},{"location":"community/contribute-codebase/#updating-the-environment","title":"Updating the environment","text":"<p>If you want to fix dependency issues, please do so in the Poetry framework. If Poetry does not work for you for some reason, please let us know.</p> <p>The Poetry dependencies are organized in groups. There are groups with dependencies needed for running BioCypher (<code>[tool.poetry.dependencies</code> with the group name <code>main</code>) and a group with dependencies needed for development (<code>[tool.poetry.group.dev.dependencies</code> with the group name <code>dev</code>).</p> <p>For adding new dependencies:</p> <ul> <li> <p>Add new dependencies: <code>poetry add &lt;dependency&gt; -- group &lt;group&gt;</code></p> </li> <li> <p>Update lock file (after adding new dependencies in pyproject.toml): <code>poetry lock</code></p> </li> </ul>"},{"location":"community/contribute-codebase/#code-quality-and-formal-requirements","title":"Code quality and formal requirements","text":"<p>For ensuring code quality, the following tools are used:</p> <ul> <li> <p>isort for sorting imports</p> </li> <li> <p>black for automated code formatting</p> </li> <li> <p>pre-commit-hooks for ensuring some general rules</p> </li> <li> <p>pep585-upgrade for automatically upgrading type hints to the new native types defined in PEP 585</p> </li> <li> <p>pygrep-hooks for ensuring some general naming rules</p> </li> <li> <p>NEW ruff An extremely fast Python linter and code formatter, written in Rust</p> </li> </ul> <p>Pre-commit hooks are used to automatically run these tools before each commit. They are defined in .pre-commit-config.yaml. To install the hooks run <code>poetry run pre-commit install</code>. The hooks are then executed before each commit. For running the hook for all project files (not only the changed ones) run <code>poetry run pre-commit run --all-files</code>.</p>"},{"location":"community/contribute-codebase/#testing","title":"Testing","text":"<p>The project uses pytest for testing. To run the tests, please run <code>pytest</code> in the root directory of the project. We are developing BioCypher using test-driven development. Please make sure that you add tests for your code before submitting a pull request.</p> <p>The existing tests can also help you to understand how the code works. If you have any questions, please feel free to ask them in the issue tracker.</p> <p>Before submitting a pull request, please make sure that all tests pass and that the documentation builds correctly.</p>"},{"location":"community/contribute-codebase/#small-contributions","title":"Small Contributions","text":"<p>If you want to contribute a small change (e.g. a bugfix), you can probably immediately go ahead and create a pull request. For more substantial changes or additions, please read on.</p>"},{"location":"community/contribute-codebase/#larger-contributions","title":"Larger Contributions","text":"<p>If you want to contribute a larger change, please create an issue first. This will allow us to discuss the change and make sure that it fits into the project. It can happen that development for a feature is already in progress, so it is important to check first to avoid duplicate work. If you have any questions, feel free to approach us in any way you like.</p>"},{"location":"community/contribute-codebase/#versioning","title":"Versioning","text":"<p>We use semantic versioning for the project. This means that the version number is incremented according to the following scheme:</p> <ul> <li> <p>Increment the major version number if you make incompatible API changes.</p> </li> <li> <p>Increment the minor version number if you add functionality in a backwards-   compatible manner.</p> </li> <li> <p>Increment the patch version number if you make backwards-compatible bug fixes.</p> </li> </ul> <p>We use the <code>bumpversion</code> tool to update the version number in the <code>pyproject.toml</code> file. This will create a new git tag automatically. Usually, versioning is done by the maintainers, so please do not increment versions in pull requests by default.</p>"},{"location":"community/contribute-docs/","title":"Contributing to the documentation","text":"<p>Contributing to the documentation benefits everyone who uses biocypher. We encourage you to help us improve the documentation, and you don't have to be an expert on biocypher to do so! In fact, there are sections of the docs that are worse off after being written by experts. If something in the docs doesn't make sense to you, updating the relevant section after you figure it out is a great way to ensure it will help the next person. Please visit the issues page for a full list of issues that are currently open regarding the biocypher documentation.</p>"},{"location":"community/contribute-docs/#about-the-biocypher-documentation","title":"About the Biocypher documentation","text":"<p>The documentation is written in Markdown, which is almost like writing in plain English, and built using Material for MkDocs. Some other important things to know about the docs:</p> <ul> <li>The biocypher documentation consists of two parts: the docstrings in the code   itself and the docs in this folder <code>docs/</code>.</li> </ul> <p>The docstrings provide a clear explanation of the usage of the individual   functions, while the documentation in this folder consists of tutorial-like   overviews per topic together with some other information (what's new,   installation, etc).</p> <ul> <li> <p>The docstrings follow a biocypher convention, based on the Google Docstring   Standard. Follow the biocypher docstring guide for detailed   instructions on how to write a correct docstring.</p> </li> <li> <p>Our API documentation files in <code>docs/reference/source</code> house the auto-generated   documentation from the docstrings. For classes, there are a few subtleties   around controlling which methods and attributes have pages auto-generated.</p> </li> </ul>"},{"location":"community/contribute/","title":"Contributing to BioCypher","text":""},{"location":"community/contribute/#bug-reports-and-enhancement-requests","title":"Bug reports and enhancement requests","text":"<p>Bug reports and enhancement requests are an important part of making BioCypher more stable and are curated though Github issues. When reporting and issue or request, please select the appropriate category and fill out the issue form fully to ensure others and the core development team can fully understand the scope of the issue.</p> <p>The issue will then show up to the BioCypher community and be open to comments/ideas from others.</p> <p>Categories</p> <ul> <li>Bug Report: Report incorrect behavior in the BioCypher library</li> <li>Documentation Improvement: Report wrong or missing documentation</li> <li>Feature Request: Suggest and idea for BioCypher</li> <li>Installation Issue: Report issues installing the BioCypher on the system</li> </ul>"},{"location":"community/contribute/#finding-and-issue-to-contribute-to","title":"Finding and issue to contribute to","text":"<p>If you are brand new to BioCypher or open-source development, we recommend searching the GitHub \"issues\" tab to find issues that interest you. Unassigned issues labeled <code>Docs</code> and <code>good first issue</code> are typically good for newer contributors.</p> <p>Once you\u2019ve found an interesting issue, it\u2019s a good idea to assign the issue to yourself, so nobody else duplicates the work on it. On the Github issue, a comment with the exact text take to automatically assign you the issue (this will take seconds and may require refreshing the page to see it).</p> <p>If for whatever reason you are not able to continue working with the issue, please unassign it, so other people know it\u2019s available again. You can check the list of assigned issues, since people may not be working in them anymore. If you want to work on one that is assigned, feel free to kindly ask the current assignee if you can take it (please allow at least a week of inactivity before considering work in the issue discontinued).</p>"},{"location":"community/contribute/#submitting-a-pull-request","title":"Submitting a Pull Request","text":""},{"location":"community/contribute/#version-control-git-and-github","title":"Version control, Git, and GitHub","text":"<p>BioCypher is hosted on GitHub, and to contribute, you will need to sign up for a free GitHub account. We use Git for version control to allow many people to work together on the project.</p> <p>If you are new to Git, you can reference some of these resources for learning Git. Feel free to reach out to the contributor community for help if needed:</p> <ul> <li>Git documentation.</li> </ul> <p>Also, the project follows a forking workflow further described on this page whereby contributors fork the repository, make changes and then create a Pull Request. So please be sure to read and follow all the instructions in this guide.</p> <p>If you are new to contributing to projects through forking on GitHub, take a look at the GitHub documentation for contributing to projects. GitHub provides a quick tutorial using a test repository that may help you become more familiar with forking a repository, cloning a fork, creating a feature branch, pushing changes and making Pull Requests.</p> <p>Below are some useful resources for learning more about forking and Pull Requests on GitHub:</p> <ul> <li> <p>the GitHub documentation for forking a repo.</p> </li> <li> <p>the GitHub documentation for collaborating with Pull Requests.</p> </li> <li> <p>the GitHub documentation for working with forks.</p> </li> </ul>"},{"location":"community/contribute/#getting-started-with-git","title":"Getting started with Git","text":"<p>GitHub has instructions for installing git, setting up your SSH key, and configuring git. All these steps need to be completed before you can work seamlessly between your local repository and GitHub.</p>"},{"location":"community/contribute/#create-a-fork-of-biocypher","title":"Create a fork of BioCypher","text":"<p>You will need your own copy of BioCypher (aka fork) to work on the code. Go to the BioCypher project page and hit the Fork button. Please uncheck the box to copy only the main branch before selecting Create Fork. You will want to clone your fork to your machine</p> <p><pre><code>git clone https://github.com/your-user-name/biocypher.git biocypher-yourname\ncd biocypher-yourname\ngit remote add upstream https://github.com/biocypher/biocypher.git\ngit fetch upstream\n</code></pre> This creates the directory <code>biocypher-yourname</code> and connects your repository to the upstream (main project) biocypher repository.</p>"},{"location":"community/contribute/#creating-a-feature-branch","title":"Creating a feature branch","text":"<p>Your local <code>main</code> branch should always reflect the current state of BioCypher repository. First ensure it\u2019s up-to-date with the main BioCypher repository.</p> <pre><code>git checkout main\ngit pull upstream main --ff-only\n</code></pre> <p>Then, create a feature branch for making your changes. For example, we are going to create a branch called <code>my-new-feature-for-biocypher</code></p> <pre><code>git checkout -b my-new-feature-for-biocypher\n</code></pre> <p>This changes your working branch from <code>main</code> to the <code>my-new-feature-for-biocypher</code> branch. Keep any changes in this branch specific to one bug or feature so it is clear what the branch brings to Biocypher. You can have many feature branches and switch in between them using the <code>git checkout</code> command.</p> <p>When you want to update the feature branch with changes in main after you created the branch, check the section on updating a PR.</p>"},{"location":"community/contribute/#making-code-changes","title":"Making code changes","text":"<p>Before modifying any code, ensure you follow the contributing environment guidelines to set up an appropriate development environment.</p> <p>Then once you have made code changes, you can see all the changes you\u00bfve currently made by running.</p> <pre><code>git status\n</code></pre> <p>For files you intended to modify or add, run</p> <pre><code>git add path/to/file-to-be-added-or-changed\n</code></pre> <p>Running <code>git status</code> again should display</p> <pre><code>On branch my-new-feature-for-BioCypher\n\n    modified:   /relative/path/to/file-to-be-added-or-changed\n</code></pre> <p>Finally, commit your changes to your local repository with an explanatory commit message <pre><code>git commit -m \"your commit message goes here\"\n</code></pre></p> <p>How to write good commit messages?</p> <p>You can consult the following references to understand how to write better commit messages. - Conventional Commits: A specification for adding human and machine readable meaning to commit messages - Git Commit Good Practice by OpenStack</p>"},{"location":"community/contribute/#pushing-your-changes","title":"Pushing your changes","text":"<p>When you want your changes to appear publicly on your GitHub page, push your forked feature branch\u2019s commits</p> <pre><code>git push origin my-new-feature-for-biocypher\n</code></pre> <p>Here <code>origin</code> is the default name given to your remote repository on GitHub. You can see the remote repositories <pre><code>git remote -v\n</code></pre> If you added the upstream repository as described above you will see something like <pre><code>origin  git@github.com:yourname/biocypher.git (fetch)\norigin  git@github.com:yourname/biocypher.git (push)\nupstream        git://github.com/biocypher/biocypher.git (fetch)\nupstream        git://github.com/biocypher/biocypher.git (push)\n</code></pre></p> <p>Now your code is on GitHub, but it is not yet a part of the BioCypher project. For that to happen, a Pull Request (PR) needs to be submitted on GitHub.</p>"},{"location":"community/contribute/#making-a-pull-request-pr","title":"Making a Pull Request (PR)","text":"<p>One you have finished your code changes, your code change will need to follow the BioCypher contribution guidelines to be successfully accepted.</p> <p>If everything looks good, you are ready to make a Pull Request. A Pull Request is how code from your local repository becomes available to the GitHub community to review and merged into project to appear the in the next release. To submit a Pull Request:</p> <ol> <li> <p>Navigate to your repository on GitHub</p> </li> <li> <p>Click on the Compare &amp; Pull Request button</p> </li> <li> <p>You can then click on Commits and Files Changed to make sure everything looks okay one last time</p> </li> <li> <p>Write a descriptive title that includes prefixes. BioCypher uses a convention for title prefixes. Here are some common ones along with general guidelines for when to use them:</p> </li> </ol> <pre><code>- ENH: Enhancement, new functionality\n\n- BUG: Bug fix\n\n- DOCS: Additions/updates to documentation\n\n- TEST: Additions/updates to tests\n\n- BUILD: Updates to the build process/scripts\n\n- PERF: Performance improvement\n</code></pre> <ol> <li>Write a description of your changes in the <code>Preview Discussion</code> tab</li> <li>Click <code>Send Pull Request</code></li> </ol> <p>This request then goes to the repositorz maintainers, and they will review the code.</p>"},{"location":"community/contribute/#updating-your-pull-request","title":"Updating your Pull Request","text":"<p>Based on the review you get on your pull request, you will probably need to make some changes to the code. You can follow the :ref:<code>code committing steps &lt;contributing.commit-code&gt;</code> again to address any feedback and update your pull request.</p> <p>It is also important that updates in the biocypher <code>main</code> branch are reflected in your pull request. To update your feature branch with changes in the biocypher <code>main</code> branch, run:</p> <pre><code>    git checkout my-new-feature-for-biocypher\n    git fetch upstream\n    git merge upstream/main\n</code></pre> <p>If there are no conflicts (or they could be fixed automatically), a file with a default commit message will open, and you can simply save and quit this file.</p> <p>If there are merge conflicts, you need to solve those conflicts. See for example at https://help.github.com/articles/resolving-a-merge-conflict-using-the-command-line/ for an explanation on how to do this.</p> <p>Once the conflicts are resolved, run:</p> <ol> <li><code>git add -u</code> to stage any files you've updated;</li> <li><code>git commit</code> to finish the merge.</li> </ol> <p>Note</p> <p>If you have uncommitted changes at the moment you want to update the branch with <code>main</code>, you will need to <code>stash</code> them prior to updating (see the <code>stash docs &lt;https://git-scm.com/book/en/v2/Git-Tools-Stashing-and-Cleaning&gt;</code>__). This will effectively store your changes and they can be reapplied after updating.</p> <p>After the feature branch has been update locally, you can now update your pull request by pushing to the branch on GitHub:</p> <pre><code>    git push origin shiny-new-feature\n</code></pre> <p>Any <code>git push</code> will automatically update your pull request with your branch's changes and restart the :ref:<code>Continuous Integration &lt;contributing.ci&gt;</code> checks.</p>"},{"location":"community/contribute/#tips-for-a-successful-pull-request","title":"Tips for a successful pull request","text":"<p>If you have made it to the <code>Making a pull request</code>_ phase, one of the core contributors may take a look. Please note however that a handful of people are responsible for reviewing all of the contributions, which can often lead to bottlenecks.</p> <p>To improve the chances of your pull request being reviewed, you should:</p> <ul> <li>Reference an open issue for non-trivial changes to clarify the PR's purpose</li> <li>Ensure you have appropriate tests. These should be the first part of any PR</li> <li>Keep your pull requests as simple as possible. Larger PRs take longer to review</li> <li>Ensure that CI is in a green state. Reviewers may not even look otherwise</li> </ul>"},{"location":"community/maintenance/","title":"Biocypher Maintenance","text":"<p>This guide is for biocypher' maintainers. It may also be interesting to contributors looking to understand the biocypher development process and what steps are necessary to become a maintainer.</p>"},{"location":"community/maintenance/#roles","title":"Roles","text":"<p>GitHub publishes the full list of permissions.</p>"},{"location":"community/maintenance/#tasks","title":"Tasks","text":"<p>biocypher is largely a volunteer project, so these tasks shouldn't be read as \"expectations\" of triage and maintainers. Rather, they're general descriptions of what it means to be a maintainer.</p> <ul> <li>Triage newly filed issues (see :ref:<code>maintaining.triage</code>)</li> <li>Review newly opened pull requests</li> <li>Respond to updates on existing issues and pull requests</li> <li>Drive discussion and decisions on stalled issues and pull requests</li> <li>Provide experience / wisdom on API design questions to ensure consistency and maintainability</li> <li>Project organization (run / attend developer meetings, represent biocypher)</li> </ul> <p>https://matthewrocklin.com/blog/2019/05/18/maintainer may be interesting background reading.</p>"},{"location":"community/maintenance/#issue-triage","title":"Issue triage","text":"<p>Triage is an important first step in addressing issues reported by the community, and even partial contributions are a great way to help maintain biocypher. Only remove the \"Needs Triage\" tag once all of the steps below have been completed.</p> <p>Here's a typical workflow for triaging a newly opened issue.</p> <ol> <li>Thank the reporter for opening an issue</li> </ol> <p>The issue tracker is many people's first interaction with the biocypher project itself,    beyond just using the library. As such, we want it to be a welcoming, pleasant    experience.</p> <ol> <li>Is the necessary information provided?</li> </ol> <p>Ideally reporters would fill out the issue template, but many don't.    If crucial information (like the version of biocypher they used), is missing    feel free to ask for that and label the issue with \"Needs info\". The    report should follow the guidelines in :ref:<code>contributing.bug_reports</code>.    You may want to link to that if they didn't follow the template.</p> <p>Make sure that the title accurately reflects the issue. Edit it yourself    if it's not clear.</p> <ol> <li>Is this a duplicate issue?</li> </ol> <p>We have many open issues. If a new issue is clearly a duplicate, label the    new issue as \"Duplicate\" and close the issue with a link to the original issue.    Make sure to still thank the reporter, and encourage them to chime in on the    original issue, and perhaps try to fix it.</p> <p>If the new issue provides relevant information, such as a better or slightly    different example, add it to the original issue as a comment or an edit to    the original post.</p> <ol> <li>Is the issue minimal and reproducible?</li> </ol> <p>For bug reports, we ask that the reporter provide a minimal reproducible    example. See https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports    for a good explanation. If the example is not reproducible, or if it's    clearly not minimal, feel free to ask the reporter if they can provide    and example or simplify the provided one. Do acknowledge that writing    minimal reproducible examples is hard work. If the reporter is struggling,    you can try to write one yourself and we'll edit the original post to include it.</p> <p>If a reproducible example can't be provided, add the \"Needs info\" label.</p> <p>If a reproducible example is provided, but you see a simplification,    edit the original post with your simpler reproducible example.</p> <p>Ensure the issue exists on the main branch and that it has the \"Needs Triage\" tag    until all steps have been completed. Add a comment to the issue once you have    verified it exists on the main branch, so others know it has been confirmed.</p> <ol> <li>Is this a clearly defined feature request?</li> </ol> <p>Generally, biocypher prefers to discuss and design new features in issues, before    a pull request is made. Encourage the submitter to include a proposed API    for the new feature. Having them write a full docstring is a good way to    pin down specifics.</p> <p>Tag new feature requests with \"Needs Discussion\", as we'll need a discussion    from several biocypher maintainers before deciding whether the proposal is in    scope for biocypher.</p> <ol> <li>Is this a usage question?</li> </ol> <p>We prefer that usage questions are asked on StackOverflow with the biocypher    tag. https://stackoverflow.com/questions/tagged/biocypher</p> <p>If it's easy to answer, feel free to link to the relevant documentation section,    let them know that in the future this kind of question should be on    StackOverflow, and close the issue.</p> <ol> <li>What labels and milestones should I add?</li> </ol> <p>Apply the relevant labels. This is a bit of an art, and comes with experience.    Look at similar issues to get a feel for how things are labeled.</p> <p>If the issue is clearly defined and the fix seems relatively straightforward,    label the issue as \"Good first issue\".</p> <p>Once you have completed the above, make sure to remove the \"needs triage\" label.</p>"},{"location":"community/maintenance/#closing-issues","title":"Closing issues","text":"<p>Be delicate here: many people interpret closing an issue as us saying that the conversation is over. It's typically best to give the reporter some time to respond or self-close their issue if it's determined that the behavior is not a bug, or the feature is out of scope. Sometimes reporters just go away though, and we'll close the issue after the conversation has died. If you think an issue should be closed but are not completely sure, please apply the \"closing candidate\" label and wait for other maintainers to take a look.</p>"},{"location":"community/maintenance/#reviewing-pull-requests","title":"Reviewing pull requests","text":"<p>Anybody can review a pull request: regular contributors, triagers, or core-team members. But only core-team members can merge pull requests when they're ready.</p> <p>Here are some things to check when reviewing a pull request.</p> <ul> <li>Tests should be in a sensible location: in the same file as closely related tests.</li> <li>New public APIs should be included somewhere in <code>doc/source/reference/</code>.</li> <li>New / changed API should use the <code>versionadded</code> or <code>versionchanged</code> directives in the docstring.</li> <li>User-facing changes should have a whatsnew in the appropriate file.</li> <li>Regression tests should reference the original GitHub issue number like <code># GH-1234</code>.</li> <li>The pull request should be labeled and assigned the appropriate milestone (the next patch release   for regression fixes and small bug fixes, the next minor milestone otherwise)</li> <li>Changes should comply with our :ref:<code>policies.version</code>.</li> </ul>"},{"location":"community/maintenance/#cleaning-up-old-issues","title":"Cleaning up old issues","text":"<p>Every open issue in biocypher has a cost. Open issues make finding duplicates harder, and can make it harder to know what needs to be done in biocypher. That said, closing issues isn't a goal on its own. Our goal is to make biocypher the best it can be, and that's best done by ensuring that the quality of our open issues is high.</p> <p>Occasionally, bugs are fixed but the issue isn't linked to in the Pull Request. In these cases, comment that \"This has been fixed, but could use a test.\" and label the issue as \"Good First Issue\" and \"Needs Test\".</p> <p>If an older issue doesn't follow our issue template, edit the original post to include a minimal example, the actual output, and the expected output. Uniformity in issue reports is valuable.</p> <p>If an older issue lacks a reproducible example, label it as \"Needs Info\" and ask them to provide one (or write one yourself if possible). If one isn't provide reasonably soon, close it according to the policies in :ref:<code>maintaining.closing</code>.</p>"},{"location":"community/maintenance/#cleaning-up-old-pull-requests","title":"Cleaning up old pull requests","text":"<p>Occasionally, contributors are unable to finish off a pull request. If some time has passed (two weeks, say) since the last review requesting changes, gently ask if they're still interested in working on this. If another two weeks or so passes with no response, thank them for their work and then either:</p> <ul> <li>close the pull request;</li> <li>push to the contributor's branch to push their work over the finish line (if   you're part of <code>biocypher-core</code>). This can be helpful for pushing an important PR   across the line, or for fixing a small merge conflict.</li> </ul> <p>If closing the pull request, then please comment on the original issue that \"There's a stalled PR at #1234 that may be helpful.\", and perhaps label the issue as \"Good first issue\" if the PR was relatively close to being accepted.</p>"},{"location":"community/maintenance/#becoming-a-biocypher-maintainer","title":"Becoming a biocypher maintainer","text":"<p>The full process is outlined in our <code>governance documents</code>_. In summary, we're happy to give triage permissions to anyone who shows interest by being helpful on the issue tracker.</p> <p>The required steps for adding a maintainer are:</p> <ol> <li>Contact the contributor and ask their interest to join.</li> <li> <p>Add the contributor to the appropriate <code>GitHub Team &lt;https://github.com/orgs/biocypher-dev/teams&gt;</code>_ if accepted the invitation.</p> </li> <li> <p><code>biocypher-core</code> is for core team members</p> </li> <li><code>biocypher-triage</code> is for biocypher triage members</li> </ol> <p>If adding to <code>biocypher-core</code>, there are two additional steps:</p> <ol> <li>Add the contributor to the biocypher Google group.</li> <li>Create a pull request to add the contributor's GitHub handle to <code>biocypher-dev/biocypher/web/biocypher/config.yml</code>.</li> </ol> <p>The current list of core-team members is at https://github.com/biocypher-dev/biocypher/blob/main/web/biocypher/config.yml</p>"},{"location":"community/maintenance/#merging-pull-requests","title":"Merging pull requests","text":"<p>Only core team members can merge pull requests. We have a few guidelines.</p> <ol> <li>You should typically not self-merge your own pull requests without approval.    Exceptions include things like small changes to fix CI    (e.g. pinning a package version). Self-merging with approval from other    core team members is fine if the change is something you're very confident    about.</li> <li>You should not merge pull requests that have an active discussion, or pull    requests that has any <code>-1</code> votes from a core maintainer. biocypher operates    by consensus.</li> <li>For larger changes, it's good to have a +1 from at least two core team members.</li> </ol> <p>In addition to the items listed in :ref:<code>maintaining.closing</code>, you should verify that the pull request is assigned the correct milestone.</p> <p>Pull requests merged with a patch-release milestone will typically be backported by our bot. Verify that the bot noticed the merge (it will leave a comment within a minute typically). If a manual backport is needed please do that, and remove the \"Needs backport\" label once you've done it manually. If you forget to assign a milestone before tagging, you can request the bot to backport it with:</p> <p>.. code-block:: console</p> <p>@Meeseeksdev backport  <p>.. _maintaining.asv-machine:</p>"},{"location":"community/maintenance/#benchmark-machine","title":"Benchmark machine","text":"<p>The team currently owns dedicated hardware for hosting a website for biocypher' ASV performance benchmark. The results are published to https://asv-runner.github.io/asv-collection/biocypher/</p>"},{"location":"community/maintenance/#configuration","title":"Configuration","text":"<p>The machine can be configured with the <code>Ansible &lt;http://docs.ansible.com/ansible/latest/index.html&gt;</code>_ playbook in https://github.com/tomaugspurger/asv-runner.</p>"},{"location":"community/maintenance/#publishing","title":"Publishing","text":"<p>The results are published to another GitHub repository, https://github.com/tomaugspurger/asv-collection. Finally, we have a cron job on our docs server to pull from https://github.com/tomaugspurger/asv-collection, to serve them from <code>/speed</code>. Ask Tom or Joris for access to the webserver.</p>"},{"location":"community/maintenance/#debugging","title":"Debugging","text":"<p>The benchmarks are scheduled by Airflow. It has a dashboard for viewing and debugging the results. You'll need to setup an SSH tunnel to view them <pre><code>ssh -L 8080:localhost:8080 biocypher@panda.likescandy.com\n</code></pre></p>"},{"location":"community/maintenance/#release-process","title":"Release process","text":"<p>The release process makes a snapshot of biocypher (a git commit) available to users with a particular version number. After the release the new biocypher version will be available in the next places:</p> <ul> <li>Git repo with a <code>new tag &lt;https://github.com/biocypher-dev/biocypher/tags&gt;</code>_</li> <li>Source distribution in a <code>GitHub release &lt;https://github.com/biocypher-dev/biocypher/releases&gt;</code>_</li> <li>Pip packages in the <code>PyPI &lt;https://pypi.org/project/biocypher/&gt;</code>_</li> <li>Conda/Mamba packages in <code>conda-forge &lt;https://anaconda.org/conda-forge/biocypher&gt;</code>_</li> </ul> <p>The process for releasing a new version of biocypher is detailed next section.</p> <p>The instructions contain <code>&lt;version&gt;</code> which needs to be replaced with the version to be released (e.g. <code>1.5.2</code>). Also the branch to be released <code>&lt;branch&gt;</code>, which depends on whether the version being released is the release candidate of a new version, or any other version. Release candidates are released from <code>main</code>, while other versions are released from their branch (e.g. <code>1.5.x</code>).</p>"},{"location":"community/maintenance/#prerequisites","title":"Prerequisites","text":"<p>In order to be able to release a new biocypher version, the next permissions are needed:</p> <ul> <li>Merge rights to the <code>biocypher &lt;https://github.com/biocypher-dev/biocypher/&gt;</code> and   <code>biocypher-feedstock &lt;https://github.com/conda-forge/biocypher-feedstock/&gt;</code> repositories.   For the latter, open a PR adding your GitHub username to the conda-forge recipe.</li> <li>Permissions to push to <code>main</code> in the biocypher repository, to push the new tags.</li> <li><code>Write permissions to PyPI &lt;https://github.com/conda-forge/biocypher-feedstock/pulls&gt;</code>_.</li> <li>Access to our website / documentation server. Share your public key with the   infrastructure committee to be added to the <code>authorized_keys</code> file of the main   server user.</li> <li>Access to the social media accounts, to publish the announcements.</li> </ul>"},{"location":"community/maintenance/#pre-release","title":"Pre-release","text":"<ol> <li> <p>Agree with the core team on the next topics:</p> </li> <li> <p>Release date (major/minor releases happen usually every 6 months, and patch releases      monthly until x.x.5, just before the next major/minor)</p> </li> <li>Blockers (issues and PRs that must be part of the release)</li> <li> <p>Next version after the one being released</p> </li> <li> <p>Update and clean release notes for the version to be released, including:</p> </li> <li> <p>Set the final date of the release</p> </li> <li>Remove any unused bullet point</li> <li> <p>Make sure there are no formatting issues, typos, etc.</p> </li> <li> <p>Make sure the CI is green for the last commit of the branch being released.</p> </li> <li> <p>If not a release candidate, make sure all backporting pull requests to the branch    being released are merged.</p> </li> <li> <p>Create a new issue and milestone for the version after the one being released.    If the release was a release candidate, we would usually want to create issues and    milestones for both the next major/minor, and the next patch release. In the    milestone of a patch release, we add the description <code>on-merge: backport to &lt;branch&gt;</code>,    so tagged PRs are automatically backported to the release branch by our bot.</p> </li> <li> <p>Change the milestone of all issues and PRs in the milestone being released to the    next milestone.</p> </li> </ol>"},{"location":"community/maintenance/#release","title":"Release","text":"<ol> <li>Create an empty commit and a tag in the last commit of the branch to be released:: <pre><code>git checkout &lt;branch&gt;\ngit pull --ff-only upstream &lt;branch&gt;\ngit clean -xdf\ngit commit --allow-empty --author=\"biocypher Development Team &lt;biocypher-dev@python.org&gt;\" -m \"RLS: &lt;version&gt;\"\ngit tag -a v&lt;version&gt; -m \"Version &lt;version&gt;\"  # NOTE that the tag is v1.5.2 with \"v\" not 1.5.2\ngit push upstream &lt;branch&gt; --follow-tags\n</code></pre></li> </ol> <p>The docs for the new version will be built and published automatically with the docs job in the CI, which will be triggered when the tag is pushed.</p> <ol> <li>Only if the release is a release candidate, we want to create a new branch for it, immediately    after creating the tag. For example, if we are releasing biocypher 1.4.0rc0, we would like to    create the branch 1.4.x to backport commits to the 1.4 versions. As well as create a tag to    mark the start of the development of 1.5.0 (assuming it is the next version)::</li> </ol> <p><pre><code>git checkout -b 1.4.x\ngit push upstream 1.4.x\ngit checkout main\ngit commit --allow-empty -m \"Start 1.5.0\"\ngit tag -a v1.5.0.dev0 -m \"DEV: Start 1.5.0\"\ngit push upstream main --follow-tags\n</code></pre> 3. Download the source distribution and wheels from the <code>wheel staging area &lt;https://anaconda.org/scientific-python-nightly-wheels/biocypher&gt;</code>_.    Be careful to make sure that no wheels are missing (e.g. due to failed builds).</p> <p>Running scripts/download_wheels.sh with the version that you want to download wheels/the sdist for should do the trick.    This script will make a <code>dist</code> folder inside your clone of biocypher and put the downloaded wheels and sdist there::</p> <p><pre><code>scripts/download_wheels.sh &lt;VERSION&gt;\n</code></pre> 4. Create a <code>new GitHub release &lt;https://github.com/biocypher-dev/biocypher/releases/new&gt;</code>_:</p> <ul> <li>Tag: <code>&lt;version&gt;</code></li> <li>Title: <code>biocypher &lt;version&gt;</code></li> <li>Description: Copy the description of the last release of the same kind (release candidate, major/minor or patch release)</li> <li>Files: <code>biocypher-&lt;version&gt;.tar.gz</code> source distribution just generated</li> <li>Set as a pre-release: Only check for a release candidate</li> <li> <p>Set as the latest release: Leave checked, unless releasing a patch release for an older version      (e.g. releasing 1.4.5 after 1.5 has been released)</p> </li> <li> <p>Upload wheels to PyPI:: <pre><code>twine upload biocypher/dist/biocypher-&lt;version&gt;*.{whl,tar.gz} --skip-existing\n</code></pre></p> </li> <li>The GitHub release will after some hours trigger an    <code>automated conda-forge PR &lt;https://github.com/conda-forge/biocypher-feedstock/pulls&gt;</code>_.    (If you don't want to wait, you can open an issue titled <code>@conda-forge-admin, please update version</code> to trigger the bot.)    Merge it once the CI is green, and it will generate the conda-forge packages.</li> </ul> <p>In case a manual PR needs to be done, the version, sha256 and build fields are the    ones that usually need to be changed. If anything else in the recipe has changed since    the last release, those changes should be available in <code>ci/meta.yaml</code>.</p>"},{"location":"community/maintenance/#post-release","title":"Post-Release","text":"<ol> <li> <p>Update symlinks to stable documentation by logging in to our web server, and    editing <code>/var/www/html/biocypher-docs/stable</code> to point to <code>version/&lt;latest-version&gt;</code>    for major and minor releases, or <code>version/&lt;minor&gt;</code> to <code>version/&lt;patch&gt;</code> for    patch releases. The exact instructions are (replace the example version numbers by    the appropriate ones for the version you are releasing):</p> <ul> <li>Log in to the server and use the correct user.</li> <li><code>cd /var/www/html/biocypher-docs/</code></li> <li><code>ln -sfn version/2.1 stable</code> (for a major or minor release)</li> <li><code>ln -sfn version/2.0.3 version/2.0</code> (for a patch release)</li> </ul> </li> <li> <p>If releasing a major or minor release, open a PR in our source code to update    <code>web/biocypher/versions.json</code>, to have the desired versions in the documentation    dropdown menu.</p> </li> <li> <p>Close the milestone and the issue for the released version.</p> </li> <li> <p>Create a new issue for the next release, with the estimated date of release.</p> </li> <li> <p>Open a PR with the placeholder for the release notes of the next version. See    for example <code>the PR for 1.5.3 &lt;https://github.com/biocypher-dev/biocypher/pull/49843/files&gt;</code>_.    Note that the template to use depends on whether it is a major, minor or patch release.</p> </li> <li> <p>Announce the new release in the official channels (use previous announcements    for reference):</p> <ul> <li>The biocypher-dev and pydata mailing lists</li> <li>Twitter, Mastodon, Telegram and LinkedIn</li> </ul> </li> <li> <p>Update this release instructions to fix anything incorrect and to update about any    change since the last release.</p> </li> </ol> <p>.. _governance documents: https://github.com/biocypher-dev/biocypher/blob/main/web/biocypher/about/governance.md .. _list of permissions: https://docs.github.com/en/organizations/managing-access-to-your-organizations-repositories/repository-roles-for-an-organization</p>"},{"location":"community/release-notes/","title":"Release notes","text":""},{"location":"community/release-notes/#version-080","title":"Version 0.8.0","text":""},{"location":"community/release-notes/#enhancements","title":"Enhancements","text":"<ul> <li>Labels can now be sorted in various way before being written to CSV, in order to reflect the ontological hierarchy flexibly. The default has changed to \"ascending\", meaning more specific to more generic labels (it was alphabetic before).</li> </ul>"},{"location":"community/release-notes/#contributors","title":"Contributors","text":"<ul> <li>Johann Dreo</li> </ul>"},{"location":"community/release-notes/#version-070","title":"Version 0.7.0","text":""},{"location":"community/release-notes/#enhancements_1","title":"Enhancements","text":"<ul> <li>Added <code>Ruff</code> as a tool for linting/formatting, including CI.</li> <li>BioCypher has support for NetworkX.</li> <li>BioChatter has improved token statistics handling (@shaohong feng) and has received a prototypic Python API calling module as a result of the German BioHackathon 3 in December. In this hackathon, we worked on integrating BioChatter-driven API calling with the scverse ecosystem (starting with core packages anndata and scanpy). Many thanks to all who contributed in the hackathon!</li> </ul>"},{"location":"community/release-notes/#contributors_1","title":"Contributors","text":"<ul> <li>Paul To</li> <li>Yaxi Liu</li> <li>Shaohong Feng</li> <li>Sebastian Lobentanzer</li> <li>Edwin Carre\u00f1o</li> </ul>"},{"location":"learn/quickstart/","title":"Quickstart","text":"<ul> <li> <p> Already Familiar?</p> <p>We have a project template (batteries included!)</p> <p> To the template</p> </li> <li> <p> New to BioCypher?</p> <p>Follow our detailed tutorial for on-boarding BioCypher.</p> <p> To the tutorial</p> </li> </ul> <p>Note</p> <p>If you already know how BioCypher works, we provide here a quickstart into the knowledge graph build process. We provide a template repository on GitHub, which you can use to get started with your own project. You can get it here. To set up a new project, simply follow the instructions in the README.</p> <p>If you are new to BioCypher and would like a step-by-step introduction to the package, please follow the tutorial.</p> <p>The BioCypher workflow of creating your own knowledge graph consists of three consecutive steps:</p> <ol> <li> <p>Clearly define the scope of your project, including the data sources you want to use, the entities and relationships you want to represent, and the ontologies that should inform these entities.</p> </li> <li> <p>Using these definitions, find existing adapters of data sources or, if necessary, create your own. For the data yielded by these adapters, create a schema configuration file that tells BioCypher how to represent the entities and relationships in the graph.</p> </li> <li> <p>Run BioCypher using the adapters and schema config to create the knowledge graph. If necessary, iterate over KG construction and configuration until you are satisfied with the result.</p> </li> </ol>"},{"location":"learn/quickstart/#the-input-adapter","title":"The input adapter","text":"<p>BioCypher follows a modular approach to data inputs; to create a knowledge graph, we use at least one adapter module that provides a data stream to build the graph from. Examples for current adapters can be found on the GitHub project adapter view. This is the first place to look when creating your own KG; BioCypher adapters are meant to be reusable and a centralised way of maintaining access to data sources.</p> <p>Adapters can ingest data from many different input formats, including Python modules as in the CROssBAR adapter (which uses the OmniPath backend software, PyPath, for downloading and caching data), advanced file management formats such as Parquet as in the Open Targets adapter, or simple CSV files as in the Dependency Map adapter.</p> <p>The main function of the adapter is to pass data into BioCypher, usually as some form of iterable (commonly a list or generator of items). As a minimal example, we load a list of proteins with identifiers, trivial names, and molecular masses from a (fictional) CSV:</p> Adapter yielding nodes<pre><code># read data into df\nwith open(\"file.csv\", \"r\") as f:\n    proteins = pd.read_csv(f)\n\n# yield proteins from data frame\ndef node_generator():\n    for p in proteins:\n        _id = p[\"uniprot_id\"]\n        _type = \"protein\"\n        _props = {\n            \"name\": p[\"trivial_name\"],\n            \"mm\": p[\"molecular_mass\"]\n        }\n\n        yield (_id, _type, _props)\n</code></pre> <p>For nodes, BioCypher expects a tuple containing three entries; the preferred identifier of the node, the type of entity, and a dictionary containing all other properties (can be empty). What BioCypher does with the received information is determined largely by the schema configuration detailed below.</p> Adapter yielding edges<pre><code># read data into df\nwith open(\"file.csv\", \"r\") as f:\n    interactions = pd.read_csv(f)\n\n# yield interactions from data frame\ndef edge_generator():\n    for i in interactions:\n        _id = i[\"id\"]\n        _source = i[\"source\"]\n        _target = i[\"target\"]\n        _type = \"interaction\"\n        _props = {\n            \"type\": i[\"relationship_type\"],\n            \"score\": i[\"score\"],\n        }\n\n        yield (_id, _source, _target, _type, _props)\n</code></pre> <p>For edges, BioCypher expects a tuple containing five entries; the preferred identifier of the edge (can be <code>None</code>), the identifier of the source node (non-optional), the identifier of the target node (non-optional), the type of relationship, and a dictionary containing all other properties (can be empty).</p> <p>For advanced usage, the type of node or edge can be determined programatically. Properties do not need to be explicitly called one by one; they can be passed in as a complete dictionary of all entries and filtered inside BioCypher by detailing the desired properties per node type in the schema configuration file.</p>"},{"location":"learn/quickstart/#the-schema-configuration-yaml-file","title":"The schema configuration YAML file","text":"<p>The second important component of translation into a BioCypher-compatible knowledge graph is the specification of graph constituents and their mode of representation in the graph. To make this known to the BioCypher module, we use the schema-config.yaml, which details only the immediate constituents of the desired graph as the top-level entries in the YAML file. While each of these top-level entries is required to be found in the underlying ontology (for instance, the Biolink model), the <code>input_label</code> field is arbitrary and has to match the <code>_type</code> yielded by the adapter (compare above).</p> <p>Other fields of each entry can refer to the representation of the entity in the KG (<code>represented_as: node</code>), and the identifier namespace chosen for each entity type. For instance, a protein could be represented by a UniProt identifier, the corresponding ENSEMBL identifier, or an HGNC gene symbol. We prefer the CURIE prefix for unambiguous identification of entities. The CURIE prefix for \"Uniprot Protein\" is <code>uniprot</code>, so a consistent protein schema definition would be:</p> <pre><code>protein:                    # top-level entry, has to match ontology\n  represented_as: node      # mode of representation: node or edge\n  preferred_id: uniprot     # preferred identifier namespace\n  input_label: protein      # label that identifies members of this class (_type)\n</code></pre> <p>Note</p> <p>For BioCypher classes, similar to the internal representation in the Biolink model, we use lower sentence-case notation, e.g., <code>protein</code> and <code>small molecule</code>. For file names and Neo4j labels, these are converted to PascalCase. For more information, see the Ontology tutorial.</p> <p>The above configuration of the protein class specifies its representation as a node, that we wish to use the UniProt identifier as the main identifier for proteins, and that proteins in the data stream from the adapter carry the label (<code>_type</code>) <code>protein</code> (in lowercase). Should we want to use the ENSEMBL namespace instead of UniProt IDs, the corresponding CURIE prefix, in this case, <code>ensembl</code>, can be substituted:</p> <pre><code>protein:\n  represented_as: node\n  preferred_id: ensembl\n  input_label: protein\n</code></pre> <p>If there exists no identifier system that is suitable for coverage of the data (which is fairly common when it comes to relationships), <code>preferred_id</code> field can be omitted. This will lead to the creation of a generic <code>id</code> property on this node or edge type.</p>"},{"location":"learn/quickstart/#biocypher-api-documentation","title":"BioCypher API documentation","text":"<p>BioCypher is instantiated using the <code>BioCypher()</code> class, which can be called without arguments, given that the configuration files are either present in the working directory, or the pipeline should be run with default settings.</p> <pre><code>from biocypher import BioCypher\nbc = BioCypher()\n</code></pre> <p>BioCypher's main functionality is writing the graph (nodes and edges) to a database or files for database import. We exemplarise this using the Neo4j output format, writing CSV files formatted for the Neo4j admin import. In this example, <code>node_generator()</code> and <code>edge_generator()</code> are the adapter functions that yield nodes and edges, respectively (see above).</p> <pre><code>bc.write_nodes(node_generator())\nbc.write_edges(edge_generator())\n</code></pre> <p>Node and edge generators can contain arbitrarily many types of nodes and edges, which will be mapped via the schema configuration and sorted by BioCypher. One instance of the BioCypher class keeps track of the nodes and edges that have been written to the database, so that multiple calls to <code>write_nodes()</code> and <code>write_edges()</code> will not lead to duplicate entries in the database.</p> <p>For on-line writing to a database or a Pandas dataframe, we use the functions with <code>add</code> instead of <code>write</code>. For instance, to add nodes and edges to a Pandas dataframe, we can use:</p> <pre><code>bc.add_nodes(node_generator())\nbc.add_edges(edge_generator())\n</code></pre> <p>To retrieve the dataframe once all entities are in the graph, we can call <code>to_df()</code>:</p> <pre><code>df = bc.to_df()\n</code></pre> <p>For more information on the usage of these functions, please refer to the Tutorial section and the full API documentation.</p>"},{"location":"learn/quickstart/#the-biocypher-configuration-yaml-file","title":"The Biocypher configuration YAML file","text":"<p>Most of the configuration options for BioCypher can and should be specified in the configuration YAML file, <code>biocypher_config.yaml</code>. While BioCypher comes with default settings (the ones you can see in the Configuration section), we can override them by specifying the desired settings in the local configuration in the root or the <code>config</code> directory of the project. The primary BioCypher settings are found in the top-level entry <code>biocypher</code>. For instance, you can select your output format (<code>dbms</code>) and output path, the location of the schema configuration file, and the ontology to be used.</p> biocypher_config.yaml<pre><code>biocypher:\n  dbms: postgresql\n  output_path: postgres_out/\n  schema_config: config/schema-config.yaml\n  head_ontology:\n    url: https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl\n    root_node: entity\n</code></pre> <p>You can currently select between <code>postgresql</code>, <code>neo4j</code>, <code>rdf</code> (beta), and <code>arangodb</code> (beta) as your output format; more options will be added in the future. The <code>output_path</code> is relative to your working directory, as is the schema-config path. The <code>ontology</code> should be specified as a (preferably persistent) URL to the ontology file, and a <code>root_node</code> to specify the node from which the ontology should be traversed.  We recommend using a URL that specifies the exact version of the ontology, as in the example above.</p>"},{"location":"learn/quickstart/#dbms-specific-settings","title":"DBMS-specific settings","text":"<p>In addition to the general settings, you can specify settings specific to each DBMS (database management system) in the configuration file under the <code>postgresql</code>, <code>arangodb</code>, <code>rdf</code>, or <code>neo4j</code> entry. For instance, you can specify the database name, the host, the port, and the credentials for your database. You can also set delimiters for the entities and arrays in your import files. For a list of all settings, please refer to the Configuration section.</p> biocypher_config.yaml<pre><code>neo4j:\n  database_name: biocypher\n  uri: neo4j://localhost:7687\n  user: neo4j\n  password: neo4j\n  delimiter: ','\n</code></pre>"},{"location":"learn/quickstart/#additional-resources","title":"Additional Resources","text":"<ul> <li>BioCypher API Reference</li> <li>BioCypher Configuration Reference</li> <li>BioCypher Schema Reference</li> </ul>"},{"location":"learn/explanation/","title":"Explanation:","text":""},{"location":"learn/explanation/#purpose","title":"Purpose","text":"<p>The purpose of this documentation is to provide a comprehensive guide to understanding and working with knowledge graphs within the context of BioCypher.  It aims to explain the fundamental concepts of knowledge graphs and how they are represented in BioCypher.</p>"},{"location":"learn/explanation/#catalog","title":"Catalog","text":""},{"location":"learn/explanation/#basics-of-knowledge-graphs","title":"Basics of Knowledge Graphs.","text":"<ul> <li>Here goes a simple explanation about edges, nodes, and graphs. TODO: Create a page for this, including images.</li> </ul>"},{"location":"learn/explanation/#how-a-graph-is-represented-in-biocypher","title":"How a graph is represented in BioCypher?","text":"<ul> <li>Here goes a simple explanation about how the graphs are represented in BioCypher.</li> </ul>"},{"location":"learn/explanation/#about-this-documentation","title":"About this documentation","text":"<p>TO DO: delete once the webpage is live in production.</p>"},{"location":"learn/explanation/about-documentation/","title":"About the structure: tutorials, how-to guides, technical reference and explanations","text":"<p>Our documentation follows the \"Diataxis documentation framework\" proposed by Daniel Procida. This framework divide the documentation universe in four parts that represent different purposes or functions. They are: Tutorials, How-to guides, Technical Reference and Explanations.</p>"},{"location":"learn/explanation/about-documentation/#brief-summary","title":"Brief Summary:","text":"<p>Table 1. Diataxis documentation framework (extracted from link)</p> Tutorials How-to Guides Reference Explanation what they do introduce, educate, lead guide state, describe, inform explain, clarify, discuss answers the question Can you teach me to...? How do I...? \"What is...?\" \"Why...?\" oriented to learning a goal information understanding purpose to provide a learning experience to help achieve a particular goal to describe the machinery to illuminate a topic form a lesson a series of steps dry description discursive explanation analogy teaching a small child how to cook a recipe in a cookery book a reference encyclopedia article an article on culinary social history"},{"location":"learn/explanation/exp001_basics_knowledge_graphs/","title":"Basics of Knowledge Graphs","text":"<ul> <li> <p>Nodes</p> </li> <li> <p>Edges</p> </li> <li> <p>Operations</p> </li> </ul>"},{"location":"learn/guides/","title":"Index","text":""},{"location":"learn/guides/#how-to-guides","title":"How-to Guides:","text":"<ol> <li> <p>How-to create a standalone Docker image for a BioCypher KG?</p> </li> <li> <p>How-to use or create an Adapter to read data?</p> </li> <li>How-to combine data from different Adapters?</li> <li>How-to create a graph from a CSV file</li> <li>How-to create convert a graph to Pandas Dataframe.</li> <li>How-to create a complete pipeline (from CSV file to Neo4j)</li> <li>How to configure BioCypher</li> <li>How to define an Schema for your Graph</li> </ol>"},{"location":"learn/guides/adapters/","title":"Adapters","text":""},{"location":"learn/guides/adapters/#introduction","title":"Introduction","text":"<p>BioCypher is a modular framework, with the main purpose of avoiding redundant maintenance work for maintainers of secondary resources and end users alike. To achieve this, we use a collection of reusable \u201cadapters\u201d for the different sources of biomedical knowledge as well as for different ontologies. To see whether your favourite resource is already supported or currently in development, or if you would like to contribute to the development of a new adapter, please refer to this GitHub projects view (check the tabs for different views) or the meta-graph instance.</p> <p>Note</p> <p>We are currently working on adapter documentation, so the collection in the GitHub Projects view may be less than complete. Please get in touch if you want to make sure that your favourite resource is supported.</p> <ul> <li> <p> Adapter Tutorial</p> <p>For more information on developing your own adapters, please refer to this tutorial</p> <p> To the Adpaters Tutorial</p> </li> </ul> <p>The project view is built from issues in the BioCypher GitHub repository, which carry <code>Fields</code> (a GitHub Projects-specific attribute) to describe their category and features. In detail, these are as follows:</p> <ul> <li> <p><code>Component Type</code>: This refers to the class of component and can be one of <code>Adapter</code>, <code>Ontology</code>, or <code>Pipeline</code>.</p> </li> <li> <p><code>Adapter Granularity</code>: This is only applicable to adapters and can be either <code>Primary</code> (denoting an atomic resource that is represented by the adapter) or <code>Secondary</code> (denoting a composite resource, often pre-harmonised).</p> </li> <li> <p><code>Adapter Input Format</code> provides a drop-down menu of the different formats that can be ingested, such as <code>Flat File</code>, <code>API</code>, or <code>OWL</code>. Select the one that applies to the resource.</p> </li> <li> <p><code>Resource URL</code>: A free-text field to provide a link to the resource, also used for identification purposes.</p> </li> <li> <p><code>Resource Type</code>: Currently only <code>Database</code> or <code>Ontology</code>, but more granular reporting is planned.</p> </li> <li> <p><code>Data Type</code> provides a drop-down menu of the different data types that can be ingested, such as <code>Proteomics</code>, <code>Genetics</code>, or <code>Clinical</code>. Select the one that applies to the resource. This field primarily makes sense for primary adapters, but is interesting information particularly for the pipelines that use the adapters. For that reason, when building the meta-graph (see below), we propagate this information from the adapters to the pipelines.</p> </li> </ul> <p>Caution</p> <p>There is currently one type of meta-information that needs to be provided via free-text annotation in the text body of the issue: the links of pipelines to the input adapters and ontologies they use. For the meta-graph pipeline to work correctly, this information needs to be provided in the issue of the pipeline, in a line that starts with <code>Uses:</code>, followed by a space-separated list of issue numbers representing the used components.</p> <p>To make this annotation less error-prone, we use the auto-completion GitHub provides for referencing issues. Typing a <code>#</code> character and then a few characters of the title of the issue to be linked to the pipeline will show a list of possible matches. Select the correct one and the issue number will be inserted automatically.</p> <p>The meta-graph pipeline extracts this information and uses it to build the meta-graph described below.</p>"},{"location":"learn/guides/adapters/#biocypher-meta-graph","title":"BioCypher meta-graph","text":"<ul> <li> <p> Repo with Docker setup</p> <p>For more information on developing your own adapters, please refer to this tutorial</p> <p> To the BioCypher Meta-graph</p> </li> </ul> <p>We have built a BioCypher pipeline (from the template repository) that fetches information about all adapters from the BioCypher GitHub repository via the GitHub API and builds a graph of all adapters and their dependencies.  Browsing this graph can give an overview of the current state of the adapters supported by BioCypher and the pipelines they are used in. The graph can be built locally by cloning the repository and running the pipeline using <code>docker compose up</code>. The graph is then available at <code>localhost:7474/browser/</code> in the Neo4j Browser.</p> <p>If you're unfamiliar with Neo4j, you can use the following Cypher query to retrieve an overview of all graph contents:</p> <pre><code>MATCH (n)\nRETURN n\n</code></pre> <p>For more information on how to use the graph, please refer to the Neo4j documentation.</p>"},{"location":"learn/guides/htg001_standalone_docker_biocypher/","title":"Standalone Docker Image","text":"<p>In order to build a standalone Docker image for a BioCypher KG, you only need small modifications to the Docker image of the template. We will render the data that ususally is stored in the <code>biocypher_neo4j_volume</code> to our local disk by exchanging <code>biocypher_neo4j_volume</code> with a local directory, <code>./biocypher_neo4j_volume</code>. Then, we use a Dockerfile to build an image that contains the final database. This image can be used to deploy the database anywhere, without the need to run the BioCypher code. This process is demonstrated in the drug-interactions example repository.</p> <ol> <li> <p>Clone the example repository</p> <pre><code>git clone https://github.com/biocypher/drug-interactions.git\ncd drug-interactions\n</code></pre> </li> <li> <p>Attach volumes to disk by modifying the docker-compose.yml. In the example repository, we have created a dedicated compose file for the standalone image. You can see the differences between the standard and standalone compose files here. IMPORTANT: only run the standalone compose file once, as the data in the <code>./biocypher_neo4j_volume</code> directory is persistent and interferes with subsequent runs. If you want to run it again, you need to delete the <code>./biocypher_neo4j_volume</code> directory.</p> </li> <li> <p>Run the standalone compose file. This will create the <code>./biocypher_neo4j_volume</code> directory and store the data in it. You can stop the container after the database has been created.</p> <pre><code>docker compose -f docker-compose-local-disk.yml up -d\ndocker compose -f docker-compose-local-disk.yml down\n</code></pre> </li> <li> <p>Create standalone <code>Dockerfile</code> (example here):</p> <pre><code># Dockerfile\nFROM neo4j:4.4-enterprise\nCOPY ./biocypher_neo4j_volume /data\nRUN chown -R 7474:7474 /data\nEXPOSE 7474\nEXPOSE 7687\n</code></pre> </li> <li> <p>Build the standalone image.</p> <pre><code>docker build -t drug-interactions:latest .\n</code></pre> <p>This image can be deployed anywhere, without the need to run the BioCypher code. For example, you can add it to a Docker Compose file (example here):</p> <pre><code># docker-compose.yml\nversion: '3.9'\nservices:\n[other services ...]\n\nbiocypher:\n    container_name: biocypher\n    image: biocypher/drug-interactions:latest\n    environment:\n    NEO4J_dbms_security_auth__enabled: \"false\"\n    NEO4J_dbms_databases_default__to__read__only: \"false\"\n    NEO4J_ACCEPT_LICENSE_AGREEMENT: \"yes\"\n    ports:\n    - \"0.0.0.0:7474:7474\"\n    - \"0.0.0.0:7687:7687\"\n</code></pre> </li> </ol>"},{"location":"learn/guides/ontologies/","title":"Handling Ontologies","text":"<p>BioCypher relies on ontologies to ground the knowledge graph contents in biology. This has the advantages of providing machine readability and therefore automation capabilities as well as making working with BioCypher accessible to biologically oriented researchers. However, it also means that BioCypher requires a certain amount of knowledge about ontologies and how to use them. We try to make dealing with ontologies as easy as possible, but some basic understanding is required. In the following we will cover the basics of ontologies and how to use them in BioCypher.</p>"},{"location":"learn/guides/ontologies/#what-is-an-ontology","title":"What is an ontology?","text":"<p>An ontology is a formal representation of a domain of knowledge. It is a hierarchical structure of concepts and relations. The concepts are organized into a hierarchy, where each concept is a subclass of a more general concept. For instance, a wardrobe is a subclass of a piece of furniture. Individual wardrobes, such as yours or mine, are instances of the concept wardrobe, and as such would be represented as Wardrobe nodes in a knowledge graph. In BioCypher, these nodes would additionally inherit the PieceOfFurniture label from the ontological hierarchy of things.</p> <p>Note</p> <p>Why is the class called piece of furniture but the label is PieceOfFurniture?</p> <p>The Biolink model uses two different case notations for its labels: the \"internal\" designation of classes is in lower sentence case (\"protein\", \"pairwise molecular interaction\"), while the \"external\" designation is in PascalCase (\"Protein\", \"PairwiseMolecularInteraction\"). BioCypher uses the same paradigm: in most cases (input, schema configuration, internally), the lower sentence case is used, while in the output (Neo4j labels, file system names) the PascalCase is more suitable; Neo4j labels and system file names don't deal well with spaces and special characters. Therefore, we check the output file names for their compliance with the Neo4j naming rules. All non compliant characters are removed from the file name (e.g. if the ontology class is called \"desk (piece of furniture)\", the brackets would be removed and the file name will be \"DeskPieceOfFurniture\"). We also remove the \"biolink:\" CURIE prefix for use in file names and Neo4j labels.</p> <p>The relations between concepts can also be organized into a hierarchy. In the specific case of a Neo4j graph, however, relationships cannot possess multiple labels; therefore, if concept inheritance is desired for relationships, they need to be \"reified\", i.e., turned into nodes. BioCypher provides a simple way of converting edges to nodes and vice versa (using the <code>represented_as</code> field). For a more in-depth explanation of ontologies, we recommend this introduction.</p>"},{"location":"learn/guides/ontologies/#how-biocypher-uses-ontologies","title":"How BioCypher uses ontologies","text":"<p>BioCypher is agnostic to the choice of ontology. Practically, we have built our initial projects around the Biolink model, because it provides a large but shallow collection of concepts that are relevant to the biomedical domain. Other examples of generalist ontologies are the Experimental Factor Ontology and the Basic Formal Ontology. To account for the specific requirements of expert systems, it is possible to use multiple ontologies in the same project. For instance, one might want to extend the rather basic classes relating to molecular interactions in Biolink (the most specific being <code>pairwise molecular interaction</code>) with more specific classes from a more domain-specific ontology, such as the EBI molecular interactions ontology (PSI-MI). A different project may need to define very specific genetics concepts, and thus extend the Biolink model at the terminal node <code>sequence variant</code> with the corresponding subtree of the Sequence Ontology. The OBO Foundry and the BioPortal collect many such specialised ontologies.</p> <p>The default format for ingesting ontology definitions into BioCypher is the Web Ontology Language (OWL); BioCypher can read <code>.owl</code>, <code>.rdf</code>, and <code>.ttl</code> files. The preferred way to specify the ontology or ontologies to be used in a project is to specify them in the biocypher configuration file (<code>biocypher_config.yaml</code>). This file is used to specify the location of the ontology files, as well as the root node of the main (\"head\") ontology and join nodes as fusion points for all \"tail\" ontologies. For more info, see the section on hybridising ontologies.</p>"},{"location":"learn/guides/ontologies/#visualising-ontologies","title":"Visualising ontologies","text":"<p>BioCypher provides a simple way of visualising the ontology hierarchy. This is useful for debugging and for getting a quick overview of the ontology and which parts are actually used in the knowledge graph to be created. Depending on your use case you can either visualise the parts of the ontology used in the knowledge graph (sufficient for most use cases) or the full ontology. If the used ontology is more complex and contains multiple inheritance please refer to the section on visualising complex ontologies.</p>"},{"location":"learn/guides/ontologies/#visualise-only-the-parts-of-the-ontology-used-in-the-knowledge-graph","title":"Visualise only the parts of the ontology used in the knowledge graph","text":"<p>To get an overview of the structure of our project, we can run the following command via the interface:</p> Visualising the ontology hierarchy<pre><code>from biocypher import BioCypher\nbc = BioCypher(\n    offline=True,  # no need to connect or to load data\n    schema_config_path=\"tutorial/06_schema_config.yaml\",\n)\nbc.show_ontology_structure()\n</code></pre> <p>This will build the ontology scaffold and print a tree visualisation of its hierarchy to the console using the treelib library. You can see this in action in tutorial part 6 (<code>tutorial/06_relationships.py</code>). The output will look something like this:</p> <pre><code>Showing ontology structure, based on Biolink 3.0.3:\nentity\n\u251c\u2500\u2500 association\n\u2502   \u2514\u2500\u2500 gene to gene association\n\u2502       \u2514\u2500\u2500 pairwise gene to gene interaction\n\u2502           \u2514\u2500\u2500 pairwise molecular interaction\n\u2502               \u2514\u2500\u2500 protein protein interaction\n\u251c\u2500\u2500 mixin\n\u2514\u2500\u2500 named thing\n    \u2514\u2500\u2500 biological entity\n        \u2514\u2500\u2500 polypeptide\n            \u2514\u2500\u2500 protein\n                \u251c\u2500\u2500 entrez.protein\n                \u251c\u2500\u2500 protein isoform\n                \u2514\u2500\u2500 uniprot.protein\n</code></pre> <p>Note</p> <p>BioCypher will only show the parts of the ontology that are actually used in the knowledge graph with the exception of intermediary nodes that are needed to build a complete tree. For instance, the <code>protein</code> class is linked to the root class <code>entity</code> via <code>polypeptide</code>, <code>biological entity</code>, and <code>named thing</code>, all of which are not part of the input data.</p>"},{"location":"learn/guides/ontologies/#visualise-the-full-ontology","title":"Visualise the full ontology","text":"<p>If you want to see the complete ontology tree, you can call <code>show_ontology_structure</code> with the parameter <code>full=True</code>.</p> Visualising the full ontology hierarchy<pre><code>from biocypher import BioCypher\nbc = BioCypher(\n    offline=True,  # no need to connect or to load data\n    schema_config_path=\"tutorial/06_schema_config.yaml\",\n)\nbc.show_ontology_structure(full=True)\n</code></pre>"},{"location":"learn/guides/ontologies/#visualise-complex-ontologies","title":"Visualise complex ontologies","text":"<p>Not all ontologies can be easily visualised as a tree, such as ontologies with multiple inheritance, where classes in the ontology can have multiple parent classes. This violates the definition of a tree, where each node can only have one parent node. Consequently, ontologies with multiple inheritance cannot be visualised as a tree.</p> <p>BioCypher can still handle these ontologies, and you can call <code>show_ontology_structure()</code> to get a visualisation of the ontology. However, each ontology class will only be added to the hierarchy tree once (a class with multiple parent classes is only placed under one parent in the hierarchy tree). Since this will occur the first time the class is seen, the ontology class might not be placed where you would expect it. This only applies to the visualisation; the underlying ontology is still correct and contains all ontology classes and their relationships.</p> <p>Note</p> <p>When calling <code>show_ontology_structure()</code>, BioCypher automatically checks if the ontology contains multiple inheritance and logs a warning message if so.</p> <p>If you need to get a visualisation of the ontology with multiple inheritance, you can call <code>show_ontology_structure()</code> with the parameter <code>to_disk=/some/path/where_to_store_the_file</code>. This creates a <code>GraphML</code> file and stores it at the specified location.</p>"},{"location":"learn/guides/ontologies/#using-ontologies-plain-biolink","title":"Using ontologies: plain Biolink","text":"<p>BioCypher maps any input data to the underlying ontology; in the basic case, the Biolink model. This mapping is defined in the schema configuration (<code>schema_config.yaml</code>, see also here). In the simplest case, the representation of a concept in the knowledge graph to be built and the Biolink model class representing this concept are synonymous. For instance, the concept protein is represented by the Biolink class protein. To introduce proteins into the knowledge graph, one would simply define a node constituent with the class label protein. This is the mechanism we implicitly used for proteins in the basic tutorial (part 1); to reiterate:</p> schema_config.yaml<pre><code>protein:\n  represented_as: node\n  # ...\n</code></pre>"},{"location":"learn/guides/ontologies/#model-extensions","title":"Model extensions","text":"<p>There are multiple reasons why a user might want to modify the basic model of the ontology or ontologies used. A class that is relevant to the user's task might be missing (Explicit inheritance). A class might not be granular enough, and the user would like to split it into subclasses based on distinct inputs (Implicit inheritance). For some very common use cases, we recommend going one step further and, maybe after some testing using the above \"soft\" model extensions, proposing the introduction of a new class to the model itself. For instance, Biolink is an open source community project, and new classes can be requested by opening an issue or filing a pull request directly on the Biolink model GitHub repository. Similar mechanisms apply for OBO Foundry ontologies.</p> <p>BioCypher provides further methods for ontology manipulation. The name of a class of the model may be too unwieldy for the use inside the desired knowledge graph, and the user would like to introduce a synonym/alias (Synonyms). Finally, the user might want to extend the basic model with another, more specialised ontology (Hybridising ontologies).</p>"},{"location":"learn/guides/ontologies/#explicit-inheritance","title":"Explicit inheritance","text":"<p>Explicit inheritance is the most straightforward way of extending the basic model. It is also the most common use case. For instance, the Biolink model does not contain a class for <code>protein isoform</code>, and neither does it contain a relationship class for <code>protein protein interaction</code>, both of which we have already used in the basic tutorial. Since protein isoforms are specific types of protein, it makes sense to extend the existing Biolink model class <code>protein</code> with the concept of protein isoforms. To do this, we simply add a new class <code>protein isoform</code> to the schema configuration, and specify that it is a subclass of <code>protein</code> using the (optional) <code>is_a</code> field:</p> schema_config.yaml<pre><code>protein isoform:\n  is_a: protein\n  represented_as: node\n  # ...\n</code></pre> <p>Explicit inheritance can also be used to introduce new relationship classes. However, if the output is a Neo4j graph, these relationships must be represented as nodes to provide full functionality, since edges do not allow multiple labels. This does not mean that explicit inheritance cannot be used in edges; it is even recommended to do so to situate all components of the knowledge graph in the ontological hierarchy. However, to have the ancestry represented in the resulting Neo4j graph DB, multiple labels are required. For instance, we have already used the <code>protein protein interaction</code> relationship in the basic tutorial (part 6), making it a child of the Biolink model class <code>pairwise molecular interaction</code>. To reiterate:</p> schema_config.yaml<pre><code>protein protein interaction:\n  is_a: pairwise molecular interaction\n  represented_as: node\n  # ...\n</code></pre> <p>The <code>is_a</code> field can be used to specify multiple inheritance, i.e., multiple ancestor classes and their direct parent-child relationships can be created by specifying multiple classes (as a list) in the <code>is_a</code> field. For instance, if we wanted to further extend the protein-protein interaction with a more specific <code>enzymatic interaction</code> class, we could do so as follows:</p> schema_config.yaml<pre><code>enzymatic interaction:\n  is_a: [protein protein interaction, pairwise molecular interaction]\n  represented_as: node\n  # ...\n</code></pre> <p>Note</p> <p>To create this multiple inheritance chain, we do not require the creation of a <code>protein protein interaction</code> class as shown above; all intermediary classes are automatically created by BioCypher and inserted into the ontological hierarchy. To obtain a continuous ontology tree, the target class (i.e., the last in the list) must be a real Biolink model class.</p>"},{"location":"learn/guides/ontologies/#implicit-inheritance","title":"Implicit inheritance","text":"<p>The base model (in the standard case, Biolink) can also be extended without specifying an explicit <code>is_a</code> field. This \"implicit\" inheritance happens when a class has multiple input labels that each refer to a distinct preferred identifier. In other words, if both the <code>input_label</code> and the <code>preferred_id</code> fields of a schema configuration class are lists, BioCypher will automatically create a subclass for each of the preferred identifiers. This is demonstrated in part 3 of the basic tutorial.</p> <p>Caution</p> <p>If only the <code>input_label</code> field - but not the <code>preferred_id</code> field - is a list, BioCypher will merge the inputs instead. This is useful for cases where different input streams should be unified under the same class label. See part 2 of the basic tutorial for more information.</p> <p>To make this more concrete, let's consider the example of <code>pathway</code> annotations. There are multiple projects that provide pathway annotations, such as Reactome and Wikipathways, and, in contrast to proteins, pathways are not easily mapped one-to-one. For classes where mapping is difficult or even impossible, we can use implicit subclassing instead. The Biolink model contains a <code>pathway</code> class, which we can use as a parent class of the Reactome and Wikipathways classes; we simply need to provide the pathways as two separate inputs with their own labels (e.g., \"react\" and \"wiki\"), and specify a corresponding list of preferred identifiers in the <code>preferred_id</code> field:</p> schema_config.yaml<pre><code>pathway:\n  represented_as: node\n  preferred_id: [reactome, wikipathways]\n  input_label: [react, wiki]\n  # ...\n</code></pre> <p>This will prompt BioCypher to create two subclasses of <code>pathway</code>, one for each input, and to map the input data to these subclasses. In the resulting knowledge graph, the Reactome and Wikipathways pathways will be represented as distinct classes by prepending the preferred identifier to the class label: <code>Reactome.Pathway</code> and <code>Wikipathways.Pathway</code>. By virtue of BioCypher's multiple labelling paradigm, those nodes will also inherit the <code>Pathway</code> class label as well as all parent labels and mixins of <code>Pathway</code> (<code>BiologicalProcess</code>, etc.). This allows us to query the graph for all <code>Pathway</code> nodes as well as for specific datasets depending on the desired granularity.</p> <p>Note</p> <p>This also works for relationships, but in this case, not the preferred identifiers but the sources (defined in the <code>source</code> field) are used to create the subclasses.</p>"},{"location":"learn/guides/ontologies/#synonyms","title":"Synonyms","text":"<p>Note: Tutorial Files</p> <p>The code for this tutorial can be found at <code>tutorial/07__synonyms.py</code>. Schema files are at <code>tutorial/07_schema_config.yaml</code>, configuration in <code>tutorial/07_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p> <p>In some cases, an ontology may contain a biological concept, but the name of the concept does for some reason not agree with the users desired knowledge graph structure. For instance, the user may not want to represent protein complexes in the graph as <code>macromolecular complex</code> nodes due to ease of use and/or readability criteria and rather call these nodes <code>complex</code>. In such cases, the user can introduce a synonym for the ontology class. This is done by selecting another, more desirable name for the respective class(es) and specifying the <code>synonym_for</code> field in their schema configuration. In this case, as we would like to represent protein complexes as <code>complex</code> nodes, we can do so as follows:</p> schema_config.yaml<pre><code>complex:\n  synonym_for: macromolecular complex\n  represented_as: node\n  # ...\n</code></pre> <p>Importantly, BioCypher preserves these mappings to enable compatibility between different structural instantiations of the ontology (or combination of ontologies). All entities that are mapped to ontology classes in any way can be harmonised even between different types of concrete representations.</p> <p>Note</p> <p>It is essential that the desired class name is used as the main class key in the schema configuration, and the ontology class name is given in the <code>synonym_for</code> field. The name given in the <code>synonym_for</code> field must be an existing class name (in this example, a real Biolink class).</p> <p>We can visualise the structure of the ontology as we have before. Instead of using <code>bc.show_ontology_structure()</code> however, we can use the <code>bc.summary()</code> method to show the structure and simultaneously check for duplicates and missing labels. This is useful for debugging purposes, and we can see that the import was completed without encountering duplicates, and all labels in the input are accounted for in the schema configuration. We also observe in the tree that the <code>complex</code> class is now a synonym for the <code>macromolecular complex</code> class (their being synonyms indicated as an equals sign):</p> <pre><code>Showing ontology structure based on https://raw.githubusercontent.com/biolink/biolink-model/v3.2.1/biolink-model.owl.ttl\nentity\n\u251c\u2500\u2500 association\n\u2502   \u2514\u2500\u2500 gene to gene association\n\u2502       \u2514\u2500\u2500 pairwise gene to gene interaction\n\u2502           \u2514\u2500\u2500 pairwise molecular interaction\n\u2502               \u2514\u2500\u2500 protein protein interaction\n\u2514\u2500\u2500 named thing\n    \u2514\u2500\u2500 biological entity\n        \u251c\u2500\u2500 complex = macromolecular complex\n        \u2514\u2500\u2500 polypeptide\n            \u2514\u2500\u2500 protein\n                \u251c\u2500\u2500 entrez.protein\n                \u251c\u2500\u2500 protein isoform\n                \u2514\u2500\u2500 uniprot.protein\n</code></pre>"},{"location":"learn/guides/ontologies/#hybridising-ontologies","title":"Hybridising ontologies","text":"<p>A broad, general ontology is a useful tool for knowledge representation, but often the task at hand requires more specific and granular concepts. In such cases, it is possible to hybridise the general ontology with a more specific one. For instance, there are many different types of sequence variants in biology, but Biolink only provides a generic \"sequence variant\" class (and it clearly exceeds the scope of Biolink to provide granular classes for all thinkable cases). However, there are many specialist ontologies, such as the Sequence Ontology (SO), which provides a more granular representation of sequence variants, and MONDO, which provides a more granular representation of diseases.</p> <p>To hybridise the Biolink model with the SO and MONDO, we can use the generic ontology adapter class of BioCypher by providing \"tail ontologies\" as dictionaries consisting of an OWL format ontology file and a set of nodes, one in the head ontology (which by default is Biolink), and one in the tail ontology. Each of the tail ontologies will then be joined to the head ontology to form the hybridised ontology at the specified nodes. It is up to the user to make sure that the concept at which the ontologies shall be joined makes sense as a point of contact between the ontologies; ideally, it is the exact same concept.</p> <p>Hint</p> <p>If the concept does not exist in the head ontology, but is a feasible child class of an existing concept, you can set the <code>merge_nodes</code> option to <code>False</code> to prevent the merging of head and tail join nodes, but instead adding the tail join node as a child of the head join node you have specified. For instance, in the example below, we merge <code>sequence variant</code> from Biolink and <code>sequence_variant</code> from Sequence Ontology into a single node, but we add the MONDO subtree of <code>human disease</code> as a child of <code>disease</code> in Biolink.</p> <p><code>merge_nodes</code> is set to <code>True</code> by default, so there is no need to specify it in the configuration file if you want to merge the nodes.</p> <p>The ontology adapter also accepts any arbitrary \"head ontology\" as a base ontology, but if none is provided, the Biolink model is used as the default head ontology. However, it is strongly recommended to explicitly specify your desired ontology version here. These options can be provided to the BioCypher interface as parameters, or as options in the BioCypher configuration file, which is the preferred method for transparency reasons:</p> Using biocypher_config.yaml<pre><code># ...\n\nbiocypher:  # biocypher settings\n\n  # Ontology configuration\n  head_ontology:\n    url: https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl\n    root_node: entity\n\n  tail_ontologies:\n\n    so:\n      url: data/so.owl\n      head_join_node: sequence variant\n      tail_join_node: sequence_variant\n\n    mondo:\n      url: http://purl.obolibrary.org/obo/mondo.owl\n      head_join_node: disease\n      tail_join_node: human disease\n      merge_nodes: false\n\n# ...\n</code></pre> <p>Note</p> <p>The <code>url</code> parameter can be either a local path or a URL to a remote resource.</p> <p>If you need to pass the ontology configuration programmatically, you can do so as follows at BioCypher interface instantiation:</p> Programmatic usage<pre><code>bc = BioCypher(\n    # ...\n\n    head_ontology={\n      'url': 'https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl',\n      'root_node': 'entity',\n    },\n\n    tail_ontologies={\n        'so':\n            {\n                'url': 'test/ontologies/so.owl',\n                'head_join_node': 'sequence variant',\n                'tail_join_node': 'sequence_variant',\n            },\n        'mondo':\n            {\n                'url': 'test/ontologies/mondo.owl',\n                'head_join_node': 'disease',\n                'tail_join_node': 'human disease',\n                'merge_nodes': False,\n            }\n    },\n\n    # ...\n)\n</code></pre>"},{"location":"learn/guides/outputs/","title":"Outputs","text":""},{"location":"learn/guides/outputs/#introduction","title":"Introduction","text":"<p>BioCypher development was initially centred around a Neo4j graph database output due to the migration of OmniPath to a Neo4j backend. Importantly, we understand BioCypher as an abstraction of the build process of a biomedical knowledge graph, and thus are open towards any output format for the knowledge representation.</p> <p>The used output format is specified via the <code>dbms</code> parameter in the <code>biocypher_config.yaml</code> (see the Configuration for an example). Currently supported are:</p> <ul> <li><code>neo4j</code></li> <li><code>arangodb</code></li> <li><code>rdf</code></li> <li><code>postgres</code></li> <li><code>sqlite</code></li> <li><code>tabular</code></li> <li><code>csv</code></li> <li><code>pandas</code></li> <li><code>networkx</code></li> </ul> <p>Furthermore, you can specify whether to use the <code>offline</code> or <code>online</code> mode.</p> <ul> <li> <p>For the online mode set <code>offline: false</code>. The behavior of the online mode   depends on the specified <code>dbms</code>. If the specified <code>dbms</code> is an in-memory   database (e.g. <code>csv</code>, <code>networkx</code>), the in-memory Knowledge Graph can   directly be accessed from the BioCypher instance. If the specified <code>dbms</code> is   a database (e.g. <code>neo4j</code>), the online mode requires a running database   instance and BioCypher will connect to this instance and directly writes the   output to the database.</p> </li> <li> <p>For the offline mode set <code>offline: true</code>. BioCypher will <code>write</code> the   knowledge graph to files in a designated output folder (standard being   <code>biocypher-out/</code> and the current datetime). Furthermore you can generate a   bash script to insert the knowledge graph files into the specified <code>dbms</code> by   running <code>bc.write_import_call()</code>.</p> </li> </ul> <p>Warning</p> <p>The <code>online</code> mode is currently only supported for <code>neo4j</code>, <code>tabular</code>, <code>csv</code>, <code>pandas</code>, and <code>networkx</code>.</p> <p>Details about the usage of the <code>online</code> and <code>offline</code> mode and the different supported output formats are described on the following pages:</p>"},{"location":"learn/guides/outputs/#available-output-formats","title":"Available Output formats","text":"Method Offline mode Online mode (in-memory) <code>Neo4j</code> <code>ArangoDB</code>  (pending) <code>RDF</code>  (pending) <code>PostgreSQL</code>  (pending) <code>SQLite</code>  (pending) <code>Tabular</code> <code>CSV</code> <code>Pandas</code> <code>NetworkX</code>"},{"location":"learn/guides/outputs/arangodb-output/","title":"ArangoDB","text":"<p>The documentation related to ArangoDB and BioCypher is under construction</p>"},{"location":"learn/guides/outputs/arangodb-output/#install-arangodb","title":"Install ArangoDB","text":""},{"location":"learn/guides/outputs/arangodb-output/#arangodb-settings","title":"ArangoDB settings","text":""},{"location":"learn/guides/outputs/arangodb-output/#offline-mode","title":"Offline mode","text":""},{"location":"learn/guides/outputs/arangodb-output/#online-mode","title":"Online mode","text":""},{"location":"learn/guides/outputs/neo4j-output/","title":"Neo4j","text":"<p>In the following section, we give an overview of interacting with Neo4j from the perspective of BioCypher, but we refer the reader to the Neo4j documentation for more details.</p>"},{"location":"learn/guides/outputs/neo4j-output/#install-neo4j","title":"Install Neo4j","text":"<p>Neo4j provide a Neo4j Desktop application that can be used to create a local instance of Neo4j. The desktop application provides information about the DBMS folder and can open a terminal at the DBMS root location.</p> <p>Neo4j is also available as a command line interface (CLI) tool. To use the CLI with the BioCypher admin import call, directory structures and permissions need to be set up correctly. The Neo4j CLI tool can be downloaded from the Neo4j download center. Please follow the Neo4j documentation for correct setup and usage on your system.</p> <p>Be mindful that different versions of Neo4j may differ in features and thus are also documented differently.</p> <p>Note</p> <p>We use the APOC library for Neo4j, which is not included automatically, but needs to be installed as a plugin to the DMBS. For more information, please refer to the APOC documentation.</p>"},{"location":"learn/guides/outputs/neo4j-output/#neo4j-settings","title":"Neo4j settings","text":"<p>To overwrite the standard settings of Neo4j, add a <code>neo4j</code> section to the <code>biocypher_config.yaml</code> file. The following settings are possible:</p> biocypher_config.yaml<pre><code>neo4j:  ### Neo4j configuration ###\n\n  # Database name\n  database_name: neo4j\n\n  # Wipe DB before import (offline mode: --force)\n  wipe: true\n\n  # Neo4j authentication\n  uri: neo4j://localhost:7687\n  user: neo4j\n  password: neo4j\n\n  # Neo4j admin import batch writer settings\n  delimiter: ';'\n  array_delimiter: '|'\n  quote_character: \"'\"\n\n  # How to write the labels in the export files.\n  labels_order: \"Ascending\" # Default: From more specific to more generic.\n  # Or:\n  # labels_order: \"Descending\" # From more generic to more specific.\n  # labels_order: \"Alphabetical\" # Alphabetically. Legacy option.\n  # labels_order: \"Leaves\" # Only the more specific label.\n\n\n  # MultiDB functionality\n  # Set to false for using community edition or older versions of Neo4j\n  multi_db: true\n\n  # Import options\n  skip_duplicate_nodes: false\n  skip_bad_relationships: false\n\n  # Import call prefixes to adjust the autogenerated shell script\n  import_call_bin_prefix: bin/  # path to \"neo4j-admin\"\n  import_call_file_prefix: path/to/files/\n</code></pre>"},{"location":"learn/guides/outputs/neo4j-output/#offline-mode","title":"Offline mode","text":"<p>Particularly if the data are very extensive (or performance is of the utmost priority), BioCypher can be used to facilitate a speedy and safe import of the data using the <code>neo4j-admin import</code> console command. Admin Import is a particularly fast method of writing data to a newly created graph database (the database needs to be completely empty) that gains most of its performance advantage from turning off safety features regarding input data consistency. Therefore, a sound and consistent representation of the nodes and edges in the graph is of high importance in this process, which is why the BioCypher export functionality has been specifically designed to perform type and content checking for all data to be written to the graph.</p> <p>Data input from the source database is exactly as in the case of interacting with a running database, with the data representation being converted to a series of CSV files in a designated output folder (standard being <code>biocypher-out/</code> and the current datetime).  BioCypher creates separate header and data files for all node and edge types to be represented in the graph. Additionally, it creates a file called <code>neo4j-admin-import-call.sh</code> containing the console command for creating a new database, which only has to be executed from the directory of the currently running Neo4j database.</p> <p>The name of the database to be created is given by the <code>db_name</code> setting, and can be arbitrary. In case the <code>db_name</code> is not the default Neo4j database name, <code>neo4j</code>, the database needs to be created in Neo4j before or after using the <code>neo4j-admin import</code> statement. This can be done by executing, in the running database (<code>&lt;db_name&gt;</code> being the name assigned in the method):</p> <ol> <li><code>:use system</code></li> <li><code>create database &lt;db_name&gt;</code></li> <li><code>:use &lt;db_name&gt;</code></li> </ol> <p>After writing knowledge graph files with BioCypher in offline mode for the Neo4j DBMS (database management system), the graph can now be imported into Neo4j using the <code>neo4j-admin</code> command line tool. This is not necessary if the graph is created in online mode. For convenience, BioCypher provides the command line call required to import the data into Neo4j:</p> <pre><code>bc.write_import_call()\n</code></pre> <p>This creates an executable shell script in the output directory that can be executed from the location of the database folder (or copied into the Neo4j terminal) to import the graph into Neo4j. Since BioCypher creates separate header and data files for each entity type, the import call conveniently aggregates this information into one command, detailing the location of all files on disk, so no data need to be copied around.</p> <p>Note</p> <p>The generated import call differs between Neo4j version 4 and 5. Starting from major version 5, Neo4j <code>import</code> command needs the <code>database</code> scope. BioCypher takes care of this. The generated import script <code>neo4j-admin-import-call.sh</code> first checks the Neo4j database version and uses the correct import statement for the detected version. Therefore make sure to run the script from the targeted DBMS root location.</p> <p>Neo4j can manage multiple projects, each with multiple DBMS (database management system) instances, each of which can house multiple databases. The screenshot below shows a project managed by Neo4j Desktop (project name \"BioCypher\") containing a DBMS instance (called \"Test DBMS\") with multiple named databases in it: the non-removable \"system\" DB, the default \"neo4j\" DB, and several user-created databases.</p> <p></p> <p>Crucially, the import call generated by BioCypher needs to be executed by the <code>neo4j-admin</code> binary in the DBMS folder (in the <code>bin/</code> directory of the root of the DBMS folder). The DBMS folder is the folder that contains the <code>data/</code> directory, which in turn contains the <code>databases/</code> folder, which is where the graph data is stored in individual folders corresponding to the DB names in the DBMS. On Neo4j Desktop, you can open a terminal at the DBMS root location by clicking on the three dots next to the DBMS name and selecting \"Terminal\" (see screenshot below).</p> <p></p> <p>Since the import call should be executed in the root of the DMBS folder, BioCypher generates the import call starting with <code>bin/neo4j-admin</code>, which can be copied into the terminal opened at the DBMS root location. For other operating systems and Neo4j installations (e.g. in Docker containers), please refer to the Neo4j documentation to find the correct location of your DMBS. We are working on extensions of the BioCypher process to improve interoperability with Neo4j as well as other storage systems.</p>"},{"location":"learn/guides/outputs/neo4j-output/#online-mode","title":"Online mode","text":"<p>BioCypher provides a Python driver for interacting with Neo4j, which is accessed through the <code>BioCypher</code> class when setting <code>offline</code> to <code>False</code>. More details can be found in the API docs.</p> <p>If there exists no BioCypher graph in the currently active database, or if the user explicitly specifies so using the <code>wipe</code> attribute of the driver, a new BioCypher database is created using the schema configuration specified in the schema-config.yaml.</p>"},{"location":"learn/guides/outputs/neo4j-output/#note-on-labels-order","title":"Note on labels order","text":"<p>Neo4j do not support managing the hierarchy of types of the vocabulary given by the input ontology. What it does is to attach to nodes and edges each type label of all the ancestors in the types hierarchy.</p> <p>By default, the Neo4j driver exports those type labels as an alphabetically-sorted list, which may be useful for comparing output files. But this may be less easy to understand within a graph browser that would show the labels in the same order.</p> <p>To get a more readable labels list, you can set <code>labels_order</code> as either <code>Ascending</code> or <code>Descending</code>; both of which will display labels in the order given by the type hierarchy of the ontology's vocabulary.</p> <p>To get even simpler label, you can set <code>labels_order: Leaves</code>, which will write down only the more specific type label (the \"leaf\" of the types tree). Be warn that the resulting export will completely lose the ontological information, hence making it impossible to query the graph on high-level types.</p>"},{"location":"learn/guides/outputs/networkx-output/","title":"NetworkX","text":"<p>When setting the <code>dbms</code> parameter in the <code>biocypher_config.yaml</code> to <code>networkx</code>, the BioCypher Knowledge Graph is transformed into a NetworkX DiGraph.</p>"},{"location":"learn/guides/outputs/networkx-output/#networkx-settings","title":"NetworkX settings","text":"<p>To overwrite the standard settings of NetworkX, add a <code>networkx</code> section to the <code>biocypher_config.yaml</code> file.  At the moment there are no configuration options supported/implemented.  Feel free to reach out and create issues or pull requests if you need specific configuration options.</p> biocypher_config.yaml<pre><code>networkx:\n  ### NetworkX configuration ###\n</code></pre>"},{"location":"learn/guides/outputs/networkx-output/#offline-mode","title":"Offline mode","text":""},{"location":"learn/guides/outputs/networkx-output/#running-biocypher","title":"Running BioCypher","text":"<p>After running BioCypher with the <code>offline</code> parameter set to <code>true</code> and the <code>dbms</code> set to <code>networkx</code>, the output folder contains:</p> <ul> <li> <p><code>networkx_graph.pkl</code>: The pickle file containing with the BioCypher Knowledge Graph as NetworkX DiGraph.</p> </li> <li> <p><code>import_networkx.py</code>: A Python script to load the created pickle file.</p> </li> </ul> <p>Note</p> <p>If any of the files is missing make sure to run <code>bc.write_import_call()</code>.</p>"},{"location":"learn/guides/outputs/networkx-output/#online-mode","title":"Online mode","text":"<p>After running BioCypher with the <code>offline</code> parameter set to <code>false</code> and the <code>dbms</code> set to <code>networkx</code>, you can get the in-memory networkx representation of the Knowledge Graph directly from BioCypher by calling the <code>get_kg()</code> function.</p> <pre><code># Initialize BioCypher\nbc = BioCypher(\n    biocypher_config_path=\"biocypher_config.yaml\",\n    schema_config_path=\"schema_config.yaml\",\n)\n\n# Add nodes and edges\nbc.add_nodes(nodes)\nbc.add_edges(edges)\n\n# Get the in-memory representation of the Knowledge Graph\nin_memory_kg = bc.get_kg()\n</code></pre>"},{"location":"learn/guides/outputs/postgresql-output/","title":"PostgreSQL","text":"<p>When setting the <code>dbms</code> parameter in the <code>biocypher_config.yaml</code> to <code>postgres</code>, the BioCypher Knowledge Graph is written to a PostgreSQL database. PostgreSQL is a relational database management system.</p>"},{"location":"learn/guides/outputs/postgresql-output/#install-postgresql","title":"Install PostgreSQL","text":"<p>To get a PostgreSQL instance running quickly, you can use Docker. The following command will start a PostgreSQL instance with the password <code>postgres</code> and the port <code>5432</code> exposed to the host system.</p> <pre><code>docker run --restart always \\\n           --publish=5432:5432 \\\n           --env POSTGRES_PASSWORD=postgres \\\n           -d postgres\n</code></pre> <p>The <code>postgresql-client</code> (also known as <code>psql</code> command line tool) can be used to connect and interact with the running PostgreSQL database instance. Installation instructions can be found here.</p>"},{"location":"learn/guides/outputs/postgresql-output/#postgresql-settings","title":"PostgreSQL settings","text":"<p>To overwrite the standard settings of PostgreSQL, add a <code>postgresql</code> section to the <code>biocypher_config.yaml</code> file. The following settings are possible:</p> biocypher_config.yaml<pre><code>postgresql:  ### PostgreSQL configuration ###\n\n  # PostgreSQL connection credentials\n  database_name: postgres\n  user: postgres\n  password: postgres\n  port: 5432\n\n  # PostgreSQL import batch writer settings\n  quote_character: '\"'\n  delimiter: '\\t'\n\n  # Import call prefixes to adjust the autogenerated shell script\n  import_call_bin_prefix: ''  # path to \"psql\"\n  import_call_file_prefix: /path/to/files/\n</code></pre>"},{"location":"learn/guides/outputs/postgresql-output/#offline-mode","title":"Offline mode","text":""},{"location":"learn/guides/outputs/postgresql-output/#running-biocypher","title":"Running BioCypher","text":"<p>After running BioCypher with the <code>offline</code> parameter set to <code>true</code> and the <code>dbms</code> set to <code>postgres</code>, the output folder contains:</p> <ul> <li> <p><code>entity-create_table.sql</code>: The SQL scripts to create the tables for the nodes/edges. Entity is replaced by your nodes and edges and for each node and edge type an own SQL script is generated.</p> </li> <li> <p><code>entity-part000.csv</code>: The CSV file containing the data for the entity.</p> </li> <li> <p><code>postgres-import-call.sh</code>: The import script to create a database with the SQL scripts and insert the data from the CSV files.</p> </li> </ul> <p>Note</p> <p>If the <code>postgres-import-call.sh</code> is missing, you can create it by running <code>bc.write_import_call()</code>.</p>"},{"location":"learn/guides/outputs/postgresql-output/#create-the-postgresql-database","title":"Create the PostgreSQL database","text":"<p>A running PostgreSQL instance (e.g. in a Docker container created with the command from above) is required to create the database and import the data. By running the import script <code>postgres-import-call.sh</code>, the content should be written to the PostgreSQL database.</p>"},{"location":"learn/guides/outputs/postgresql-output/#access-the-postgresql-database","title":"Access the PostgreSQL database","text":"<p>To check the content of the database, you can use the <code>psql</code> command line tool.</p> <p>First connect to the running PostgreSQL database instance: <pre><code>psql -h localhost -p 5432 -U postgres\n</code></pre></p> <p>Then you can execute SQL queries. For example, you can list all tables in the database by running the following command in the terminal: <pre><code>SELECT table_name\nFROM information_schema.tables\nWHERE table_schema = 'public';\n</code></pre></p>"},{"location":"learn/guides/outputs/rdf-output/","title":"Resource Description Framework (RDF)","text":"<p>In this section, we will learn how to use and implement the output to RDF using the <code>_RDFWriter</code> module.</p>"},{"location":"learn/guides/outputs/rdf-output/#rdf-settings","title":"RDF settings","text":"<p>To write your output to RDF, you have to specify some RDF settings in the <code>biocypher_config.yaml</code>. Using <code>rdf_format</code>, you can choose to export to <code>XML</code>, <code>Turtle</code> or any other format <code>rdflib</code> supports. The second configuration is the <code>rdf_namespaces</code>, where you can specify which namespaces exist in your data. If, for instance, your data contain SO (Sequence ontology) terms such as <code>SO:0000001</code>, IDs will be converted into valid URIs to allow referencing. Thus, <code>SO:0000001</code> will be converted into <code>http://purl.obolibrary.org/obo/SO_0000001</code>. When a node cannot be converted, a default URI will be used (<code>https://biocypher.org/biocypher#&lt;node_id&gt;</code>). Running the pipeline, the <code>_RDFWriter</code> will create a file for every node and relationship type you have specified.</p> biocypher_config.yaml<pre><code>biocypher:\n  strict_mode: true\n  schema_config_path: config/schema_config.yaml\n  dbms: rdf\n\n### RDF configuration ###\nrdf:\n  rdf_format: turtle\n  # options: xml, n3, turtle, nt, pretty-xml, trix, trig, nquads, json-ld\n  rdf_namespaces:\n    so: http://purl.obolibrary.org/obo/SO_\n    efo: http://www.ebi.ac.uk/efo/EFO_\n</code></pre>"},{"location":"learn/guides/outputs/sqlite-output/","title":"SQLite","text":"<p>When setting the <code>dbms</code> parameter in the <code>biocypher_config.yaml</code> to <code>sqlite</code>, the BioCypher Knowledge Graph is written to a SQLite database. SQLite is a lightweight relational database management system. It is suitable for fast prototyping and development. For more mature applications have a look at PostgreSQL.</p>"},{"location":"learn/guides/outputs/sqlite-output/#sqlite-settings","title":"SQLite settings","text":"<p>To overwrite the standard settings of SQLite, add a <code>sqlite</code> section to the <code>biocypher_config.yaml</code> file. The following settings are possible:</p> biocypher_config.yaml<pre><code>sqlite:\n  ### SQLite configuration ###\n\n  database_name: sqlite.db # DB name\n\n  # SQLite import batch writer settings\n  quote_character: '\"'\n  delimiter: '\\t'\n  # import_call_bin_prefix: '' # path to \"sqlite3\"\n  # import_call_file_prefix: '/path/to/files'\n</code></pre>"},{"location":"learn/guides/outputs/sqlite-output/#offline-mode","title":"Offline mode","text":""},{"location":"learn/guides/outputs/sqlite-output/#running-biocypher","title":"Running BioCypher","text":"<p>After running BioCypher with the <code>offline</code> parameter set to <code>true</code> and the <code>dbms</code> set to <code>sqlite</code>, the output folder contains:</p> <ul> <li> <p><code>entity-create_table.sql</code>: The SQL scripts to create the tables for the nodes/edges. Entity is replaced by your nodes and edges and for each node and edge type an own SQL script is generated.</p> </li> <li> <p><code>entity-part000.csv</code>: The CSV file containing the data for the entity.</p> </li> <li> <p><code>sqlite-import-call.sh</code>: The import script to create a database with the SQL scripts and insert the data from the CSV files.</p> </li> </ul> <p>Note</p> <p>If the <code>sqlite-import-call.sh</code> is missing, you can create it by running <code>bc.write_import_call()</code>.</p>"},{"location":"learn/guides/outputs/sqlite-output/#create-the-sqlite-database","title":"Create the SQLite database","text":"<p>To create the SQLite database, run the import script <code>sqlite-import-call.sh</code>. In the default case (without any changes to the <code>database_name</code>in the configuration), the file containing the database is created with the name <code>sqlite.db</code>.</p> <p>Note</p> <p>The import script expects, that the sqlite3 command line tool is installed on your system.</p>"},{"location":"learn/guides/outputs/sqlite-output/#access-the-sqlite-database","title":"Access the SQLite database","text":"<p>Now you can access the created SQLite database. This can be done with the sqlite3 command line tool. For example, you can list all tables in the database by running the following command in the terminal:</p> <pre><code>sqlite3 sqlite.db \"SELECT name FROM sqlite_master WHERE type='table';\"\n</code></pre>"},{"location":"learn/guides/outputs/tabular-output/","title":"Tabular output","text":"<p>When setting the <code>dbms</code> parameter in the <code>biocypher_config.yaml</code> to <code>tabular</code>, <code>csv</code>, or <code>pandas</code>, the BioCypher Knowledge Graph is created in one of several possible tabular formats.</p>"},{"location":"learn/guides/outputs/tabular-output/#tabular-output-settings","title":"Tabular output settings","text":"<p>To overwrite the standard settings of the CSV writer, add a <code>csv</code> section to the <code>biocypher_config.yaml</code> file. The following settings are possible:</p> biocypher_config.yaml<pre><code>csv:\n  ### CSV/Pandas configuration ###\n  delimiter: ','  # The delimiter to be used in the CSV files. Default is ','.\n</code></pre>"},{"location":"learn/guides/outputs/tabular-output/#offline-mode","title":"Offline mode","text":""},{"location":"learn/guides/outputs/tabular-output/#running-biocypher","title":"Running BioCypher","text":"<p>After running BioCypher with the <code>offline</code> parameter set to <code>true</code> and the <code>dbms</code> set to <code>tabular</code>, <code>csv</code>, or <code>pandas</code>, the output folder contains:</p> <ul> <li> <p><code>*.csv</code>: The CSV files containing the node/edge data.</p> </li> <li> <p><code>import_pandas_csv.csv</code>: A Python script to load the created CSV files into Pandas DataFrames.</p> </li> </ul>"},{"location":"learn/guides/outputs/tabular-output/#online-mode","title":"Online mode","text":"<p>After running BioCypher with the <code>offline</code> parameter set to <code>false</code> and the <code>dbms</code> set to <code>tabular</code>, <code>csv</code>, or <code>pandas</code>, you can get the in-memory representation of the Knowledge Graph directly from BioCypher by calling the <code>get_kg()</code> function. This returns a dictionary with the corresponding data type (e.g., <code>Pandas</code> dataframes) for every node and edge type.</p> <pre><code># Initialize BioCypher\nbc = BioCypher(\n    biocypher_config_path=\"biocypher_config.yaml\",\n    schema_config_path=\"schema_config.yaml\",\n)\n\n# Add nodes and edges\nbc.add_nodes(nodes)\nbc.add_edges(edges)\n\n# Get the in-memory representation of the Knowledge Graph\nin_memory_kg = bc.get_kg()\n</code></pre>"},{"location":"learn/tutorials/","title":"Tutorials","text":"<p>The BioCypher tutorials are written as Jupyter notebooks and run directly in Google Colab\u2014a hosted notebook environment that requires no setup. At the top of each tutorial, you'll see a Run in Google Colab button. Click the button to open the notebook and run the code yourself.</p>"},{"location":"learn/tutorials/#list-of-tutorials-current-status","title":"List of Tutorials (current status)","text":"<ol> <li>Getting started</li> <li>Example Notebook: BioCypher and Pandas</li> <li>Tutorial - Basics</li> <li>Tutorial - Handling Ontologies</li> <li>Tutorial - Adapters</li> </ol>"},{"location":"learn/tutorials/pandas_tutorial/","title":"Example Notebook: BioCypher and Pandas","text":"Tip: Run the tutorial interactively in      Google Colab.  <p>While BioCypher was designed as a graph-focused framework, due to commonalities in bioinformatics workflows, BioCypher also supports Pandas DataFrames. This allows integration with methods that use tabular data, such as machine learning and statistical analysis, for instance in the scVerse framework.</p> <p>To run this tutorial interactively, you will first need to install perform some setup steps specific to running on Google Colab. You can collapse this section and run the setup steps with one click, as they are not required for the explanation of BioCyper's functionality. You can of course also run the steps one by one, if you want to see what is happening. The real tutorial starts with section 1, \"Adding data\" (do not follow this link on colab, as you will be taken back to the website; please scroll down instead).</p> In\u00a0[\u00a0]: Copied! <pre>!pip install biocypher\n</pre> !pip install biocypher In\u00a0[\u00a0]: Copied! <pre>import yaml\nimport requests\nimport subprocess\n\nschema_path = \"https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/\"\n</pre> import yaml import requests import subprocess  schema_path = \"https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/\" In\u00a0[\u00a0]: Copied! <pre>!wget -O data_generator.py \"https://github.com/biocypher/biocypher/raw/main/tutorial/data_generator.py\"\n</pre> !wget -O data_generator.py \"https://github.com/biocypher/biocypher/raw/main/tutorial/data_generator.py\" In\u00a0[\u00a0]: Copied! <pre>owner = \"biocypher\"\nrepo = \"biocypher\"\npath = \"tutorial\"  # The path within the repository (optional, leave empty for the root directory)\ngithub_url = \"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n\napi_url = github_url.format(owner=owner, repo=repo, path=path)\nresponse = requests.get(api_url)\n\n# Get list of yaml files from the repo\nfiles = response.json()\nyamls = []\nfor file in files:\n    if file[\"type\"] == \"file\":\n        if file[\"name\"].endswith(\".yaml\"):\n            yamls.append(file[\"name\"])\n\n# wget all yaml files\nfor yaml in yamls:\n    url_path = schema_path + yaml\n    subprocess.run([\"wget\", url_path])\n</pre> owner = \"biocypher\" repo = \"biocypher\" path = \"tutorial\"  # The path within the repository (optional, leave empty for the root directory) github_url = \"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"  api_url = github_url.format(owner=owner, repo=repo, path=path) response = requests.get(api_url)  # Get list of yaml files from the repo files = response.json() yamls = [] for file in files:     if file[\"type\"] == \"file\":         if file[\"name\"].endswith(\".yaml\"):             yamls.append(file[\"name\"])  # wget all yaml files for yaml in yamls:     url_path = schema_path + yaml     subprocess.run([\"wget\", url_path]) <p>Let's also define functions with which we can visualize those</p> In\u00a0[\u00a0]: Copied! <pre># helper function to print yaml files\nimport yaml\ndef print_yaml(file_path):\n    with open(file_path, 'r') as file:\n        yaml_data = yaml.safe_load(file)\n\n    print(\"--------------\")\n    print(yaml.dump(yaml_data, sort_keys=False, indent=4))\n    print(\"--------------\")\n</pre> # helper function to print yaml files import yaml def print_yaml(file_path):     with open(file_path, 'r') as file:         yaml_data = yaml.safe_load(file)      print(\"--------------\")     print(yaml.dump(yaml_data, sort_keys=False, indent=4))     print(\"--------------\") In\u00a0[\u00a0]: Copied! <pre># create a list of proteins to be imported\nfrom data_generator import Protein\nn_proteins = 3\nproteins = [Protein() for _ in range(n_proteins)]\n</pre> # create a list of proteins to be imported from data_generator import Protein n_proteins = 3 proteins = [Protein() for _ in range(n_proteins)] <p>Each protein in our simulated data has a UniProt ID, a label (\"uniprot_protein\"), and a dictionary of properties describing it. This is - purely by coincidence - very close to the input BioCypher expects (for nodes):</p> <ul> <li>a unique identifier</li> <li>an input label (to allow mapping to the ontology, see the second step below)</li> <li>a dictionary of further properties (which can be empty)</li> </ul> <p>These should be presented to BioCypher in the form of a tuple. To achieve this representation, we can use a generator function that iterates through our simulated input data and, for each entity, forms the corresponding tuple. The use of a generator allows for efficient streaming of larger datasets where required.</p> In\u00a0[\u00a0]: Copied! <pre>def node_generator(proteins):\n    for protein in proteins:\n        yield (\n            protein.get_id(),\n            protein.get_label(),\n            protein.get_properties(),\n        )\nentities = node_generator(proteins)\n</pre> def node_generator(proteins):     for protein in proteins:         yield (             protein.get_id(),             protein.get_label(),             protein.get_properties(),         ) entities = node_generator(proteins) <p>The concept of an adapter can become arbitrarily complex and involve programmatic access to databases, API requests, asynchronous queries, context managers, and other complicating factors. However, it always boils down to providing the BioCypher driver with a collection of tuples, one for each entity in the input data. For more info, see the section on Adapters.</p> <p>As descibed above, nodes possess:</p> <ul> <li>a mandatory ID,</li> <li>a mandatory label, and</li> <li>a property dictionary,</li> </ul> <p>while edges possess:</p> <ul> <li>an (optional) ID,</li> <li>two mandatory IDs for source and target,</li> <li>a mandatory label, and</li> <li>a property dictionary.</li> </ul> <p>How these entities are mapped to the ontological hierarchy underlying a BioCypher graph is determined by their mandatory labels, which connect the input data stream to the schema configuration. This we will see in the following section.</p> In\u00a0[\u00a0]: Copied! <pre>print_yaml('01_schema_config.yaml')\n</pre> print_yaml('01_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id: uniprot\n    input_label: uniprot_protein\n\n--------------\n</pre> <p>The first line (<code>protein</code>) identifies our entity and connects to the ontological backbone; here we define the first class to be represented in the graph. In the configuration YAML, we represent entities\u202f\u2014 similar to the internal representation of Biolink\u202f\u2014 in lower sentence case (e.g., \"small molecule\"). Conversely, for class names, in file names, and property graph labels, we use PascalCase instead (e.g., \"SmallMolecule\") to avoid issues with handling spaces. The transformation is done by BioCypher internally. BioCypher does not strictly enforce the entities allowed in this class definition; in fact, we provide several methods of extending the existing ontological backbone ad hoc by providing custom inheritance or hybridising ontologies. However, every entity should at some point be connected to the underlying ontology, otherwise the multiple hierarchical labels will not be populated. Following this first line are three indented values of the protein class.</p> <p>The second line (<code>represented_as</code>) tells BioCypher in which way each entity should be represented in the graph; the only options are <code>node</code> and <code>edge</code>. Representation as an edge is only possible when source and target IDs are provided in the input data stream. Conversely, relationships can be represented as both <code>node</code> or <code>edge</code>, depending on the desired output. When a relationship should be represented as a node, i.e., \"reified\", BioCypher takes care to create a set of two edges and a node in place of the relationship. This is useful when we want to connect the relationship to other entities in the graph, for example literature references.</p> <p>The third line (<code>preferred_id</code>) informs the uniqueness of represented entities by selecting an ontological namespace around which the definition of uniqueness should revolve. In our example, if a protein has its own uniprot ID, it is understood to be a unique entity. When there are multiple protein isoforms carrying the same uniprot ID, they are understood to be aggregated to result in only one unique entity in the graph. Decisions around uniqueness of graph constituents sometimes require some consideration in task-specific applications. Selection of a namespace also has effects in identifier mapping; in our case, for protein nodes that do not carry a uniprot ID, identifier mapping will attempt to find a uniprot ID given the other identifiers of that node. To account for the broadest possible range of identifier systems while also dealing with parsing of namespace prefixes and validation, we refer to the Bioregistry project namespaces, which should be preferred values for this field.</p> <p>Finally, the fourth line (<code>input_label</code>) connects the input data stream to the configuration; here we indicate which label to expect in the input tuple for each class in the graph. In our case, we expect \"uniprot_protein\" as the label for each protein in the input data stream; all other input entities that do not carry this label are ignored as long as they are not in the schema configuration.</p> In\u00a0[\u00a0]: Copied! <pre>from biocypher import BioCypher\nbc = BioCypher(\n    biocypher_config_path='01_biocypher_config.yaml',\n    schema_config_path='01_schema_config.yaml',\n)\n# Add the entities that we generated above to the graph\nbc.add(entities)\n</pre> from biocypher import BioCypher bc = BioCypher(     biocypher_config_path='01_biocypher_config.yaml',     schema_config_path='01_schema_config.yaml', ) # Add the entities that we generated above to the graph bc.add(entities) <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> In\u00a0[\u00a0]: Copied! <pre># Print the graph as a dictionary of pandas DataFrame(s) per node label\nbc.to_df()[\"protein\"]\n</pre> # Print the graph as a dictionary of pandas DataFrame(s) per node label bc.to_df()[\"protein\"] Out[\u00a0]: <pre>{'protein':   protein                                           sequence  \\\n 0  F7V4U2  RMFDDRFPVELRICTGSLVIINLGEFAEQHDKQDGSKPSHQPMFAT...   \n 1  K2Y8U3  HWPPSGVSCGVFPECWYRWRDEQWACFGPHIKYNKDNTWSWAQWMH...   \n 2  L1V6V9  QAEPKYKLAQENCRVQIKLPKIVGTCRPHWMTKTYHVLHTCVLWKS...   \n \n            description taxon      id preferred_id  \n 0  i f c m m q e o o s  9606  F7V4U2      uniprot  \n 1  e y p g j t j y r x  9606  K2Y8U3      uniprot  \n 2  a i b t l j e g n j  9606  L1V6V9      uniprot  }</pre> In\u00a0[\u00a0]: Copied! <pre>from data_generator import Protein, EntrezProtein\n</pre> from data_generator import Protein, EntrezProtein In\u00a0[\u00a0]: Copied! <pre>print_yaml('02_schema_config.yaml')\n</pre> print_yaml('02_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id: uniprot\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n\n--------------\n</pre> In\u00a0[\u00a0]: Copied! <pre># Create a list of proteins to be imported\nproteins = [\n    p for sublist in zip(\n        [Protein() for _ in range(n_proteins)],\n        [EntrezProtein() for _ in range(n_proteins)],\n    ) for p in sublist\n]\n# Create a new BioCypher instance\nbc = BioCypher(\n    biocypher_config_path='02_biocypher_config.yaml',\n    schema_config_path='02_schema_config.yaml',\n)\n# Run the import\nbc.add(node_generator(proteins))\n</pre> # Create a list of proteins to be imported proteins = [     p for sublist in zip(         [Protein() for _ in range(n_proteins)],         [EntrezProtein() for _ in range(n_proteins)],     ) for p in sublist ] # Create a new BioCypher instance bc = BioCypher(     biocypher_config_path='02_biocypher_config.yaml',     schema_config_path='02_schema_config.yaml', ) # Run the import bc.add(node_generator(proteins)) <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> In\u00a0[\u00a0]: Copied! <pre>bc.to_df()[\"protein\"]\n</pre> bc.to_df()[\"protein\"] Out[\u00a0]: <pre>{'protein':   protein                                           sequence  \\\n 0  K2W3K5  TVKISILFNPLPNQDMNTTTCQAESNYKAIYLYPWCSMDDVWNVEA...   \n 1  186009  FHYHGGMGPFMTYQNFLHWEQMQPMKLFNEPMQFHDWYGTHVNWPG...   \n 2  S6E6D1  CSVQIQIGMSQDSPDSSEGNMDCPPRNIGGYEIVCNVQGKRCYSTD...   \n 3  926766  HKEAELLVKGQIQTPKCLRHNHFYAKLTIVIELNYMVDRYGKDMAR...   \n 4  Z1F6R2  FMVWKDCLCIRMRHMAVPVPQYHCEYFEVILERWEVPCFSVLNRCK...   \n 5  362641  PISDEQEMGSEFCGHCNTGVYQVEMHFFECEDLNPKVQPKWIFTVT...   \n \n            description taxon      id preferred_id  \n 0  e e v h x f t f j l  9606  K2W3K5      uniprot  \n 1  b c q m l d a u u g  9606  186009      uniprot  \n 2  i z t s l x v g j l  9606  S6E6D1      uniprot  \n 3  t n a j d l j a t a  9606  926766      uniprot  \n 4  h d m k q n r e h r  9606  Z1F6R2      uniprot  \n 5  l m x k h m v g p y  9606  362641      uniprot  }</pre> <p>This again creates a single DataFrame, now for both protein types, but now including both input streams (you should note both uniprot &amp; entrez style IDs in the id column). However, we are generating our <code>entrez</code> proteins as having entrez IDs, which could result in problems in querying. Additionally, a strict import mode including regex pattern matching of identifiers will fail at this point due to the difference in pattern of UniProt vs. Entrez IDs. This issue could be resolved by mapping the Entrez IDs to UniProt IDs, but we will instead use the opportunity to demonstrate how to merge data from different sources into the same ontological class using ad hoc subclasses.</p> <p>In the previous section, we saw how to merge data from different sources into the same ontological class. However, we did not resolve the issue of the <code>entrez</code> proteins living in a different namespace than the <code>uniprot</code> proteins, which could result in problems in querying. In proteins, it would probably be more appropriate to solve this problem using identifier mapping, but in other categories, e.g., pathways, this may not be possible because of a lack of one-to-one mapping between different data sources. Thus, if we so desire, we can merge datasets into the same ontological class by creating ad hoc subclasses implicitly through BioCypher, by providing multiple preferred identifiers. In our case, we update our schema configuration as follows:</p> In\u00a0[\u00a0]: Copied! <pre>print_yaml('03_schema_config.yaml')\n</pre> print_yaml('03_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id:\n    - uniprot\n    - entrez\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n\n--------------\n</pre> <p>This will \"implicitly\" create two subclasses of the <code>protein</code> class, which will inherit the entire hierarchy of the <code>protein</code> class. The two subclasses will be named using a combination of their preferred namespace and the name of the parent class, separated by a dot, i.e., <code>uniprot.protein</code> and <code>entrez.protein</code>. In this manner, they can be identified as proteins regardless of their sources by any queries for the generic <code>protein</code> class, while still carrying information about their namespace and avoiding identifier conflicts.</p>  The only change affected upon the code from the previous section is the referral to the updated schema configuration file.   In the output, we now generate two separate files for the `protein` class, one for each subclass (with names in PascalCase).  <p>Let's create a DataFrame with the same nodes as above, but with a different schema configuration:</p> In\u00a0[\u00a0]: Copied! <pre>bc = BioCypher(\n    biocypher_config_path='03_biocypher_config.yaml',\n    schema_config_path='03_schema_config.yaml',\n)\nbc.add(node_generator(proteins))\nfor name, df in bc.to_df().items():\n    print(name)\n    display(df)\n</pre> bc = BioCypher(     biocypher_config_path='03_biocypher_config.yaml',     schema_config_path='03_schema_config.yaml', ) bc.add(node_generator(proteins)) for name, df in bc.to_df().items():     print(name)     display(df) <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> Out[\u00a0]: <pre>{'uniprot.protein':   uniprot.protein                                           sequence  \\\n 0          K2W3K5  TVKISILFNPLPNQDMNTTTCQAESNYKAIYLYPWCSMDDVWNVEA...   \n 1          S6E6D1  CSVQIQIGMSQDSPDSSEGNMDCPPRNIGGYEIVCNVQGKRCYSTD...   \n 2          Z1F6R2  FMVWKDCLCIRMRHMAVPVPQYHCEYFEVILERWEVPCFSVLNRCK...   \n \n            description taxon      id preferred_id  \n 0  e e v h x f t f j l  9606  K2W3K5      uniprot  \n 1  i z t s l x v g j l  9606  S6E6D1      uniprot  \n 2  h d m k q n r e h r  9606  Z1F6R2      uniprot  ,\n 'entrez.protein':   entrez.protein                                           sequence  \\\n 0         186009  FHYHGGMGPFMTYQNFLHWEQMQPMKLFNEPMQFHDWYGTHVNWPG...   \n 1         926766  HKEAELLVKGQIQTPKCLRHNHFYAKLTIVIELNYMVDRYGKDMAR...   \n 2         362641  PISDEQEMGSEFCGHCNTGVYQVEMHFFECEDLNPKVQPKWIFTVT...   \n \n            description taxon      id preferred_id  \n 0  b c q m l d a u u g  9606  186009       entrez  \n 1  t n a j d l j a t a  9606  926766       entrez  \n 2  l m x k h m v g p y  9606  362641       entrez  }</pre> <p>Now we see two separate DataFrames, one for each subclass of the <code>protein</code> class.</p> In\u00a0[\u00a0]: Copied! <pre>print_yaml('04_schema_config.yaml')\n</pre> print_yaml('04_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id:\n    - uniprot\n    - entrez\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n    properties:\n        sequence: str\n        description: str\n        taxon: str\n        mass: int\n\n--------------\n</pre> <p>This will add the <code>mass</code> property to all proteins (in addition to the three we had before); if not encountered, the column will be empty. Implicit subclasses will automatically inherit the property configuration; in this case, both <code>uniprot.protein</code> and <code>entrez.protein</code> will have the <code>mass</code> property, even though the <code>entrez</code> proteins do not have a <code>mass</code> value in the input data.</p>  If we wanted to ignore the mass value for all properties, we could simply remove the `mass` key from the `properties` dictionary.  In\u00a0[\u00a0]: Copied! <pre>from data_generator import EntrezProtein, RandomPropertyProtein\n</pre> from data_generator import EntrezProtein, RandomPropertyProtein In\u00a0[\u00a0]: Copied! <pre># Create a list of proteins to be imported (now with properties)\nproteins = [\n    p for sublist in zip(\n        [RandomPropertyProtein() for _ in range(n_proteins)],\n        [EntrezProtein() for _ in range(n_proteins)],\n    ) for p in sublist\n]\n# New instance, populated, and to DataFrame\nbc = BioCypher(\n    biocypher_config_path='04_biocypher_config.yaml',\n    schema_config_path='04_schema_config.yaml',\n)\nbc.add(node_generator(proteins))\nfor name, df in bc.to_df().items():\n    print(name)\n    display(df)\n</pre> # Create a list of proteins to be imported (now with properties) proteins = [     p for sublist in zip(         [RandomPropertyProtein() for _ in range(n_proteins)],         [EntrezProtein() for _ in range(n_proteins)],     ) for p in sublist ] # New instance, populated, and to DataFrame bc = BioCypher(     biocypher_config_path='04_biocypher_config.yaml',     schema_config_path='04_schema_config.yaml', ) bc.add(node_generator(proteins)) for name, df in bc.to_df().items():     print(name)     display(df) <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> Out[\u00a0]: <pre>{'uniprot.protein':   uniprot.protein                                           sequence  \\\n 0          S1Z9L5  RHLRGDVMQEDHHTSSERMVYNVLPQDYKVVSCEYWNTQVTALWVI...   \n 1          W9J5F1  IPFSQSAWAQQRIGPKGTKAHGVTQPAPMDIKNLCNLTDLTLILDF...   \n 2          T1J3U0  WFGCCHKQYVSHVIDRQDPQSPSDNPSLVSQLQFFMWGIQIQNGEI...   \n \n            description taxon  mass      id preferred_id  \n 0  u x e o k m a i o s  3899  None  S1Z9L5      uniprot  \n 1  i x k c r b p d d p  8873  None  W9J5F1      uniprot  \n 2  m a w r r u x c w o  1966  9364  T1J3U0      uniprot  ,\n 'entrez.protein':   entrez.protein                                           sequence  \\\n 0         405878  RMTDGFEWQLDFHAFIWCNQAAWQLPLEVHISQGNGGWRMGLYGNM...   \n 1         154167  CGMNYDNGYFSVAYQSYDLWYHQQLKTRGVKPAEKDSDKDLGIDVI...   \n 2         234189  GQWQECIQGFTPQQMCVDCCAETKLANKSYYHSWMTWRLSGLCFNM...   \n \n            description taxon  mass      id preferred_id  \n 0  y c s v s n e c h o  9606  None  405878       entrez  \n 1  i k n c e n r n c d  9606  None  154167       entrez  \n 2  o v w y g h y e v y  9606  None  234189       entrez  }</pre> In\u00a0[\u00a0]: Copied! <pre>from data_generator import RandomPropertyProteinIsoform\n</pre> from data_generator import RandomPropertyProteinIsoform In\u00a0[\u00a0]: Copied! <pre>print_yaml('05_schema_config.yaml')\n</pre> print_yaml('05_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id:\n    - uniprot\n    - entrez\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n    properties:\n        sequence: str\n        description: str\n        taxon: str\n        mass: int\nprotein isoform:\n    is_a: protein\n    inherit_properties: true\n    represented_as: node\n    preferred_id: uniprot\n    input_label: uniprot_isoform\n\n--------------\n</pre> <p>This allows maintenance of property lists for many classes at once. If the child class has properties already, they will be kept (if they are not present in the parent class) or replaced by the parent class properties (if they are present).</p> <p>Again, apart from adding the protein isoforms to the input stream, the code for this example is identical to the previous one except for the reference to the updated schema configuration.</p> <p>We now create three separate DataFrames, all of which are children of the <code>protein</code> class; two implicit children (<code>uniprot.protein</code> and <code>entrez.protein</code>) and one explicit child (<code>protein isoform</code>).</p> In\u00a0[\u00a0]: Copied! <pre># create a list of proteins to be imported\nproteins = [\n    p for sublist in zip(\n        [RandomPropertyProtein() for _ in range(n_proteins)],\n        [RandomPropertyProteinIsoform() for _ in range(n_proteins)],\n        [EntrezProtein() for _ in range(n_proteins)],\n    ) for p in sublist\n]\n\n# Create BioCypher driver\nbc = BioCypher(\n    biocypher_config_path='05_biocypher_config.yaml',\n    schema_config_path='05_schema_config.yaml',\n)\n# Run the import\nbc.add(node_generator(proteins))\n\nfor name, df in bc.to_df().items():\n    print(name)\n    display(df)\n</pre> # create a list of proteins to be imported proteins = [     p for sublist in zip(         [RandomPropertyProtein() for _ in range(n_proteins)],         [RandomPropertyProteinIsoform() for _ in range(n_proteins)],         [EntrezProtein() for _ in range(n_proteins)],     ) for p in sublist ]  # Create BioCypher driver bc = BioCypher(     biocypher_config_path='05_biocypher_config.yaml',     schema_config_path='05_schema_config.yaml', ) # Run the import bc.add(node_generator(proteins))  for name, df in bc.to_df().items():     print(name)     display(df) <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> <pre>uniprot.protein\n  uniprot.protein                                           sequence  \\\n0          A9L6G4  SWIVVGQPDSHNKRLVNYHWMRCEHPLRCWRPIYVVRVSFQSQCEQ...   \n1          E4N2H2  PGVMILDNMQHKCSKELSTRQIITNHWICNSAPISWSSGMDRSCLD...   \n2          V4F1T1  DQCHNLCPGSSFQCPENAFGNDWIDHMPQETGLMQYDDPQSGMWFT...   \n\n           description taxon  mass      id preferred_id  \n0  m o k j a f w v w r  4220  None  A9L6G4      uniprot  \n1  n v i r s f m f d w  6339  6481  E4N2H2      uniprot  \n2  w e v v a b o b b u  9176  6510  V4F1T1      uniprot  \nprotein isoform\n  protein isoform                                           sequence  \\\n0          F0N9A4  QDVVLVEGCGDEGWIHMPEKRPGQAYKWCERFRPIPDFTNSIKIAY...   \n1          B1W6O2  SQKHFRRWWTNDCFGQELMSIYYNVKFWDNLIEMTGGPASRVCLGQ...   \n2          G6V5R9  ASAITPFSYEKPHTVTLDATEVFPKMQDAQAIEREIHFSKSTLVYG...   \n\n           description taxon  mass      id preferred_id  \n0  r f e a v a a g w r  8061  None  F0N9A4      uniprot  \n1  a c a v v k v k c w  6786  None  B1W6O2      uniprot  \n2  c k g d a l f r t v  6868  1323  G6V5R9      uniprot  \nentrez.protein\n  entrez.protein                                           sequence  \\\n0          52329  DYRSMAPTFILMKIYPACDAITKRRWSVATVKDGEFIWWSAVKIFP...   \n1         581107  LLVFNMGQLAVAGYGNTMVSAMMCFCCDVKARMGMSWLPKITTMQW...   \n2         270569  MVCSHHELAVAFQTMCPIQGDAATAKANAHRTTDKQNWMVVKWFRT...   \n\n           description taxon  mass      id preferred_id  \n0  q k r b h g t q x x  9606  None   52329       entrez  \n1  h f g z j r b g m w  9606  None  581107       entrez  \n2  s b p v f u t y g v  9606  None  270569       entrez  \n</pre> In\u00a0[\u00a0]: Copied! <pre>print_yaml('06_schema_config_pandas.yaml')\n</pre> print_yaml('06_schema_config_pandas.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id:\n    - uniprot\n    - entrez\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n    properties:\n        sequence: str\n        description: str\n        taxon: str\n        mass: int\nprotein isoform:\n    is_a: protein\n    inherit_properties: true\n    represented_as: node\n    preferred_id: uniprot\n    input_label: uniprot_isoform\nprotein protein interaction:\n    is_a: pairwise molecular interaction\n    represented_as: edge\n    preferred_id: intact\n    input_label: interacts_with\n    properties:\n        method: str\n        source: str\n\n--------------\n</pre> <p>Now that we have added <code>protein protein interaction</code> as an edge, we have to simulate some interactions:</p> In\u00a0[\u00a0]: Copied! <pre>from data_generator import InteractionGenerator\n\n# Simulate edges for proteins we defined above\nppi = InteractionGenerator(\n    interactors=[p.get_id() for p in proteins],\n    interaction_probability=0.05,\n).generate_interactions()\n</pre> from data_generator import InteractionGenerator  # Simulate edges for proteins we defined above ppi = InteractionGenerator(     interactors=[p.get_id() for p in proteins],     interaction_probability=0.05, ).generate_interactions() In\u00a0[\u00a0]: Copied! <pre># naturally interactions/edges contain information about the interacting source and target nodes\n# let's look at the first one in the list\ninteraction = ppi[0]\nf\"{interaction.get_source_id()} {interaction.label} {interaction.get_target_id()}\"\n</pre> # naturally interactions/edges contain information about the interacting source and target nodes # let's look at the first one in the list interaction = ppi[0] f\"{interaction.get_source_id()} {interaction.label} {interaction.get_target_id()}\" Out[\u00a0]: <pre>'A9L6G4 interacts_with V4F1T1'</pre> In\u00a0[\u00a0]: Copied! <pre># similarly to nodes, it also has a dictionary of properties\ninteraction.get_properties()\n</pre> # similarly to nodes, it also has a dictionary of properties interaction.get_properties() Out[\u00a0]: <pre>{'source': 'signor', 'method': 'u z c x m d c u g s'}</pre> <p>As with nodes, we add first createa a new BioCypher instance, and then populate it with nodes as well as edges:</p> In\u00a0[\u00a0]: Copied! <pre>bc = BioCypher(\n    biocypher_config_path='06_biocypher_config.yaml',\n    schema_config_path='06_schema_config_pandas.yaml',\n)\n</pre> bc = BioCypher(     biocypher_config_path='06_biocypher_config.yaml',     schema_config_path='06_schema_config_pandas.yaml', ) In\u00a0[\u00a0]: Copied! <pre># Extract id, source, target, label, and property dictionary\ndef edge_generator(ppi):\n    for interaction in ppi:\n        yield (\n            interaction.get_id(),\n            interaction.get_source_id(),\n            interaction.get_target_id(),\n            interaction.get_label(),\n            interaction.get_properties(),\n        )\n\nbc.add(node_generator(proteins))\nbc.add(edge_generator(ppi))\n</pre> # Extract id, source, target, label, and property dictionary def edge_generator(ppi):     for interaction in ppi:         yield (             interaction.get_id(),             interaction.get_source_id(),             interaction.get_target_id(),             interaction.get_label(),             interaction.get_properties(),         )  bc.add(node_generator(proteins)) bc.add(edge_generator(ppi))  <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> <p>Let's look at the interaction DataFrame:</p> In\u00a0[\u00a0]: Copied! <pre>bc.to_df()[\"protein protein interaction\"]\n</pre> bc.to_df()[\"protein protein interaction\"] Out[\u00a0]: protein protein interaction _from _to source method 0 intact703256 A9L6G4 V4F1T1 signor u z c x m d c u g s 1 None E4N2H2 F0N9A4 intact None <p>Finally, it is worth noting that BioCypher relies on ontologies, which are machine readable representations of domains of knowledge that we use to ground the contents of our knowledge graphs. While details about ontologies are out of scope for this tutorial, and are described in detail in the BioCypher documentation, we can still have a glimpse at the ontology that we used implicitly in this tutorial:</p> In\u00a0[\u00a0]: Copied! <pre>bc.show_ontology_structure()\n</pre> bc.show_ontology_structure() <pre>Showing ontology structure based on https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl\nentity\n\u251c\u2500\u2500 association\n\u2502   \u2514\u2500\u2500 gene to gene association\n\u2502       \u2514\u2500\u2500 pairwise gene to gene interaction\n\u2502           \u2514\u2500\u2500 pairwise molecular interaction\n\u2502               \u2514\u2500\u2500 protein protein interaction\n\u2514\u2500\u2500 named thing\n    \u2514\u2500\u2500 biological entity\n        \u2514\u2500\u2500 polypeptide\n            \u2514\u2500\u2500 protein\n                \u251c\u2500\u2500 entrez.protein\n                \u251c\u2500\u2500 protein isoform\n                \u2514\u2500\u2500 uniprot.protein\n</pre> Out[\u00a0]: <pre>&lt;treelib.tree.Tree at 0x7f7327b3a880&gt;</pre>"},{"location":"learn/tutorials/pandas_tutorial/#example-notebook-biocypher-and-pandas","title":"Example Notebook: BioCypher and Pandas\u00b6","text":""},{"location":"learn/tutorials/pandas_tutorial/#introduction","title":"Introduction\u00b6","text":"<p>The main purpose of BioCypher is to facilitate the pre-processing of biomedical data, and thus save development time in the maintenance of curated knowledge graphs, while allowing simple and efficient creation of task-specific lightweight knowledge graphs in a user-friendly and biology-centric fashion.</p> <p>We are going to use a toy example to familiarise the user with the basic functionality of BioCypher. One central task of BioCypher is the harmonisation of dissimilar datasets describing the same entities. Thus, in this example, the input data - which in the real-world use case could come from any type of interface - are represented by simulated data containing some examples of differently formatted biomedical entities such as proteins and their interactions.</p> <p>There are two other versions of this tutorial, which only differ in the output format. The first uses a CSV output format to write files suitable for Neo4j admin import, and the second creates an in-memory collection of Pandas dataframes. You can find the former in the tutorial directory of the BioCypher repository. This tutorial simply takes the latter, in-memory approach to a Jupyter notebook.</p>"},{"location":"learn/tutorials/pandas_tutorial/#setup","title":"Setup\u00b6","text":""},{"location":"learn/tutorials/pandas_tutorial/#tutorial-files","title":"Tutorial files\u00b6","text":"<p>In the <code>biocypher</code> root directory, you will find a <code>tutorial</code> directory with the files for this tutorial. The <code>data_generator.py</code> file contains the simulated data generation code, and the other files, specifically the <code>.yaml</code> files, are named according to the tutorial step they are used in.</p> <p>Let's download these:</p>"},{"location":"learn/tutorials/pandas_tutorial/#configuration","title":"Configuration\u00b6","text":"<p>BioCypher is configured using a YAML file; it comes with a default (which you can see in the Configuration section). You can use it, for instance, to select an output format, the output directory, separators, logging level, and other options. For this tutorial, we will use a dedicated configuration file for each of the steps. The configuration files are located in the <code>tutorial</code> directory, and are called using the <code>biocypher_config_path</code> argument at instantiation of the BioCypher interface. For more information, see also the Quickstart Configuration section.</p>"},{"location":"learn/tutorials/pandas_tutorial/#section-1-adding-data","title":"Section 1: Adding data\u00b6","text":""},{"location":"learn/tutorials/pandas_tutorial/#input-data-stream-adapter","title":"Input data stream (\"adapter\")\u00b6","text":"<p>The basic operation of adding data to the knowledge graph requires two components: an input stream of data (which we call adapter) and a configuration for the resulting desired output (the schema configuration). The former will be simulated by calling the <code>Protein</code> class of our data generator 10 times.</p>"},{"location":"learn/tutorials/pandas_tutorial/#schema-configuration","title":"Schema configuration\u00b6","text":"<p>How each BioCypher graph is structured is determined by the schema configuration YAML file that is given to the BioCypher interface. This also serves to ground the entities of the graph in the biomedical realm by using an ontological hierarchy. In this tutorial, we refer to the Biolink model as the general backbone of our ontological hierarchy. The basic premise of the schema configuration YAML file is that each component of the desired knowledge graph output should be configured here; if (and only if) an entity is represented in the schema configuration and is present in the input data stream, will it be part of our knowledge graph.</p> <p>In our case, since we only import proteins, we only require few lines of configuration:</p>"},{"location":"learn/tutorials/pandas_tutorial/#creating-the-graph-using-the-biocypher-interface","title":"Creating the graph (using the BioCypher interface)\u00b6","text":"<p>All that remains to be done now is to instantiate the BioCypher interface (as the main means of communicating with BioCypher) and call the function to create the graph.</p>"},{"location":"learn/tutorials/pandas_tutorial/#section-2-merging-data","title":"Section 2: Merging data\u00b6","text":""},{"location":"learn/tutorials/pandas_tutorial/#plain-merge","title":"Plain merge\u00b6","text":"<p>Using the workflow described above with minor changes, we can merge data from different input streams. If we do not want to introduce additional ontological subcategories, we can simply add the new input stream to the existing one and add the new label to the schema configuration (the new label being <code>entrez_protein</code>). In this case, we would add the following to the schema configuration:</p>"},{"location":"learn/tutorials/pandas_tutorial/#ad-hoc-subclassing","title":"Ad hoc subclassing\u00b6","text":""},{"location":"learn/tutorials/pandas_tutorial/#section-3-handling-properties","title":"Section 3: Handling properties\u00b6","text":"<p>While ID and label are mandatory components of our knowledge graph, properties are optional and can include different types of information on the entities. In source data, properties are represented in arbitrary ways, and designations rarely overlap even for the most trivial of cases (spelling differences, formatting, etc). Additionally, some data sources contain a large wealth of information about entities, most of which may not be needed for the given task. Thus, it is often desirable to filter out properties that are not needed to save time, disk space, and memory.</p> <p>Maintaining consistent properties per entity type is particularly important when using the admin import feature of Neo4j, which requires consistency between the header and data files. Properties that are introduced into only some of the rows will lead to column misalignment and import failure. In \"online mode\", this is not an issue.</p> <p>We will take a look at how to handle property selection in BioCypher in a way that is flexible and easy to maintain.</p>"},{"location":"learn/tutorials/pandas_tutorial/#designated-properties","title":"Designated properties\u00b6","text":"<p>The simplest and most straightforward way to ensure that properties are consistent for each entity type is to designate them explicitly in the schema configuration. This is done by adding a <code>properties</code> key to the entity type configuration. The value of this key is another dictionary, where in the standard case the keys are the names of the properties that the entity type should possess, and the values give the type of the property. Possible values are:</p> <ul> <li><p><code>str</code> (or <code>string</code>),</p> </li> <li><p><code>int</code> (or <code>integer</code>, <code>long</code>),</p> </li> <li><p><code>float</code> (or <code>double</code>, <code>dbl</code>),</p> </li> <li><p><code>bool</code> (or <code>boolean</code>),</p> </li> <li><p>arrays of any of these types (indicated by square brackets, e.g. <code>string[]</code>).</p> </li> </ul> <p>In the case of properties that are not present in (some of) the source data, BioCypher will add them to the output with a default value of <code>None</code>. Additional properties in the input that are not represented in these designated property names will be ignored. Let's imagine that some, but not all, of our protein nodes have a <code>mass</code> value. If we want to include the mass value on all proteins, we can add the following to our schema configuration:</p>"},{"location":"learn/tutorials/pandas_tutorial/#inheriting-properties","title":"Inheriting properties\u00b6","text":"<p>Sometimes, explicit designation of properties requires a lot of maintenance work, particularly for classes with many properties. In these cases, it may be more convenient to inherit properties from a parent class. This is done by adding a <code>properties</code> key to a suitable parent class configuration, and then defining inheritance via the <code>is_a</code> key in the child class configuration and setting the <code>inherit_properties</code> key to <code>true</code>.</p> <p>Let's say we have an additional <code>protein isoform</code> class, which can reasonably inherit from <code>protein</code> and should carry the same properties as the parent. We can add the following to our schema configuration:</p>"},{"location":"learn/tutorials/pandas_tutorial/#section-4-handling-relationships","title":"Section 4: Handling relationships\u00b6","text":"<p>Naturally, we do not only want nodes in our knowledge graph, but also edges. In BioCypher, the configuration of relationships is very similar to that of nodes, with some key differences. First the similarities: the top-level class configuration of edges is the same; class names refer to ontological classes or are an extension thereof. Similarly, the <code>is_a</code> key is used to define inheritance, and the <code>inherit_properties</code> key is used to inherit properties from a parent class. Relationships also possess a <code>preferred_id</code> key, an <code>input_label</code> key, and a <code>properties</code> key, which work in the same way as for nodes.</p> <p>Relationships also have a <code>represented_as</code> key, which in this case can be either <code>node</code> or <code>edge</code>. The <code>node</code> option is used to \"reify\" the relationship in order to be able to connect it to other nodes in the graph. In addition to the configuration of nodes, relationships also have fields for the <code>source</code> and <code>target</code> node types, which refer to the ontological classes of the respective nodes, and are currently optional.</p> <p>To add protein-protein interactions to our graph, we can modify the schema configuration above to the following:</p>"},{"location":"learn/tutorials/pandas_tutorial_refactored/","title":"Example Notebook: BioCypher and Pandas","text":"Tip: Run the tutorial interactively in      Google Colab.  <p>While BioCypher was designed as a graph-focused framework, due to commonalities in bioinformatics workflows, BioCypher also supports Pandas DataFrames. This allows integration with methods that use tabular data, such as machine learning and statistical analysis, for instance in the scVerse framework.</p> <p>To run this tutorial interactively, you will first need to install perform some setup steps specific to running on Google Colab. You can collapse this section and run the setup steps with one click, as they are not required for the explanation of BioCyper's functionality. You can of course also run the steps one by one, if you want to see what is happening. The real tutorial starts with section 1, \"Adding data\" (do not follow this link on colab, as you will be taken back to the website; please scroll down instead).</p> In\u00a0[\u00a0]: Copied! <pre>!pip install biocypher\n</pre> !pip install biocypher In\u00a0[\u00a0]: Copied! <pre>import yaml\nimport requests\nimport subprocess\n\nschema_path = \"https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/\"\n</pre> import yaml import requests import subprocess  schema_path = \"https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/\" In\u00a0[\u00a0]: Copied! <pre>!wget -O data_generator.py \"https://github.com/biocypher/biocypher/raw/main/tutorial/data_generator.py\"\n</pre> !wget -O data_generator.py \"https://github.com/biocypher/biocypher/raw/main/tutorial/data_generator.py\" In\u00a0[\u00a0]: Copied! <pre>owner = \"biocypher\"\nrepo = \"biocypher\"\npath = \"tutorial\"  # The path within the repository (optional, leave empty for the root directory)\ngithub_url = \"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n\napi_url = github_url.format(owner=owner, repo=repo, path=path)\nresponse = requests.get(api_url)\n\n# Get list of yaml files from the repo\nfiles = response.json()\nyamls = []\nfor file in files:\n    if file[\"type\"] == \"file\":\n        if file[\"name\"].endswith(\".yaml\"):\n            yamls.append(file[\"name\"])\n\n# wget all yaml files\nfor yaml in yamls:\n    url_path = schema_path + yaml\n    subprocess.run([\"wget\", url_path])\n</pre> owner = \"biocypher\" repo = \"biocypher\" path = \"tutorial\"  # The path within the repository (optional, leave empty for the root directory) github_url = \"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"  api_url = github_url.format(owner=owner, repo=repo, path=path) response = requests.get(api_url)  # Get list of yaml files from the repo files = response.json() yamls = [] for file in files:     if file[\"type\"] == \"file\":         if file[\"name\"].endswith(\".yaml\"):             yamls.append(file[\"name\"])  # wget all yaml files for yaml in yamls:     url_path = schema_path + yaml     subprocess.run([\"wget\", url_path]) <p>Let's also define functions with which we can visualize those</p> In\u00a0[\u00a0]: Copied! <pre># helper function to print yaml files\nimport yaml\ndef print_yaml(file_path):\n    with open(file_path, 'r') as file:\n        yaml_data = yaml.safe_load(file)\n\n    print(\"--------------\")\n    print(yaml.dump(yaml_data, sort_keys=False, indent=4))\n    print(\"--------------\")\n</pre> # helper function to print yaml files import yaml def print_yaml(file_path):     with open(file_path, 'r') as file:         yaml_data = yaml.safe_load(file)      print(\"--------------\")     print(yaml.dump(yaml_data, sort_keys=False, indent=4))     print(\"--------------\") In\u00a0[\u00a0]: Copied! <pre># create a list of proteins to be imported\nfrom data_generator import Protein\nn_proteins = 3\nproteins = [Protein() for _ in range(n_proteins)]\n</pre> # create a list of proteins to be imported from data_generator import Protein n_proteins = 3 proteins = [Protein() for _ in range(n_proteins)] <p>Each protein in our simulated data has a UniProt ID, a label (\"uniprot_protein\"), and a dictionary of properties describing it. This is - purely by coincidence - very close to the input BioCypher expects (for nodes):</p> <ul> <li>a unique identifier</li> <li>an input label (to allow mapping to the ontology, see the second step below)</li> <li>a dictionary of further properties (which can be empty)</li> </ul> <p>These should be presented to BioCypher in the form of a tuple. To achieve this representation, we can use a generator function that iterates through our simulated input data and, for each entity, forms the corresponding tuple. The use of a generator allows for efficient streaming of larger datasets where required.</p> In\u00a0[\u00a0]: Copied! <pre>def node_generator(proteins):\n    for protein in proteins:\n        yield (\n            protein.get_id(),\n            protein.get_label(),\n            protein.get_properties(),\n        )\nentities = node_generator(proteins)\n</pre> def node_generator(proteins):     for protein in proteins:         yield (             protein.get_id(),             protein.get_label(),             protein.get_properties(),         ) entities = node_generator(proteins) <p>The concept of an adapter can become arbitrarily complex and involve programmatic access to databases, API requests, asynchronous queries, context managers, and other complicating factors. However, it always boils down to providing the BioCypher driver with a collection of tuples, one for each entity in the input data. For more info, see the section on Adapters.</p> <p>As descibed above, nodes possess:</p> <ul> <li>a mandatory ID,</li> <li>a mandatory label, and</li> <li>a property dictionary,</li> </ul> <p>while edges possess:</p> <ul> <li>an (optional) ID,</li> <li>two mandatory IDs for source and target,</li> <li>a mandatory label, and</li> <li>a property dictionary.</li> </ul> <p>How these entities are mapped to the ontological hierarchy underlying a BioCypher graph is determined by their mandatory labels, which connect the input data stream to the schema configuration. This we will see in the following section.</p> In\u00a0[\u00a0]: Copied! <pre>print_yaml('01_schema_config.yaml')\n</pre> print_yaml('01_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id: uniprot\n    input_label: uniprot_protein\n\n--------------\n</pre> <p>The first line (<code>protein</code>) identifies our entity and connects to the ontological backbone; here we define the first class to be represented in the graph. In the configuration YAML, we represent entities\u202f\u2014 similar to the internal representation of Biolink\u202f\u2014 in lower sentence case (e.g., \"small molecule\"). Conversely, for class names, in file names, and property graph labels, we use PascalCase instead (e.g., \"SmallMolecule\") to avoid issues with handling spaces. The transformation is done by BioCypher internally. BioCypher does not strictly enforce the entities allowed in this class definition; in fact, we provide several methods of extending the existing ontological backbone ad hoc by providing custom inheritance or hybridising ontologies. However, every entity should at some point be connected to the underlying ontology, otherwise the multiple hierarchical labels will not be populated. Following this first line are three indented values of the protein class.</p> <p>The second line (<code>represented_as</code>) tells BioCypher in which way each entity should be represented in the graph; the only options are <code>node</code> and <code>edge</code>. Representation as an edge is only possible when source and target IDs are provided in the input data stream. Conversely, relationships can be represented as both <code>node</code> or <code>edge</code>, depending on the desired output. When a relationship should be represented as a node, i.e., \"reified\", BioCypher takes care to create a set of two edges and a node in place of the relationship. This is useful when we want to connect the relationship to other entities in the graph, for example literature references.</p> <p>The third line (<code>preferred_id</code>) informs the uniqueness of represented entities by selecting an ontological namespace around which the definition of uniqueness should revolve. In our example, if a protein has its own uniprot ID, it is understood to be a unique entity. When there are multiple protein isoforms carrying the same uniprot ID, they are understood to be aggregated to result in only one unique entity in the graph. Decisions around uniqueness of graph constituents sometimes require some consideration in task-specific applications. Selection of a namespace also has effects in identifier mapping; in our case, for protein nodes that do not carry a uniprot ID, identifier mapping will attempt to find a uniprot ID given the other identifiers of that node. To account for the broadest possible range of identifier systems while also dealing with parsing of namespace prefixes and validation, we refer to the Bioregistry project namespaces, which should be preferred values for this field.</p> <p>Finally, the fourth line (<code>input_label</code>) connects the input data stream to the configuration; here we indicate which label to expect in the input tuple for each class in the graph. In our case, we expect \"uniprot_protein\" as the label for each protein in the input data stream; all other input entities that do not carry this label are ignored as long as they are not in the schema configuration.</p> In\u00a0[\u00a0]: Copied! <pre>from biocypher import BioCypher\nbc = BioCypher(\n    biocypher_config_path='01_biocypher_config.yaml',\n    schema_config_path='01_schema_config.yaml',\n)\n# Add the entities that we generated above to the graph\nbc.add(entities)\n</pre> from biocypher import BioCypher bc = BioCypher(     biocypher_config_path='01_biocypher_config.yaml',     schema_config_path='01_schema_config.yaml', ) # Add the entities that we generated above to the graph bc.add(entities) <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> In\u00a0[\u00a0]: Copied! <pre># Print the graph as a dictionary of pandas DataFrame(s) per node label\nbc.to_df()[\"protein\"]\n</pre> # Print the graph as a dictionary of pandas DataFrame(s) per node label bc.to_df()[\"protein\"] Out[\u00a0]: <pre>{'protein':   protein                                           sequence  \\\n 0  F7V4U2  RMFDDRFPVELRICTGSLVIINLGEFAEQHDKQDGSKPSHQPMFAT...   \n 1  K2Y8U3  HWPPSGVSCGVFPECWYRWRDEQWACFGPHIKYNKDNTWSWAQWMH...   \n 2  L1V6V9  QAEPKYKLAQENCRVQIKLPKIVGTCRPHWMTKTYHVLHTCVLWKS...   \n \n            description taxon      id preferred_id  \n 0  i f c m m q e o o s  9606  F7V4U2      uniprot  \n 1  e y p g j t j y r x  9606  K2Y8U3      uniprot  \n 2  a i b t l j e g n j  9606  L1V6V9      uniprot  }</pre> In\u00a0[\u00a0]: Copied! <pre>from data_generator import Protein, EntrezProtein\n</pre> from data_generator import Protein, EntrezProtein In\u00a0[\u00a0]: Copied! <pre>print_yaml('02_schema_config.yaml')\n</pre> print_yaml('02_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id: uniprot\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n\n--------------\n</pre> In\u00a0[\u00a0]: Copied! <pre># Create a list of proteins to be imported\nproteins = [\n    p for sublist in zip(\n        [Protein() for _ in range(n_proteins)],\n        [EntrezProtein() for _ in range(n_proteins)],\n    ) for p in sublist\n]\n# Create a new BioCypher instance\nbc = BioCypher(\n    biocypher_config_path='02_biocypher_config.yaml',\n    schema_config_path='02_schema_config.yaml',\n)\n# Run the import\nbc.add(node_generator(proteins))\n</pre> # Create a list of proteins to be imported proteins = [     p for sublist in zip(         [Protein() for _ in range(n_proteins)],         [EntrezProtein() for _ in range(n_proteins)],     ) for p in sublist ] # Create a new BioCypher instance bc = BioCypher(     biocypher_config_path='02_biocypher_config.yaml',     schema_config_path='02_schema_config.yaml', ) # Run the import bc.add(node_generator(proteins)) <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> In\u00a0[\u00a0]: Copied! <pre>bc.to_df()[\"protein\"]\n</pre> bc.to_df()[\"protein\"] Out[\u00a0]: <pre>{'protein':   protein                                           sequence  \\\n 0  K2W3K5  TVKISILFNPLPNQDMNTTTCQAESNYKAIYLYPWCSMDDVWNVEA...   \n 1  186009  FHYHGGMGPFMTYQNFLHWEQMQPMKLFNEPMQFHDWYGTHVNWPG...   \n 2  S6E6D1  CSVQIQIGMSQDSPDSSEGNMDCPPRNIGGYEIVCNVQGKRCYSTD...   \n 3  926766  HKEAELLVKGQIQTPKCLRHNHFYAKLTIVIELNYMVDRYGKDMAR...   \n 4  Z1F6R2  FMVWKDCLCIRMRHMAVPVPQYHCEYFEVILERWEVPCFSVLNRCK...   \n 5  362641  PISDEQEMGSEFCGHCNTGVYQVEMHFFECEDLNPKVQPKWIFTVT...   \n \n            description taxon      id preferred_id  \n 0  e e v h x f t f j l  9606  K2W3K5      uniprot  \n 1  b c q m l d a u u g  9606  186009      uniprot  \n 2  i z t s l x v g j l  9606  S6E6D1      uniprot  \n 3  t n a j d l j a t a  9606  926766      uniprot  \n 4  h d m k q n r e h r  9606  Z1F6R2      uniprot  \n 5  l m x k h m v g p y  9606  362641      uniprot  }</pre> <p>This again creates a single DataFrame, now for both protein types, but now including both input streams (you should note both uniprot &amp; entrez style IDs in the id column). However, we are generating our <code>entrez</code> proteins as having entrez IDs, which could result in problems in querying. Additionally, a strict import mode including regex pattern matching of identifiers will fail at this point due to the difference in pattern of UniProt vs. Entrez IDs. This issue could be resolved by mapping the Entrez IDs to UniProt IDs, but we will instead use the opportunity to demonstrate how to merge data from different sources into the same ontological class using ad hoc subclasses.</p> <p>In the previous section, we saw how to merge data from different sources into the same ontological class. However, we did not resolve the issue of the <code>entrez</code> proteins living in a different namespace than the <code>uniprot</code> proteins, which could result in problems in querying. In proteins, it would probably be more appropriate to solve this problem using identifier mapping, but in other categories, e.g., pathways, this may not be possible because of a lack of one-to-one mapping between different data sources. Thus, if we so desire, we can merge datasets into the same ontological class by creating ad hoc subclasses implicitly through BioCypher, by providing multiple preferred identifiers. In our case, we update our schema configuration as follows:</p> In\u00a0[\u00a0]: Copied! <pre>print_yaml('03_schema_config.yaml')\n</pre> print_yaml('03_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id:\n    - uniprot\n    - entrez\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n\n--------------\n</pre> <p>This will \"implicitly\" create two subclasses of the <code>protein</code> class, which will inherit the entire hierarchy of the <code>protein</code> class. The two subclasses will be named using a combination of their preferred namespace and the name of the parent class, separated by a dot, i.e., <code>uniprot.protein</code> and <code>entrez.protein</code>. In this manner, they can be identified as proteins regardless of their sources by any queries for the generic <code>protein</code> class, while still carrying information about their namespace and avoiding identifier conflicts.</p>  The only change affected upon the code from the previous section is the referral to the updated schema configuration file.   In the output, we now generate two separate files for the `protein` class, one for each subclass (with names in PascalCase).  <p>Let's create a DataFrame with the same nodes as above, but with a different schema configuration:</p> In\u00a0[\u00a0]: Copied! <pre>bc = BioCypher(\n    biocypher_config_path='03_biocypher_config.yaml',\n    schema_config_path='03_schema_config.yaml',\n)\nbc.add(node_generator(proteins))\nfor name, df in bc.to_df().items():\n    print(name)\n    display(df)\n</pre> bc = BioCypher(     biocypher_config_path='03_biocypher_config.yaml',     schema_config_path='03_schema_config.yaml', ) bc.add(node_generator(proteins)) for name, df in bc.to_df().items():     print(name)     display(df) <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> Out[\u00a0]: <pre>{'uniprot.protein':   uniprot.protein                                           sequence  \\\n 0          K2W3K5  TVKISILFNPLPNQDMNTTTCQAESNYKAIYLYPWCSMDDVWNVEA...   \n 1          S6E6D1  CSVQIQIGMSQDSPDSSEGNMDCPPRNIGGYEIVCNVQGKRCYSTD...   \n 2          Z1F6R2  FMVWKDCLCIRMRHMAVPVPQYHCEYFEVILERWEVPCFSVLNRCK...   \n \n            description taxon      id preferred_id  \n 0  e e v h x f t f j l  9606  K2W3K5      uniprot  \n 1  i z t s l x v g j l  9606  S6E6D1      uniprot  \n 2  h d m k q n r e h r  9606  Z1F6R2      uniprot  ,\n 'entrez.protein':   entrez.protein                                           sequence  \\\n 0         186009  FHYHGGMGPFMTYQNFLHWEQMQPMKLFNEPMQFHDWYGTHVNWPG...   \n 1         926766  HKEAELLVKGQIQTPKCLRHNHFYAKLTIVIELNYMVDRYGKDMAR...   \n 2         362641  PISDEQEMGSEFCGHCNTGVYQVEMHFFECEDLNPKVQPKWIFTVT...   \n \n            description taxon      id preferred_id  \n 0  b c q m l d a u u g  9606  186009       entrez  \n 1  t n a j d l j a t a  9606  926766       entrez  \n 2  l m x k h m v g p y  9606  362641       entrez  }</pre> <p>Now we see two separate DataFrames, one for each subclass of the <code>protein</code> class.</p> In\u00a0[\u00a0]: Copied! <pre>print_yaml('04_schema_config.yaml')\n</pre> print_yaml('04_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id:\n    - uniprot\n    - entrez\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n    properties:\n        sequence: str\n        description: str\n        taxon: str\n        mass: int\n\n--------------\n</pre> <p>This will add the <code>mass</code> property to all proteins (in addition to the three we had before); if not encountered, the column will be empty. Implicit subclasses will automatically inherit the property configuration; in this case, both <code>uniprot.protein</code> and <code>entrez.protein</code> will have the <code>mass</code> property, even though the <code>entrez</code> proteins do not have a <code>mass</code> value in the input data.</p>  If we wanted to ignore the mass value for all properties, we could simply remove the `mass` key from the `properties` dictionary.  In\u00a0[\u00a0]: Copied! <pre>from data_generator import EntrezProtein, RandomPropertyProtein\n</pre> from data_generator import EntrezProtein, RandomPropertyProtein In\u00a0[\u00a0]: Copied! <pre># Create a list of proteins to be imported (now with properties)\nproteins = [\n    p for sublist in zip(\n        [RandomPropertyProtein() for _ in range(n_proteins)],\n        [EntrezProtein() for _ in range(n_proteins)],\n    ) for p in sublist\n]\n# New instance, populated, and to DataFrame\nbc = BioCypher(\n    biocypher_config_path='04_biocypher_config.yaml',\n    schema_config_path='04_schema_config.yaml',\n)\nbc.add(node_generator(proteins))\nfor name, df in bc.to_df().items():\n    print(name)\n    display(df)\n</pre> # Create a list of proteins to be imported (now with properties) proteins = [     p for sublist in zip(         [RandomPropertyProtein() for _ in range(n_proteins)],         [EntrezProtein() for _ in range(n_proteins)],     ) for p in sublist ] # New instance, populated, and to DataFrame bc = BioCypher(     biocypher_config_path='04_biocypher_config.yaml',     schema_config_path='04_schema_config.yaml', ) bc.add(node_generator(proteins)) for name, df in bc.to_df().items():     print(name)     display(df) <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> Out[\u00a0]: <pre>{'uniprot.protein':   uniprot.protein                                           sequence  \\\n 0          S1Z9L5  RHLRGDVMQEDHHTSSERMVYNVLPQDYKVVSCEYWNTQVTALWVI...   \n 1          W9J5F1  IPFSQSAWAQQRIGPKGTKAHGVTQPAPMDIKNLCNLTDLTLILDF...   \n 2          T1J3U0  WFGCCHKQYVSHVIDRQDPQSPSDNPSLVSQLQFFMWGIQIQNGEI...   \n \n            description taxon  mass      id preferred_id  \n 0  u x e o k m a i o s  3899  None  S1Z9L5      uniprot  \n 1  i x k c r b p d d p  8873  None  W9J5F1      uniprot  \n 2  m a w r r u x c w o  1966  9364  T1J3U0      uniprot  ,\n 'entrez.protein':   entrez.protein                                           sequence  \\\n 0         405878  RMTDGFEWQLDFHAFIWCNQAAWQLPLEVHISQGNGGWRMGLYGNM...   \n 1         154167  CGMNYDNGYFSVAYQSYDLWYHQQLKTRGVKPAEKDSDKDLGIDVI...   \n 2         234189  GQWQECIQGFTPQQMCVDCCAETKLANKSYYHSWMTWRLSGLCFNM...   \n \n            description taxon  mass      id preferred_id  \n 0  y c s v s n e c h o  9606  None  405878       entrez  \n 1  i k n c e n r n c d  9606  None  154167       entrez  \n 2  o v w y g h y e v y  9606  None  234189       entrez  }</pre> In\u00a0[\u00a0]: Copied! <pre>from data_generator import RandomPropertyProteinIsoform\n</pre> from data_generator import RandomPropertyProteinIsoform In\u00a0[\u00a0]: Copied! <pre>print_yaml('05_schema_config.yaml')\n</pre> print_yaml('05_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id:\n    - uniprot\n    - entrez\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n    properties:\n        sequence: str\n        description: str\n        taxon: str\n        mass: int\nprotein isoform:\n    is_a: protein\n    inherit_properties: true\n    represented_as: node\n    preferred_id: uniprot\n    input_label: uniprot_isoform\n\n--------------\n</pre> <p>This allows maintenance of property lists for many classes at once. If the child class has properties already, they will be kept (if they are not present in the parent class) or replaced by the parent class properties (if they are present).</p> <p>Again, apart from adding the protein isoforms to the input stream, the code for this example is identical to the previous one except for the reference to the updated schema configuration.</p> <p>We now create three separate DataFrames, all of which are children of the <code>protein</code> class; two implicit children (<code>uniprot.protein</code> and <code>entrez.protein</code>) and one explicit child (<code>protein isoform</code>).</p> In\u00a0[\u00a0]: Copied! <pre># create a list of proteins to be imported\nproteins = [\n    p for sublist in zip(\n        [RandomPropertyProtein() for _ in range(n_proteins)],\n        [RandomPropertyProteinIsoform() for _ in range(n_proteins)],\n        [EntrezProtein() for _ in range(n_proteins)],\n    ) for p in sublist\n]\n\n# Create BioCypher driver\nbc = BioCypher(\n    biocypher_config_path='05_biocypher_config.yaml',\n    schema_config_path='05_schema_config.yaml',\n)\n# Run the import\nbc.add(node_generator(proteins))\n\nfor name, df in bc.to_df().items():\n    print(name)\n    display(df)\n</pre> # create a list of proteins to be imported proteins = [     p for sublist in zip(         [RandomPropertyProtein() for _ in range(n_proteins)],         [RandomPropertyProteinIsoform() for _ in range(n_proteins)],         [EntrezProtein() for _ in range(n_proteins)],     ) for p in sublist ]  # Create BioCypher driver bc = BioCypher(     biocypher_config_path='05_biocypher_config.yaml',     schema_config_path='05_schema_config.yaml', ) # Run the import bc.add(node_generator(proteins))  for name, df in bc.to_df().items():     print(name)     display(df) <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> <pre>uniprot.protein\n  uniprot.protein                                           sequence  \\\n0          A9L6G4  SWIVVGQPDSHNKRLVNYHWMRCEHPLRCWRPIYVVRVSFQSQCEQ...   \n1          E4N2H2  PGVMILDNMQHKCSKELSTRQIITNHWICNSAPISWSSGMDRSCLD...   \n2          V4F1T1  DQCHNLCPGSSFQCPENAFGNDWIDHMPQETGLMQYDDPQSGMWFT...   \n\n           description taxon  mass      id preferred_id  \n0  m o k j a f w v w r  4220  None  A9L6G4      uniprot  \n1  n v i r s f m f d w  6339  6481  E4N2H2      uniprot  \n2  w e v v a b o b b u  9176  6510  V4F1T1      uniprot  \nprotein isoform\n  protein isoform                                           sequence  \\\n0          F0N9A4  QDVVLVEGCGDEGWIHMPEKRPGQAYKWCERFRPIPDFTNSIKIAY...   \n1          B1W6O2  SQKHFRRWWTNDCFGQELMSIYYNVKFWDNLIEMTGGPASRVCLGQ...   \n2          G6V5R9  ASAITPFSYEKPHTVTLDATEVFPKMQDAQAIEREIHFSKSTLVYG...   \n\n           description taxon  mass      id preferred_id  \n0  r f e a v a a g w r  8061  None  F0N9A4      uniprot  \n1  a c a v v k v k c w  6786  None  B1W6O2      uniprot  \n2  c k g d a l f r t v  6868  1323  G6V5R9      uniprot  \nentrez.protein\n  entrez.protein                                           sequence  \\\n0          52329  DYRSMAPTFILMKIYPACDAITKRRWSVATVKDGEFIWWSAVKIFP...   \n1         581107  LLVFNMGQLAVAGYGNTMVSAMMCFCCDVKARMGMSWLPKITTMQW...   \n2         270569  MVCSHHELAVAFQTMCPIQGDAATAKANAHRTTDKQNWMVVKWFRT...   \n\n           description taxon  mass      id preferred_id  \n0  q k r b h g t q x x  9606  None   52329       entrez  \n1  h f g z j r b g m w  9606  None  581107       entrez  \n2  s b p v f u t y g v  9606  None  270569       entrez  \n</pre> In\u00a0[\u00a0]: Copied! <pre>print_yaml('06_schema_config_pandas.yaml')\n</pre> print_yaml('06_schema_config_pandas.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id:\n    - uniprot\n    - entrez\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n    properties:\n        sequence: str\n        description: str\n        taxon: str\n        mass: int\nprotein isoform:\n    is_a: protein\n    inherit_properties: true\n    represented_as: node\n    preferred_id: uniprot\n    input_label: uniprot_isoform\nprotein protein interaction:\n    is_a: pairwise molecular interaction\n    represented_as: edge\n    preferred_id: intact\n    input_label: interacts_with\n    properties:\n        method: str\n        source: str\n\n--------------\n</pre> <p>Now that we have added <code>protein protein interaction</code> as an edge, we have to simulate some interactions:</p> In\u00a0[\u00a0]: Copied! <pre>from data_generator import InteractionGenerator\n\n# Simulate edges for proteins we defined above\nppi = InteractionGenerator(\n    interactors=[p.get_id() for p in proteins],\n    interaction_probability=0.05,\n).generate_interactions()\n</pre> from data_generator import InteractionGenerator  # Simulate edges for proteins we defined above ppi = InteractionGenerator(     interactors=[p.get_id() for p in proteins],     interaction_probability=0.05, ).generate_interactions() In\u00a0[\u00a0]: Copied! <pre># naturally interactions/edges contain information about the interacting source and target nodes\n# let's look at the first one in the list\ninteraction = ppi[0]\nf\"{interaction.get_source_id()} {interaction.label} {interaction.get_target_id()}\"\n</pre> # naturally interactions/edges contain information about the interacting source and target nodes # let's look at the first one in the list interaction = ppi[0] f\"{interaction.get_source_id()} {interaction.label} {interaction.get_target_id()}\" Out[\u00a0]: <pre>'A9L6G4 interacts_with V4F1T1'</pre> In\u00a0[\u00a0]: Copied! <pre># similarly to nodes, it also has a dictionary of properties\ninteraction.get_properties()\n</pre> # similarly to nodes, it also has a dictionary of properties interaction.get_properties() Out[\u00a0]: <pre>{'source': 'signor', 'method': 'u z c x m d c u g s'}</pre> <p>As with nodes, we add first createa a new BioCypher instance, and then populate it with nodes as well as edges:</p> In\u00a0[\u00a0]: Copied! <pre>bc = BioCypher(\n    biocypher_config_path='06_biocypher_config.yaml',\n    schema_config_path='06_schema_config_pandas.yaml',\n)\n</pre> bc = BioCypher(     biocypher_config_path='06_biocypher_config.yaml',     schema_config_path='06_schema_config_pandas.yaml', ) In\u00a0[\u00a0]: Copied! <pre># Extract id, source, target, label, and property dictionary\ndef edge_generator(ppi):\n    for interaction in ppi:\n        yield (\n            interaction.get_id(),\n            interaction.get_source_id(),\n            interaction.get_target_id(),\n            interaction.get_label(),\n            interaction.get_properties(),\n        )\n\nbc.add(node_generator(proteins))\nbc.add(edge_generator(ppi))\n</pre> # Extract id, source, target, label, and property dictionary def edge_generator(ppi):     for interaction in ppi:         yield (             interaction.get_id(),             interaction.get_source_id(),             interaction.get_target_id(),             interaction.get_label(),             interaction.get_properties(),         )  bc.add(node_generator(proteins)) bc.add(edge_generator(ppi))  <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> <p>Let's look at the interaction DataFrame:</p> In\u00a0[\u00a0]: Copied! <pre>bc.to_df()[\"protein protein interaction\"]\n</pre> bc.to_df()[\"protein protein interaction\"] Out[\u00a0]: protein protein interaction _from _to source method 0 intact703256 A9L6G4 V4F1T1 signor u z c x m d c u g s 1 None E4N2H2 F0N9A4 intact None <p>Finally, it is worth noting that BioCypher relies on ontologies, which are machine readable representations of domains of knowledge that we use to ground the contents of our knowledge graphs. While details about ontologies are out of scope for this tutorial, and are described in detail in the BioCypher documentation, we can still have a glimpse at the ontology that we used implicitly in this tutorial:</p> In\u00a0[\u00a0]: Copied! <pre>bc.show_ontology_structure()\n</pre> bc.show_ontology_structure() <pre>Showing ontology structure based on https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl\nentity\n\u251c\u2500\u2500 association\n\u2502   \u2514\u2500\u2500 gene to gene association\n\u2502       \u2514\u2500\u2500 pairwise gene to gene interaction\n\u2502           \u2514\u2500\u2500 pairwise molecular interaction\n\u2502               \u2514\u2500\u2500 protein protein interaction\n\u2514\u2500\u2500 named thing\n    \u2514\u2500\u2500 biological entity\n        \u2514\u2500\u2500 polypeptide\n            \u2514\u2500\u2500 protein\n                \u251c\u2500\u2500 entrez.protein\n                \u251c\u2500\u2500 protein isoform\n                \u2514\u2500\u2500 uniprot.protein\n</pre> Out[\u00a0]: <pre>&lt;treelib.tree.Tree at 0x7f7327b3a880&gt;</pre>"},{"location":"learn/tutorials/pandas_tutorial_refactored/#example-notebook-biocypher-and-pandas","title":"Example Notebook: BioCypher and Pandas\u00b6","text":""},{"location":"learn/tutorials/pandas_tutorial_refactored/#introduction","title":"Introduction\u00b6","text":"<p>The main purpose of BioCypher is to facilitate the pre-processing of biomedical data, and thus save development time in the maintenance of curated knowledge graphs, while allowing simple and efficient creation of task-specific lightweight knowledge graphs in a user-friendly and biology-centric fashion.</p> <p>We are going to use a toy example to familiarise the user with the basic functionality of BioCypher. One central task of BioCypher is the harmonisation of dissimilar datasets describing the same entities. Thus, in this example, the input data - which in the real-world use case could come from any type of interface - are represented by simulated data containing some examples of differently formatted biomedical entities such as proteins and their interactions.</p> <p>There are two other versions of this tutorial, which only differ in the output format. The first uses a CSV output format to write files suitable for Neo4j admin import, and the second creates an in-memory collection of Pandas dataframes. You can find the former in the tutorial directory of the BioCypher repository. This tutorial simply takes the latter, in-memory approach to a Jupyter notebook.</p>"},{"location":"learn/tutorials/pandas_tutorial_refactored/#setup","title":"Setup\u00b6","text":""},{"location":"learn/tutorials/pandas_tutorial_refactored/#tutorial-files","title":"Tutorial files\u00b6","text":"<p>In the <code>biocypher</code> root directory, you will find a <code>tutorial</code> directory with the files for this tutorial. The <code>data_generator.py</code> file contains the simulated data generation code, and the other files, specifically the <code>.yaml</code> files, are named according to the tutorial step they are used in.</p> <p>Let's download these:</p>"},{"location":"learn/tutorials/pandas_tutorial_refactored/#configuration","title":"Configuration\u00b6","text":"<p>BioCypher is configured using a YAML file; it comes with a default (which you can see in the Configuration section). You can use it, for instance, to select an output format, the output directory, separators, logging level, and other options. For this tutorial, we will use a dedicated configuration file for each of the steps. The configuration files are located in the <code>tutorial</code> directory, and are called using the <code>biocypher_config_path</code> argument at instantiation of the BioCypher interface. For more information, see also the Quickstart Configuration section.</p>"},{"location":"learn/tutorials/pandas_tutorial_refactored/#section-1-adding-data","title":"Section 1: Adding data\u00b6","text":""},{"location":"learn/tutorials/pandas_tutorial_refactored/#input-data-stream-adapter","title":"Input data stream (\"adapter\")\u00b6","text":"<p>The basic operation of adding data to the knowledge graph requires two components: an input stream of data (which we call adapter) and a configuration for the resulting desired output (the schema configuration). The former will be simulated by calling the <code>Protein</code> class of our data generator 10 times.</p>"},{"location":"learn/tutorials/pandas_tutorial_refactored/#schema-configuration","title":"Schema configuration\u00b6","text":"<p>How each BioCypher graph is structured is determined by the schema configuration YAML file that is given to the BioCypher interface. This also serves to ground the entities of the graph in the biomedical realm by using an ontological hierarchy. In this tutorial, we refer to the Biolink model as the general backbone of our ontological hierarchy. The basic premise of the schema configuration YAML file is that each component of the desired knowledge graph output should be configured here; if (and only if) an entity is represented in the schema configuration and is present in the input data stream, will it be part of our knowledge graph.</p> <p>In our case, since we only import proteins, we only require few lines of configuration:</p>"},{"location":"learn/tutorials/pandas_tutorial_refactored/#creating-the-graph-using-the-biocypher-interface","title":"Creating the graph (using the BioCypher interface)\u00b6","text":"<p>All that remains to be done now is to instantiate the BioCypher interface (as the main means of communicating with BioCypher) and call the function to create the graph.</p>"},{"location":"learn/tutorials/pandas_tutorial_refactored/#section-2-merging-data","title":"Section 2: Merging data\u00b6","text":""},{"location":"learn/tutorials/pandas_tutorial_refactored/#plain-merge","title":"Plain merge\u00b6","text":"<p>Using the workflow described above with minor changes, we can merge data from different input streams. If we do not want to introduce additional ontological subcategories, we can simply add the new input stream to the existing one and add the new label to the schema configuration (the new label being <code>entrez_protein</code>). In this case, we would add the following to the schema configuration:</p>"},{"location":"learn/tutorials/pandas_tutorial_refactored/#ad-hoc-subclassing","title":"Ad hoc subclassing\u00b6","text":""},{"location":"learn/tutorials/pandas_tutorial_refactored/#section-3-handling-properties","title":"Section 3: Handling properties\u00b6","text":"<p>While ID and label are mandatory components of our knowledge graph, properties are optional and can include different types of information on the entities. In source data, properties are represented in arbitrary ways, and designations rarely overlap even for the most trivial of cases (spelling differences, formatting, etc). Additionally, some data sources contain a large wealth of information about entities, most of which may not be needed for the given task. Thus, it is often desirable to filter out properties that are not needed to save time, disk space, and memory.</p> <p>Maintaining consistent properties per entity type is particularly important when using the admin import feature of Neo4j, which requires consistency between the header and data files. Properties that are introduced into only some of the rows will lead to column misalignment and import failure. In \"online mode\", this is not an issue.</p> <p>We will take a look at how to handle property selection in BioCypher in a way that is flexible and easy to maintain.</p>"},{"location":"learn/tutorials/pandas_tutorial_refactored/#designated-properties","title":"Designated properties\u00b6","text":"<p>The simplest and most straightforward way to ensure that properties are consistent for each entity type is to designate them explicitly in the schema configuration. This is done by adding a <code>properties</code> key to the entity type configuration. The value of this key is another dictionary, where in the standard case the keys are the names of the properties that the entity type should possess, and the values give the type of the property. Possible values are:</p> <ul> <li><p><code>str</code> (or <code>string</code>),</p> </li> <li><p><code>int</code> (or <code>integer</code>, <code>long</code>),</p> </li> <li><p><code>float</code> (or <code>double</code>, <code>dbl</code>),</p> </li> <li><p><code>bool</code> (or <code>boolean</code>),</p> </li> <li><p>arrays of any of these types (indicated by square brackets, e.g. <code>string[]</code>).</p> </li> </ul> <p>In the case of properties that are not present in (some of) the source data, BioCypher will add them to the output with a default value of <code>None</code>. Additional properties in the input that are not represented in these designated property names will be ignored. Let's imagine that some, but not all, of our protein nodes have a <code>mass</code> value. If we want to include the mass value on all proteins, we can add the following to our schema configuration:</p>"},{"location":"learn/tutorials/pandas_tutorial_refactored/#inheriting-properties","title":"Inheriting properties\u00b6","text":"<p>Sometimes, explicit designation of properties requires a lot of maintenance work, particularly for classes with many properties. In these cases, it may be more convenient to inherit properties from a parent class. This is done by adding a <code>properties</code> key to a suitable parent class configuration, and then defining inheritance via the <code>is_a</code> key in the child class configuration and setting the <code>inherit_properties</code> key to <code>true</code>.</p> <p>Let's say we have an additional <code>protein isoform</code> class, which can reasonably inherit from <code>protein</code> and should carry the same properties as the parent. We can add the following to our schema configuration:</p>"},{"location":"learn/tutorials/pandas_tutorial_refactored/#section-4-handling-relationships","title":"Section 4: Handling relationships\u00b6","text":"<p>Naturally, we do not only want nodes in our knowledge graph, but also edges. In BioCypher, the configuration of relationships is very similar to that of nodes, with some key differences. First the similarities: the top-level class configuration of edges is the same; class names refer to ontological classes or are an extension thereof. Similarly, the <code>is_a</code> key is used to define inheritance, and the <code>inherit_properties</code> key is used to inherit properties from a parent class. Relationships also possess a <code>preferred_id</code> key, an <code>input_label</code> key, and a <code>properties</code> key, which work in the same way as for nodes.</p> <p>Relationships also have a <code>represented_as</code> key, which in this case can be either <code>node</code> or <code>edge</code>. The <code>node</code> option is used to \"reify\" the relationship in order to be able to connect it to other nodes in the graph. In addition to the configuration of nodes, relationships also have fields for the <code>source</code> and <code>target</code> node types, which refer to the ontological classes of the respective nodes, and are currently optional.</p> <p>To add protein-protein interactions to our graph, we can modify the schema configuration above to the following:</p>"},{"location":"learn/tutorials/tutorial001_basics/","title":"Tutorial - Basics","text":"<p>The main purpose of BioCypher is to facilitate the pre-processing of biomedical data to save development time in the maintenance of curated knowledge graphs and to allow the simple and efficient creation of task-specific lightweight knowledge graphs in a user-friendly and biology-centric fashion.</p> <p>We are going to use a toy example to familiarise the user with the basic functionality of BioCypher. One central task of BioCypher is the harmonisation of dissimilar datasets describing the same entities. Thus, in this example, the input data - which in the real-world use case could come from any type of interface - are represented by simulated data containing some examples of differently formatted biomedical entities such as proteins and their interactions.</p> <p>There are two versions of this tutorial, which only differ in the output format. The first uses a CSV output format to write files suitable for Neo4j admin import, and the second creates an in-memory collection of Pandas dataframes. You can find both in the <code>tutorial</code> directory of the BioCypher repository; the Pandas version of each tutorial step is suffixed with <code>_pandas</code>.</p> <p>Neo4j</p> <p>While you can use the files generated to create an actual Neo4j database, it is not required for this tutorial. For checking the output, you can simply open the CSV files in a text editor or your IDE; by default, they will be written to the <code>biocypher-out</code> directory. If you simply want to run the tutorial to see how it works, you can also run the Pandas version.</p>"},{"location":"learn/tutorials/tutorial001_basics/#setup","title":"Setup","text":"<p>To run this tutorial, you will need to have cloned and installed the BioCypher repository on your machine. We recommend using Poetry:</p> <pre><code>git clone https://github.com/biocypher/biocypher.git\ncd biocypher\npoetry install\n</code></pre> <p>Poetry environment</p> <p>In order to run the tutorial code, you will need to activate the Poetry environment. This can be done by running <code>poetry shell</code> in the <code>biocypher</code> directory. Alternatively, you can run the code from within the Poetry environment by prepending <code>poetry run</code> to the command. For example, to run the tutorial code, you can run <code>poetry run python tutorial/01__basic_import.py</code>.</p> <p>In the <code>biocypher</code> root directory, you will find a <code>tutorial</code> directory with the files for this tutorial. The <code>data_generator.py</code> file contains the simulated data generation code, and the other files are named according to the tutorial step they are used in. The <code>biocypher-out</code> directory will be created automatically when you run the tutorial code.</p>"},{"location":"learn/tutorials/tutorial001_basics/#configuration","title":"Configuration","text":"<p>BioCypher is configured using a YAML file; it comes with a default (which you can see in the Configuration section). You can use it, for instance, to select an output format, the output directory, separators, logging level, and other options. For this tutorial, we will use a dedicated configuration file for each of the steps. The configuration files are located in the <code>tutorial</code> directory, and are called using the <code>biocypher_config_path</code> argument at instantiation of the BioCypher interface. For more information, see also the Quickstart Configuration section.</p>"},{"location":"learn/tutorials/tutorial001_basics/#section-1-adding-data","title":"Section 1: Adding data","text":"<p>Note: Poetry environment</p> <p>The code for this tutorial can be found at <code>tutorial/01__basic_import.py</code>. The schema is at <code>tutorial/01_schema_config.yaml</code>, configuration in <code>tutorial/01_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p>"},{"location":"learn/tutorials/tutorial001_basics/#input-data-stream-adapter","title":"Input data stream (\"adapter\")","text":"<p>The basic operation of adding data to the knowledge graph requires two components: an input stream of data (which we call adapter) and a configuration for the resulting desired output (the schema configuration). The former will be simulated by calling the <code>Protein</code> class of our data generator 10 times.</p> <pre><code>from tutorial.data_generator import Protein\nproteins = [Protein() for _ in range(10)]\n</code></pre> <p>Each protein in our simulated data has a UniProt ID, a label (\"uniprot_protein\"), and a dictionary of properties describing it. This is purely by coincidence - very close to the input BioCypher expects (for nodes):</p> <ul> <li>a unique identifier</li> <li>an input label (to allow mapping to the ontology, see the second step below)</li> <li>a dictionary of further properties (which can be empty)</li> </ul> <p>These should be presented to BioCypher in the form of a tuple. To achieve this representation, we can use a generator function that iterates through our simulated input data and, for each entity, forms the corresponding tuple. The use of a generator allows for efficient streaming of larger datasets where required.</p> <pre><code>def node_generator():\n    for protein in proteins:\n        yield (\n            protein.get_id(),\n            protein.get_label(),\n            protein.get_properties()\n        )\n</code></pre> <p>The concept of an adapter can become arbitrarily complex and involve programmatic access to databases, API requests, asynchronous queries, context managers, and other complicating factors. However, it always boils down to providing the BioCypher driver with a collection of tuples, one for each entity in the input data. For more info, see the section on Adapters.</p> <p>As descibed above, nodes possess:</p> <ul> <li>a mandatory ID,</li> <li>a mandatory label, and</li> <li>a property dictionary,</li> </ul> <p>while edges possess:</p> <ul> <li>an (optional) ID,</li> <li>two mandatory IDs for source and target,</li> <li>a mandatory label, and</li> <li>a property dictionary.</li> </ul> <p>How these entities are mapped to the ontological hierarchy underlying a BioCypher graph is determined by their mandatory labels, which connect the input data stream to the schema configuration. This we will see in the following section.</p>"},{"location":"learn/tutorials/tutorial001_basics/#schema-configuration","title":"Schema configuration","text":"<p>How each BioCypher graph is structured is determined by the schema configuration YAML file that is given to the BioCypher interface. This also serves to ground the entities of the graph in the biomedical realm by using an ontological hierarchy. In this tutorial, we refer to the Biolink model as the general backbone of our ontological hierarchy. The basic premise of the schema configuration YAML file is that each component of the desired knowledge graph output should be configured here; if (and only if) an entity is represented in the schema configuration and is present in the input data stream, will it be part of our knowledge graph.</p> <p>In our case, since we only import proteins, we only require few lines of configuration:</p> <pre><code>protein: # mapping\n  represented_as: node # schema configuration\n  preferred_id: uniprot # uniqueness\n  input_label: uniprot_protein # connection to input stream\n</code></pre> <p>The first line (<code>protein</code>) identifies our entity and connects to the ontological backbone; here we define the first class to be represented in the graph. In the configuration YAML, we represent entities\u202f\u2014 similar to the internal representation of Biolink\u202f\u2014 in lower sentence case (e.g., \"small molecule\"). Conversely, for class names, in file names, and property graph labels, we use PascalCase instead (e.g., \"SmallMolecule\") to avoid issues with handling spaces. The transformation is done by BioCypher internally. BioCypher does not strictly enforce the entities allowed in this class definition; in fact, we provide several methods of extending the existing ontological backbone ad hoc by providing custom inheritance or hybridising ontologies. However, every entity should at some point be connected to the underlying ontology, otherwise the multiple hierarchical labels will not be populated. Following this first line are three indented values of the protein class.</p> <p>The second line (<code>represented_as</code>) tells BioCypher in which way each entity should be represented in the graph; the only options are <code>node</code> and <code>edge</code>. Representation as an edge is only possible when source and target IDs are provided in the input data stream. Conversely, relationships can be represented as both <code>node</code> or <code>edge</code>, depending on the desired output. When a relationship should be represented as a node, i.e., \"reified\", BioCypher takes care to create a set of two edges and a node in place of the relationship. This is useful when we want to connect the relationship to other entities in the graph, for example literature references.</p> <p>The third line (<code>preferred_id</code>) informs the uniqueness of represented entities by selecting an ontological namespace around which the definition of uniqueness should revolve. In our example, if a protein has its own uniprot ID, it is understood to be a unique entity. When there are multiple protein isoforms carrying the same uniprot ID, they are understood to be aggregated to result in only one unique entity in the graph. Decisions around uniqueness of graph constituents sometimes require some consideration in task-specific applications. Selection of a namespace also has effects in identifier mapping; in our case, for protein nodes that do not carry a uniprot ID, identifier mapping will attempt to find a uniprot ID given the other identifiers of that node. To account for the broadest possible range of identifier systems while also dealing with parsing of namespace prefixes and validation, we refer to the Bioregistry project namespaces, which should be preferred values for this field.</p> <p>Finally, the fourth line (<code>input_label</code>) connects the input data stream to the configuration; here we indicate which label to expect in the input tuple for each class in the graph. In our case, we expect \"uniprot_protein\" as the label for each protein in the input data stream; all other input entities that do not carry this label are ignored as long as they are not in the schema configuration.</p>"},{"location":"learn/tutorials/tutorial001_basics/#creating-the-graph-using-the-biocypher-interface","title":"Creating the graph (using the BioCypher interface)","text":"<p>All that remains to be done now is to instantiate the BioCypher interface (as the main means of communicating with BioCypher) and call the function to create the graph. While this can be done \"online\", i.e., by connecting to a running DBMS instance, we will in this example use the offline mode of BioCypher, which does not require setting up a graph database instance. The following code will use the data stream and configuration set up above to write the files for knowledge graph creation:</p> <pre><code>import os\nos.chdir('../')\n</code></pre> <pre><code>from biocypher import BioCypher\n\nbc = BioCypher(\n    biocypher_config_path=\"tutorial/01_biocypher_config.yaml\",\n    schema_config_path=\"tutorial/01_schema_config.yaml\",\n)\n\nbc.write_nodes(node_generator())\n</code></pre> <p>We pass our configuration files at instantiation of the interface, and we pass the data stream to the <code>write_nodes</code> function. BioCypher will then create the graph and write it to the output directory, which is set to <code>biocypher-out/</code> by default, creating a subfolder with the current datetime for each driver instance.</p> <p>Note</p> <p>The <code>biocypher_config_path</code> parameter at instantiation of the <code>BioCypher</code> class should in most cases not be needed; we are using it here to increase convenience of the tutorial and to showcase its use. We are overriding the default value of only two settings: the offline mode (<code>offline</code> in <code>biocypher</code>) and the database name (<code>database_name</code> in <code>neo4j</code>).</p> <p>By default, BioCypher will look for a file named <code>biocypher_config.yaml</code> in the current working directory and in its subfolder <code>config</code>, as well as in various user directories. For more information, see the section on configuration.</p>"},{"location":"learn/tutorials/tutorial001_basics/#importing-data-into-neo4j","title":"Importing data into Neo4j","text":"<p>If you want to build an actual Neo4j graph from the tutorial output files, please follow the Neo4j import tutorial.</p>"},{"location":"learn/tutorials/tutorial001_basics/#quality-control-and-convenience-functions","title":"Quality control and convenience functions","text":"<p>BioCypher provides a number of convenience functions for quality control and data exploration. In addition to writing the import call for Neo4j, we can print a log of ontological classes that were present in the input data but are not accounted for in the schema configuration, as well as a log of duplicates in the input data (for the level of granularity that was used for the import). We can also print the ontological hierarchy derived from the underlying model(s) according to the classes that were given in the schema configuration:</p> <pre><code>bc.log_missing_input_labels()   # show input unaccounted for in the schema\nbc.log_duplicates()             # show duplicates in the input data\nbc.show_ontology_structure()    # show ontological hierarchy\n</code></pre>"},{"location":"learn/tutorials/tutorial001_basics/#section-2-merging-data","title":"Section 2: Merging data","text":""},{"location":"learn/tutorials/tutorial001_basics/#plain-merge","title":"Plain merge","text":"<p>Tutorial files</p> <p>The code for this tutorial can be found at <code>tutorial/02__merge.py</code>. Schema files are at <code>tutorial/02_schema_config.yaml</code>, configuration in <code>tutorial/02_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p> <p>Using the workflow described above with minor changes, we can merge data from different input streams. If we do not want to introduce additional ontological subcategories, we can simply add the new input stream to the existing one and add the new label to the schema configuration (the new label being <code>entrez_protein</code>). In this case, we would add the following to the schema configuration:</p> <pre><code>protein:\n  represented_as: node\n  preferred_id: uniprot\n  input_label: [uniprot_protein, entrez_protein]\n</code></pre> <p>This again creates a single output file, now for both protein types, including both input streams, and the graph can be created as before using the command line call created by BioCypher. However, we are generating our <code>entrez</code> proteins as having entrez IDs, which could result in problems in querying. Additionally, a strict import mode including regex pattern matching of identifiers will fail at this point due to the difference in pattern of UniProt vs. Entrez IDs. This issue could be resolved by mapping the Entrez IDs to UniProt IDs, but we will instead use the opportunity to demonstrate how to merge data from different sources into the same ontological class using ad hoc subclasses.</p>"},{"location":"learn/tutorials/tutorial001_basics/#ad-hoc-subclassing","title":"Ad hoc subclassing","text":"<p>Tutorial files</p> <p>The code for this tutorial can be found at <code>tutorial/03__implicit_subclass.py</code>. Schema files are at <code>tutorial/03_schema_config.yaml</code>, configuration in <code>tutorial/03_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p> <p>In the previous section, we saw how to merge data from different sources into the same ontological class. However, we did not resolve the issue of the <code>entrez</code> proteins living in a different namespace than the <code>uniprot</code> proteins, which could result in problems in querying. In proteins, it would probably be more appropriate to solve this problem using identifier mapping, but in other categories, e.g., pathways, this may not be possible because of a lack of one-to-one mapping between different data sources. Thus, if we so desire, we can merge datasets into the same ontological class by creating ad hoc subclasses implicitly through BioCypher, by providing multiple preferred identifiers. In our case, we update our schema configuration as follows:</p> <pre><code>protein:\n  represented_as: node\n  preferred_id: [uniprot, entrez]\n  input_label: [uniprot_protein, entrez_protein]\n</code></pre> <p>This will \"implicitly\" create two subclasses of the <code>protein</code> class, which will inherit the entire hierarchy of the <code>protein</code> class. The two subclasses will be named using a combination of their preferred namespace and the name of the parent class, separated by a dot, i.e., <code>uniprot.protein</code> and <code>entrez.protein</code>. In this manner, they can be identified as proteins regardless of their sources by any queries for the generic <code>protein</code> class, while still carrying information about their namespace and avoiding identifier conflicts.</p> <p>Note</p> <p>The only change affected upon the code from the previous section is the referral to the updated schema configuration file.</p> <p>Hint</p> <p>In the output, we now generate two separate files for the <code>protein</code> class, one for each subclass (with names in PascalCase).</p>"},{"location":"learn/tutorials/tutorial001_basics/#section-3-handling-properties","title":"Section 3: Handling properties","text":"<p>While ID and label are mandatory components of our knowledge graph, properties are optional and can include different types of information on the entities. In source data, properties are represented in arbitrary ways, and designations rarely overlap even for the most trivial of cases (spelling differences, formatting, etc). Additionally, some data sources contain a large wealth of information about entities, most of which may not be needed for the given task. Thus, it is often desirable to filter out properties that are not needed to save time, disk space, and memory.</p> <p>Note</p> <p>Maintaining consistent properties per entity type is particularly important when using the admin import feature of Neo4j, which requires consistency between the header and data files. Properties that are introduced into only some of the rows will lead to column misalignment and import failure. In \"online mode\", this is not an issue.</p> <p>We will take a look at how to handle property selection in BioCypher in a way that is flexible and easy to maintain.</p>"},{"location":"learn/tutorials/tutorial001_basics/#designated-properties","title":"Designated properties","text":"<p>Tutorial files</p> <p>The code for this tutorial can be found at <code>tutorial/04__properties.py</code>. Schema files are at <code>tutorial/04_schema_config.yaml</code>, configuration in <code>tutorial/04_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p> <p>The simplest and most straightforward way to ensure that properties are consistent for each entity type is to designate them explicitly in the schema configuration. This is done by adding a <code>properties</code> key to the entity type configuration. The value of this key is another dictionary, where in the standard case the keys are the names of the properties that the entity type should possess, and the values give the type of the property. Possible values are:</p> <ul> <li> <p><code>str</code> (or <code>string</code>),</p> </li> <li> <p><code>int</code> (or <code>integer</code>, <code>long</code>),</p> </li> <li> <p><code>float</code> (or <code>double</code>, <code>dbl</code>),</p> </li> <li> <p><code>bool</code> (or <code>boolean</code>),</p> </li> <li> <p>arrays of any of these types (indicated by square brackets, e.g. <code>string[]</code>).</p> </li> </ul> <p>In the case of properties that are not present in (some of) the source data, BioCypher will add them to the output with a default value of <code>None</code>. Additional properties in the input that are not represented in these designated property names will be ignored. Let's imagine that some, but not all, of our protein nodes have a <code>mass</code> value. If we want to include the mass value on all proteins, we can add the following to our schema configuration:</p> <pre><code>protein:\n  represented_as: node\n  preferred_id: [uniprot, entrez]\n  input_label: [uniprot_protein, entrez_protein]\n  properties:\n    sequence: str\n    description: str\n    taxon: str\n    mass: dbl\n</code></pre> <p>This will add the <code>mass</code> property to all proteins (in addition to the three we had before); if not encountered, the column will be empty. Implicit subclasses will automatically inherit the property configuration; in this case, both <code>uniprot.protein</code> and <code>entrez.protein</code> will have the <code>mass</code> property, even though the <code>entrez</code> proteins do not have a <code>mass</code> value in the input data.</p> <p>Note</p> <p>If we wanted to ignore the mass value for all properties, we could simply remove the <code>mass</code> key from the <code>properties</code> dictionary.</p> <p>Tip</p> <p>BioCypher provides feedback about property conflicts; try running the code for this example (<code>04__properties.py</code>) with the schema configuration of the previous section (<code>03_schema_config.yaml</code>) and see what happens.</p>"},{"location":"learn/tutorials/tutorial001_basics/#inheriting-properties","title":"Inheriting properties","text":"<p>Tutorial files</p> <p>The code for this tutorial can be found at <code>tutorial/05__property_inheritance.py</code>. Schema files are at <code>tutorial/05_schema_config.yaml</code>, configuration in <code>tutorial/05_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p> <p>Sometimes, explicit designation of properties requires a lot of maintenance work, particularly for classes with many properties. In these cases, it may be more convenient to inherit properties from a parent class. This is done by adding a <code>properties</code> key to a suitable parent class configuration, and then defining inheritance via the <code>is_a</code> key in the child class configuration and setting the <code>inherit_properties</code> key to <code>true</code>.</p> <p>Let's say we have an additional <code>protein isoform</code> class, which can reasonably inherit from <code>protein</code> and should carry the same properties as the parent. We can add the following to our schema configuration:</p> <pre><code>protein isoform:\n  is_a: protein\n  inherit_properties: true\n  represented_as: node\n  preferred_id: uniprot\n  input_label: uniprot_isoform\n</code></pre> <p>This allows maintenance of property lists for many classes at once. If the child class has properties already, they will be kept (if they are not present in the parent class) or replaced by the parent class properties (if they are present).</p> <p>Note</p> <p>Again, apart from adding the protein isoforms to the input stream, the code for this example is identical to the previous one except for the reference to the updated schema configuration.</p> <p>Hint</p> <p>We now create three separate data files, all of which are children of the <code>protein</code> class; two implicit children (<code>uniprot.protein</code> and <code>entrez.protein</code>) and one explicit child (<code>protein isoform</code>).</p>"},{"location":"learn/tutorials/tutorial001_basics/#section-4-handling-relationships","title":"Section 4: Handling relationships","text":"<p>Tutorial Files</p> <p>The code for this tutorial can be found at <code>tutorial/06__relationships.py</code>. Schema files are at <code>tutorial/06_schema_config.yaml</code>, configuration in <code>tutorial/06_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p> <p>Naturally, we do not only want nodes in our knowledge graph, but also edges. In BioCypher, the configuration of relationships is very similar to that of nodes, with some key differences. First the similarities: the top-level class configuration of edges is the same; class names refer to ontological classes or are an extension thereof. Similarly, the <code>is_a</code> key is used to define inheritance, and the <code>inherit_properties</code> key is used to inherit properties from a parent class. Relationships also possess a <code>preferred_id</code> key, an <code>input_label</code> key, and a <code>properties</code> key, which work in the same way as for nodes.</p> <p>Relationships also have a <code>represented_as</code> key, which in this case can be either <code>node</code> or <code>edge</code>. The <code>node</code> option is used to \"reify\" the relationship in order to be able to connect it to other nodes in the graph. In addition to the configuration of nodes, relationships also have fields for the <code>source</code> and <code>target</code> node types, which refer to the ontological classes of the respective nodes, and are currently optional.</p> <p>To add protein-protein interactions to our graph, we can add the following to the schema configuration above:</p> <pre><code>protein protein interaction:\n  is_a: pairwise molecular interaction\n  represented_as: node\n  preferred_id: intact\n  input_label: interacts_with\n  properties:\n    method: str\n    source: str\n</code></pre> <p>Here, we use explicit subclassing to define the protein-protein interaction, which is not represented in the basic Biolink model, as a direct child of the Biolink \"pairwise molecular interaction\" class. We also reify this relationship by representing it as a node. This allows us to connect it to other nodes in the graph, for example to evidences for each interaction. If we do not want to reify the relationship, we can set <code>represented_as</code> to <code>edge</code> instead.</p>"},{"location":"learn/tutorials/tutorial001_basics/#relationship-identifiers","title":"Relationship identifiers","text":"<p>In biomedical data, relationships often do not have curated unique identifiers. Nevertheless, we may want to be able to refer to them in the graph. Thus, edges possess an ID field similar to nodes, which can be supplied in the input data as an optional first element in the edge tuple. Generating this ID from the properties of the edge (source and target identifiers, and additionally any properties that the edge possesses) can be done, for instance, by using the MD5 hash of the concatenation of these values. Edge IDs are active by default, but can be deactivated by setting the <code>use_id</code> field to <code>false</code> in the <code>schema_config.yaml</code> file.</p> schema_config.yaml<pre><code>protein protein interaction:\n  is_a: pairwise molecular interaction\n  represented_as: edge\n  use_id: false\n  # ...\n</code></pre>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/","title":"Tutorial - Handling Ontologies","text":"<p>BioCypher relies on ontologies to ground the knowledge graph contents in biology. This has the advantages of providing machine readability and therefore automation capabilities as well as making working with BioCypher accessible to biologically oriented researchers. However, it also means that BioCypher requires a certain amount of knowledge about ontologies and how to use them. We try to make dealing with ontologies as easy as possible, but some basic understanding is required. In the following we will cover the basics of ontologies and how to use them in BioCypher.</p>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#what-is-an-ontology","title":"What is an ontology?","text":"<p>An ontology is a formal representation of a domain of knowledge. It is a hierarchical structure of concepts and relations. The concepts are organized into a hierarchy, where each concept is a subclass of a more general concept. For instance, a wardrobe is a subclass of a piece of furniture. Individual wardrobes, such as yours or mine, are instances of the concept wardrobe, and as such would be represented as Wardrobe nodes in a knowledge graph. In BioCypher, these nodes would additionally inherit the PieceOfFurniture label from the ontological hierarchy of things.</p> <p>Note</p> <p>Why is the class called piece of furniture but the label is PieceOfFurniture?</p> <p>The Biolink model uses two different case notations for its labels: the \"internal\" designation of classes is in lower sentence case (\"protein\", \"pairwise molecular interaction\"), while the \"external\" designation is in PascalCase (\"Protein\", \"PairwiseMolecularInteraction\"). BioCypher uses the same paradigm: in most cases (input, schema configuration, internally), the lower sentence case is used, while in the output (Neo4j labels, file system names) the PascalCase is more suitable; Neo4j labels and system file names don't deal well with spaces and special characters. Therefore, we check the output file names for their compliance with the Neo4j naming rules. All non compliant characters are removed from the file name (e.g. if the ontology class is called \"desk (piece of furniture)\", the brackets would be removed and the file name will be \"DeskPieceOfFurniture\"). We also remove the \"biolink:\" CURIE prefix for use in file names and Neo4j labels.</p> <p>The relations between concepts can also be organized into a hierarchy. In the specific case of a Neo4j graph, however, relationships cannot possess multiple labels; therefore, if concept inheritance is desired for relationships, they need to be \"reified\", i.e., turned into nodes. BioCypher provides a simple way of converting edges to nodes and vice versa (using the <code>represented_as</code> field). For a more in-depth explanation of ontologies, we recommend this introduction.</p>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#how-biocypher-uses-ontologies","title":"How BioCypher uses ontologies","text":"<p>BioCypher is agnostic to the choice of ontology. Practically, we have built our initial projects around the Biolink model, because it provides a large but shallow collection of concepts that are relevant to the biomedical domain. Other examples of generalist ontologies are the Experimental Factor Ontology and the Basic Formal Ontology. To account for the specific requirements of expert systems, it is possible to use multiple ontologies in the same project. For instance, one might want to extend the rather basic classes relating to molecular interactions in Biolink (the most specific being <code>pairwise molecular interaction</code>) with more specific classes from a more domain-specific ontology, such as the EBI molecular interactions ontology (PSI-MI). A different project may need to define very specific genetics concepts, and thus extend the Biolink model at the terminal node <code>sequence variant</code> with the corresponding subtree of the Sequence Ontology. The OBO Foundry and the BioPortal collect many such specialised ontologies.</p> <p>The default format for ingesting ontology definitions into BioCypher is the Web Ontology Language (OWL); BioCypher can read <code>.owl</code>, <code>.rdf</code>, and <code>.ttl</code> files. The preferred way to specify the ontology or ontologies to be used in a project is to specify them in the biocypher configuration file (<code>biocypher_config.yaml</code>). This file is used to specify the location of the ontology files, as well as the root node of the main (\"head\") ontology and join nodes as fusion points for all \"tail\" ontologies. For more info, see the section on hybridising ontologies.</p>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#visualising-ontologies","title":"Visualising ontologies","text":"<p>BioCypher provides a simple way of visualising the ontology hierarchy. This is useful for debugging and for getting a quick overview of the ontology and which parts are actually used in the knowledge graph to be created. Depending on your use case you can either visualise the parts of the ontology used in the knowledge graph (sufficient for most use cases) or the full ontology. If the used ontology is more complex and contains multiple inheritance please refer to the section on visualising complex ontologies.</p>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#visualise-only-the-parts-of-the-ontology-used-in-the-knowledge-graph","title":"Visualise only the parts of the ontology used in the knowledge graph","text":"<p>To get an overview of the structure of our project, we can run the following command via the interface:</p> Visualising the ontology hierarchy<pre><code>from biocypher import BioCypher\nbc = BioCypher(\n    offline=True,  # no need to connect or to load data\n    schema_config_path=\"tutorial/06_schema_config.yaml\",\n)\nbc.show_ontology_structure()\n</code></pre> <p>This will build the ontology scaffold and print a tree visualisation of its hierarchy to the console using the treelib library. You can see this in action in tutorial part 6 (<code>tutorial/06_relationships.py</code>). The output will look something like this:</p> <pre><code>Showing ontology structure, based on Biolink 3.0.3:\nentity\n\u251c\u2500\u2500 association\n\u2502   \u2514\u2500\u2500 gene to gene association\n\u2502       \u2514\u2500\u2500 pairwise gene to gene interaction\n\u2502           \u2514\u2500\u2500 pairwise molecular interaction\n\u2502               \u2514\u2500\u2500 protein protein interaction\n\u251c\u2500\u2500 mixin\n\u2514\u2500\u2500 named thing\n    \u2514\u2500\u2500 biological entity\n        \u2514\u2500\u2500 polypeptide\n            \u2514\u2500\u2500 protein\n                \u251c\u2500\u2500 entrez.protein\n                \u251c\u2500\u2500 protein isoform\n                \u2514\u2500\u2500 uniprot.protein\n</code></pre> <p>Note</p> <p>BioCypher will only show the parts of the ontology that are actually used in the knowledge graph with the exception of intermediary nodes that are needed to build a complete tree. For instance, the <code>protein</code> class is linked to the root class <code>entity</code> via <code>polypeptide</code>, <code>biological entity</code>, and <code>named thing</code>, all of which are not part of the input data.</p>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#visualise-the-full-ontology","title":"Visualise the full ontology","text":"<p>If you want to see the complete ontology tree, you can call <code>show_ontology_structure</code> with the parameter <code>full=True</code>.</p> Visualising the full ontology hierarchy<pre><code>from biocypher import BioCypher\nbc = BioCypher(\n    offline=True,  # no need to connect or to load data\n    schema_config_path=\"tutorial/06_schema_config.yaml\",\n)\nbc.show_ontology_structure(full=True)\n</code></pre>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#visualise-complex-ontologies","title":"Visualise complex ontologies","text":"<p>Not all ontologies can be easily visualised as a tree, such as ontologies with multiple inheritance, where classes in the ontology can have multiple parent classes. This violates the definition of a tree, where each node can only have one parent node. Consequently, ontologies with multiple inheritance cannot be visualised as a tree.</p> <p>BioCypher can still handle these ontologies, and you can call <code>show_ontology_structure()</code> to get a visualisation of the ontology. However, each ontology class will only be added to the hierarchy tree once (a class with multiple parent classes is only placed under one parent in the hierarchy tree). Since this will occur the first time the class is seen, the ontology class might not be placed where you would expect it. This only applies to the visualisation; the underlying ontology is still correct and contains all ontology classes and their relationships.</p> <p>Note</p> <p>When calling <code>show_ontology_structure()</code>, BioCypher automatically checks if the ontology contains multiple inheritance and logs a warning message if so.</p> <p>If you need to get a visualisation of the ontology with multiple inheritance, you can call <code>show_ontology_structure()</code> with the parameter <code>to_disk=/some/path/where_to_store_the_file</code>. This creates a <code>GraphML</code> file and stores it at the specified location.</p>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#using-ontologies-plain-biolink","title":"Using ontologies: plain Biolink","text":"<p>BioCypher maps any input data to the underlying ontology; in the basic case, the Biolink model. This mapping is defined in the schema configuration (<code>schema_config.yaml</code>, see also here). In the simplest case, the representation of a concept in the knowledge graph to be built and the Biolink model class representing this concept are synonymous. For instance, the concept protein is represented by the Biolink class protein. To introduce proteins into the knowledge graph, one would simply define a node constituent with the class label protein. This is the mechanism we implicitly used for proteins in the basic tutorial (part 1); to reiterate:</p> schema_config.yaml<pre><code>protein:\n  represented_as: node\n  # ...\n</code></pre>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#model-extensions","title":"Model extensions","text":"<p>There are multiple reasons why a user might want to modify the basic model of the ontology or ontologies used. A class that is relevant to the user's task might be missing (Explicit inheritance). A class might not be granular enough, and the user would like to split it into subclasses based on distinct inputs (Implicit inheritance). For some very common use cases, we recommend going one step further and, maybe after some testing using the above \"soft\" model extensions, proposing the introduction of a new class to the model itself. For instance, Biolink is an open source community project, and new classes can be requested by opening an issue or filing a pull request directly on the Biolink model GitHub repository. Similar mechanisms apply for OBO Foundry ontologies.</p> <p>BioCypher provides further methods for ontology manipulation. The name of a class of the model may be too unwieldy for the use inside the desired knowledge graph, and the user would like to introduce a synonym/alias (Synonyms). Finally, the user might want to extend the basic model with another, more specialised ontology (Hybridising ontologies).</p>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#explicit-inheritance","title":"Explicit inheritance","text":"<p>Explicit inheritance is the most straightforward way of extending the basic model. It is also the most common use case. For instance, the Biolink model does not contain a class for <code>protein isoform</code>, and neither does it contain a relationship class for <code>protein protein interaction</code>, both of which we have already used in the basic tutorial. Since protein isoforms are specific types of protein, it makes sense to extend the existing Biolink model class <code>protein</code> with the concept of protein isoforms. To do this, we simply add a new class <code>protein isoform</code> to the schema configuration, and specify that it is a subclass of <code>protein</code> using the (optional) <code>is_a</code> field:</p> schema_config.yaml<pre><code>protein isoform:\n  is_a: protein\n  represented_as: node\n  # ...\n</code></pre> <p>Explicit inheritance can also be used to introduce new relationship classes. However, if the output is a Neo4j graph, these relationships must be represented as nodes to provide full functionality, since edges do not allow multiple labels. This does not mean that explicit inheritance cannot be used in edges; it is even recommended to do so to situate all components of the knowledge graph in the ontological hierarchy. However, to have the ancestry represented in the resulting Neo4j graph DB, multiple labels are required. For instance, we have already used the <code>protein protein interaction</code> relationship in the basic tutorial (part 6), making it a child of the Biolink model class <code>pairwise molecular interaction</code>. To reiterate:</p> schema_config.yaml<pre><code>protein protein interaction:\n  is_a: pairwise molecular interaction\n  represented_as: node\n  # ...\n</code></pre> <p>The <code>is_a</code> field can be used to specify multiple inheritance, i.e., multiple ancestor classes and their direct parent-child relationships can be created by specifying multiple classes (as a list) in the <code>is_a</code> field. For instance, if we wanted to further extend the protein-protein interaction with a more specific <code>enzymatic interaction</code> class, we could do so as follows:</p> schema_config.yaml<pre><code>enzymatic interaction:\n  is_a: [protein protein interaction, pairwise molecular interaction]\n  represented_as: node\n  # ...\n</code></pre> <p>Note</p> <p>To create this multiple inheritance chain, we do not require the creation of a <code>protein protein interaction</code> class as shown above; all intermediary classes are automatically created by BioCypher and inserted into the ontological hierarchy. To obtain a continuous ontology tree, the target class (i.e., the last in the list) must be a real Biolink model class.</p>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#implicit-inheritance","title":"Implicit inheritance","text":"<p>The base model (in the standard case, Biolink) can also be extended without specifying an explicit <code>is_a</code> field. This \"implicit\" inheritance happens when a class has multiple input labels that each refer to a distinct preferred identifier. In other words, if both the <code>input_label</code> and the <code>preferred_id</code> fields of a schema configuration class are lists, BioCypher will automatically create a subclass for each of the preferred identifiers. This is demonstrated in part 3 of the basic tutorial.</p> <p>Caution</p> <p>If only the <code>input_label</code> field - but not the <code>preferred_id</code> field - is a list, BioCypher will merge the inputs instead. This is useful for cases where different input streams should be unified under the same class label. See part 2 of the basic tutorial for more information.</p> <p>To make this more concrete, let's consider the example of <code>pathway</code> annotations. There are multiple projects that provide pathway annotations, such as Reactome and Wikipathways, and, in contrast to proteins, pathways are not easily mapped one-to-one. For classes where mapping is difficult or even impossible, we can use implicit subclassing instead. The Biolink model contains a <code>pathway</code> class, which we can use as a parent class of the Reactome and Wikipathways classes; we simply need to provide the pathways as two separate inputs with their own labels (e.g., \"react\" and \"wiki\"), and specify a corresponding list of preferred identifiers in the <code>preferred_id</code> field:</p> schema_config.yaml<pre><code>pathway:\n  represented_as: node\n  preferred_id: [reactome, wikipathways]\n  input_label: [react, wiki]\n  # ...\n</code></pre> <p>This will prompt BioCypher to create two subclasses of <code>pathway</code>, one for each input, and to map the input data to these subclasses. In the resulting knowledge graph, the Reactome and Wikipathways pathways will be represented as distinct classes by prepending the preferred identifier to the class label: <code>Reactome.Pathway</code> and <code>Wikipathways.Pathway</code>. By virtue of BioCypher's multiple labelling paradigm, those nodes will also inherit the <code>Pathway</code> class label as well as all parent labels and mixins of <code>Pathway</code> (<code>BiologicalProcess</code>, etc.). This allows us to query the graph for all <code>Pathway</code> nodes as well as for specific datasets depending on the desired granularity.</p> <p>Note</p> <p>This also works for relationships, but in this case, not the preferred identifiers but the sources (defined in the <code>source</code> field) are used to create the subclasses.</p>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#synonyms","title":"Synonyms","text":"<p>Note: Tutorial Files</p> <p>The code for this tutorial can be found at <code>tutorial/07__synonyms.py</code>. Schema files are at <code>tutorial/07_schema_config.yaml</code>, configuration in <code>tutorial/07_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p> <p>In some cases, an ontology may contain a biological concept, but the name of the concept does for some reason not agree with the users desired knowledge graph structure. For instance, the user may not want to represent protein complexes in the graph as <code>macromolecular complex</code> nodes due to ease of use and/or readability criteria and rather call these nodes <code>complex</code>. In such cases, the user can introduce a synonym for the ontology class. This is done by selecting another, more desirable name for the respective class(es) and specifying the <code>synonym_for</code> field in their schema configuration. In this case, as we would like to represent protein complexes as <code>complex</code> nodes, we can do so as follows:</p> schema_config.yaml<pre><code>complex:\n  synonym_for: macromolecular complex\n  represented_as: node\n  # ...\n</code></pre> <p>Importantly, BioCypher preserves these mappings to enable compatibility between different structural instantiations of the ontology (or combination of ontologies). All entities that are mapped to ontology classes in any way can be harmonised even between different types of concrete representations.</p> <p>Note</p> <p>It is essential that the desired class name is used as the main class key in the schema configuration, and the ontology class name is given in the <code>synonym_for</code> field. The name given in the <code>synonym_for</code> field must be an existing class name (in this example, a real Biolink class).</p> <p>We can visualise the structure of the ontology as we have before. Instead of using <code>bc.show_ontology_structure()</code> however, we can use the <code>bc.summary()</code> method to show the structure and simultaneously check for duplicates and missing labels. This is useful for debugging purposes, and we can see that the import was completed without encountering duplicates, and all labels in the input are accounted for in the schema configuration. We also observe in the tree that the <code>complex</code> class is now a synonym for the <code>macromolecular complex</code> class (their being synonyms indicated as an equals sign):</p> <pre><code>Showing ontology structure based on https://raw.githubusercontent.com/biolink/biolink-model/v3.2.1/biolink-model.owl.ttl\nentity\n\u251c\u2500\u2500 association\n\u2502   \u2514\u2500\u2500 gene to gene association\n\u2502       \u2514\u2500\u2500 pairwise gene to gene interaction\n\u2502           \u2514\u2500\u2500 pairwise molecular interaction\n\u2502               \u2514\u2500\u2500 protein protein interaction\n\u2514\u2500\u2500 named thing\n    \u2514\u2500\u2500 biological entity\n        \u251c\u2500\u2500 complex = macromolecular complex\n        \u2514\u2500\u2500 polypeptide\n            \u2514\u2500\u2500 protein\n                \u251c\u2500\u2500 entrez.protein\n                \u251c\u2500\u2500 protein isoform\n                \u2514\u2500\u2500 uniprot.protein\n</code></pre>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#hybridising-ontologies","title":"Hybridising ontologies","text":"<p>A broad, general ontology is a useful tool for knowledge representation, but often the task at hand requires more specific and granular concepts. In such cases, it is possible to hybridise the general ontology with a more specific one. For instance, there are many different types of sequence variants in biology, but Biolink only provides a generic \"sequence variant\" class (and it clearly exceeds the scope of Biolink to provide granular classes for all thinkable cases). However, there are many specialist ontologies, such as the Sequence Ontology (SO), which provides a more granular representation of sequence variants, and MONDO, which provides a more granular representation of diseases.</p> <p>To hybridise the Biolink model with the SO and MONDO, we can use the generic ontology adapter class of BioCypher by providing \"tail ontologies\" as dictionaries consisting of an OWL format ontology file and a set of nodes, one in the head ontology (which by default is Biolink), and one in the tail ontology. Each of the tail ontologies will then be joined to the head ontology to form the hybridised ontology at the specified nodes. It is up to the user to make sure that the concept at which the ontologies shall be joined makes sense as a point of contact between the ontologies; ideally, it is the exact same concept.</p> <p>Hint</p> <p>If the concept does not exist in the head ontology, but is a feasible child class of an existing concept, you can set the <code>merge_nodes</code> option to <code>False</code> to prevent the merging of head and tail join nodes, but instead adding the tail join node as a child of the head join node you have specified. For instance, in the example below, we merge <code>sequence variant</code> from Biolink and <code>sequence_variant</code> from Sequence Ontology into a single node, but we add the MONDO subtree of <code>human disease</code> as a child of <code>disease</code> in Biolink.</p> <p><code>merge_nodes</code> is set to <code>True</code> by default, so there is no need to specify it in the configuration file if you want to merge the nodes.</p> <p>The ontology adapter also accepts any arbitrary \"head ontology\" as a base ontology, but if none is provided, the Biolink model is used as the default head ontology. However, it is strongly recommended to explicitly specify your desired ontology version here. These options can be provided to the BioCypher interface as parameters, or as options in the BioCypher configuration file, which is the preferred method for transparency reasons:</p> Using biocypher_config.yaml<pre><code># ...\n\nbiocypher:  # biocypher settings\n\n  # Ontology configuration\n  head_ontology:\n    url: https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl\n    root_node: entity\n\n  tail_ontologies:\n\n    so:\n      url: data/so.owl\n      head_join_node: sequence variant\n      tail_join_node: sequence_variant\n\n    mondo:\n      url: http://purl.obolibrary.org/obo/mondo.owl\n      head_join_node: disease\n      tail_join_node: human disease\n      merge_nodes: false\n\n# ...\n</code></pre> <p>Note</p> <p>The <code>url</code> parameter can be either a local path or a URL to a remote resource.</p> <p>If you need to pass the ontology configuration programmatically, you can do so as follows at BioCypher interface instantiation:</p> Programmatic usage<pre><code>bc = BioCypher(\n    # ...\n\n    head_ontology={\n      'url': 'https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl',\n      'root_node': 'entity',\n    },\n\n    tail_ontologies={\n        'so':\n            {\n                'url': 'test/ontologies/so.owl',\n                'head_join_node': 'sequence variant',\n                'tail_join_node': 'sequence_variant',\n            },\n        'mondo':\n            {\n                'url': 'test/ontologies/mondo.owl',\n                'head_join_node': 'disease',\n                'tail_join_node': 'human disease',\n                'merge_nodes': False,\n            }\n    },\n\n    # ...\n)\n</code></pre>"},{"location":"learn/tutorials/tutorial003_adapters/","title":"Tutorial - Adapters","text":"<p>Note</p> <p>For a list of existing and planned adapters, please see here. You can also get an overview of pipelines and the adapters they use in our meta graph.</p> <p></p> <p>Note</p> <p>To facilitate the creation of a BioCypher pipeline, we have created a template repository that can be used as a starting point for your own adapter. It contains a basic structure for an adapter, as well as a script that can be used as a blueprint for a build pipeline. The repository can be found here.</p> <p>A \"BioCypher adapter\" is a python program responsible for connecting to the BioCypher core and providing it with the data from its associated resource. In doing so, it should adhere to several design principles to ensure simple interoperability between the core and multiple adapters. In essence, an adapter should conform to an interface that is defined by the core to give information about the nodes and edges the adapter provides to enable automatic harmonisation of the contents. An adapter can be \"primary\", i.e., responsible for a single \"atomic\" resource (e.g. UniProt, Reactome, etc.), or \"secondary\", i.e., connecting to a resource that is itself a combination of multiple primary resources (e.g. OmniPath, Open Targets, etc.). Due to extensive prior harmonisation, the latter is often easier to implement and thus is a good starting point that can be subsequently extended to and replaced by primary adapters.</p> <p>Warning</p> <p>The adapter interface is still under development and may change rapidly.</p>"},{"location":"learn/tutorials/tutorial003_adapters/#adapter-philosophy","title":"Adapter philosophy","text":"<p>There are currently two 'flavours' of adapters. The first is simpler and used in workflows that are similar to harmonisation scripts, where the BioCypher interface is instantiated in the same script as the adapter(s). In the second, the BioCypher interface is contained in the adapter class, which makes for a more complex architecture, but allows for more involved workflows. In pseudo-code, the two approaches look like this:</p> Simple adapter<pre><code>from biocypher import BioCypher\nfrom adapter import Adapter\n\nbc = BioCypher()\nadapter = Adapter()\n\nbc.write_nodes(adapter.get_nodes())\n</code></pre> <p>Here, the script file is the central point of control, orchestrating the entire interaction between the BioCypher core and the adapter(s). Examples of this simpler format are the Open Targets KG and the CROssBAR v2.</p> <p>On the other hand, the more involved approach looks like this:</p> Adapter base class<pre><code>from biocypher import BioCypher\n\nclass Adapter:\n    __bc = BioCypher()\n\n    def __init__(self):\n        # setup\n\n    @classmethod\n    def get_biocypher(cls):\n        return Adapter.__bc\n\n    def get_nodes(self):\n        # ...\n        return nodes\n\n    def write_nodes(self):\n        Adapter.get_biocypher.write_nodes(self.get_nodes())\n</code></pre> <p>Here, the adapter class (and adapters inheriting from it) contains a singleton instance of the BioCypher interface. Thus, the adapter needs to provide BioCypher functionality to the outside via dedicated methods. This allows for more complex workflows, for instance, reducing clutter when executing multiple adapters in a single for-loop, or writing from a stream of data, e.g. in a Neo4j transaction (which happens inside the adapter).</p> Main script<pre><code>from adapters import AdapterChild1, AdapterChild2\n\nadapters = [AdapterChild1(), AdapterChild2()]\n\nfor adapter in adapters:\n    adapter.write_nodes()\n</code></pre> <p>Examples of this approach are the IGVF Knowledge Graph and the Clinical Knowledge Graph migration.</p> <p>Note</p> <p>While there are differences in implementation details, both approaches are largely functionally equivalent. At the current time, there is no clear preference for one over the other; both are used. As the ecosystem matures and more high-level functionality is added (e.g. the pipeline), advantages of one approach over the other may become more apparent.</p>"},{"location":"learn/tutorials/tutorial003_adapters/#adapter-functions","title":"Adapter functions","text":"<p>In general, a single adapter fulfils the following tasks:</p> <ol> <li> <p>Loading the data from the primary resource, for instance by using pypath download / caching functions (as in the UniProt example adapter), by using columnar distributed data formats such as Parquet (as in the Open Targets example adapter), by using a running database instance (as in the CKG example adapter), or by simply reading a file from disk (as in the Dependency Map example adapter). Generally, any method that allows the efficient transfer of the data from adapter to BioCypher core is acceptable.</p> </li> <li> <p>Passing the data to BioCypher as a stream or list to be written to the used DBMS (or application) via a Python driver (\"online\") or via batch import (e.g. from CSV files).  The latter has the advantage of high throughput and a low memory footprint, while the former allows for a more interactive workflow but is often much slower, thus making it better suited for small incremental updates.</p> </li> <li> <p>Providing or connecting to additional functionality that is useful for the creation of knowledge graphs, such as identifier translation (e.g. via pypath.mapping as in the UniProt example adapter), or identifier and prefix standardisation and validation (e.g. via Bioregistry as in the UniProt example adapter and others).</p> </li> </ol> <p>Note</p> <p>For developers: We follow a design philosophy of \"separation of concerns\" in BioCypher. This means that the core should not be concerned with the details of how data is loaded, but only with the data itself. This is why the core does not contain any code for loading data from a resource, but only for writing it to the database. The adapter is responsible for loading the data and passing it to the core, which allows for a more modular design and makes it easier to maintain, extend, and reuse the code.</p> <p>For introduction of new features, we recommend to first implement them in the adapter, and to move them to the core only if they have shown to be useful for multiple adapters.</p>"},{"location":"learn/tutorials/tutorial003_adapters/#1-loading-the-data","title":"1. Loading the Data","text":"<p>Depending on the data source, it is up to the developer of the adapter to find and define a suitable representation to be piped into BioCypher; for instance, in out <code>pypath</code> adapter, we load the entire <code>pypath</code> object into memory to be passed to BioCypher using a generator that evaluates each <code>pypath</code> object and transforms it to the tuple representation described below. This is made possible by the \"pre-harmonised\" form in which the data is represented within <code>pypath</code>. For more heterogeneous data representations, additional transformations may be necessary before piping into BioCypher.</p> <p>For larger datasets, it can be beneficial to adopt a streaming approach or batch processing, as demonstrated in the Open Targets adapter and the CKG adapter. BioCypher can handle input streams of arbitrary length via Python generators.</p>"},{"location":"learn/tutorials/tutorial003_adapters/#2-passing-the-data","title":"2. Passing the Data","text":"<p>We currently pass data into BioCypher as a collection of tuples. Nodes are represented as 3-tuples, containing: - the node ID (unique in the space of the knowledge graph, ideally a CURIE with   a prefix registered in the Bioregistry) - the node type, i.e., its label (this is the string that is mapped to an   ontological class via the <code>input_label</code> field in the schema configuration) - a dictionary of node attributes</p> <p>While edges are represented as 5-tuples, containing: - the (optional) relationship ID (unique in the space of the KG) - the source node ID (referring to a unique node ID in the KG) - the target node ID (referring to a unique node ID in the KG) - the relationship type, i.e., its label (this is the string that is mapped to   an ontological class via the <code>input_label</code> field in the schema configuration) - a dictionary of relationship attributes</p> <p>Note</p> <p>This representation will probably be subject to change soon and yield to a more standardised interface for nodes and edges, derived from a BioCypher core class. We refer to this development in an issue.</p>"},{"location":"learn/tutorials/tutorial003_adapters/#strict-mode","title":"Strict mode","text":"<p>We can activate BioCypher strict mode with the <code>strict_mode</code> option in the configuration. In strict mode, the driver will raise an error if it encounters a node or edge without data source, version, and licence. These currently need to be provided as part of the node and edge attribute dictionaries, with the reserved keywords <code>source</code>, <code>version</code>, and <code>licence</code> (or <code>license</code>). This may change to a more rigorous implementation in the future.</p>"},{"location":"reference/","title":"Index","text":"<ul> <li> <p> API Reference</p> <p>The BioCypher's API reference provides a comprehensive overview of the API, detailing its methods, available parameters, and their functionality. This guide is designed for users who have a foundational understanding of key BioCypher concepts.</p> <p> To the reference guide</p> </li> </ul> <ul> <li> <p> BioCypher Configuration Reference</p> <p>The BioCypher configuration reference offers an in-depth guide to the settings, fields, and parameters that control BioCypher\u2019s behavior and execution. It serves as a resource for customizing BioCypher to fit specific use cases and workflows.</p> <p> To the Biocypher configuration reference</p> </li> </ul> <ul> <li> <p> Schema Configuration Reference</p> <p>The Schema configuration reference explains the parameters used in the YAML file that defines the structure of the data model. It provides guidance on mapping entities and relationships to ensure seamless integration with BioCypher.</p> <p> To the Schema configuration reference</p> </li> </ul>"},{"location":"reference/api-reference/","title":"Api reference","text":"<ul> <li> <p> Set up in 5 minutes</p> <p>Install <code>mkdocs-material</code> with <code>pip</code> and get up and running in minutes</p> <p> Getting started</p> </li> <li> <p> It's just Markdown</p> <p>Focus on your content and generate a responsive and searchable static site</p> <p> Reference</p> </li> <li> <p> Made to measure</p> <p>Change the colors, fonts, language, icons, logo and more with a few lines</p> <p> Customization</p> </li> <li> <p> Open Source, MIT</p> <p>Material for MkDocs is licensed under MIT and available on [GitHub]</p> <p> License</p> </li> </ul>"},{"location":"reference/biocypher-config-guide/","title":"BioCypher Configuration Reference","text":"<p>BioCypher comes with a default set of configuration parameters. You can overwrite them by creating a <code>biocypher_config.yaml</code> file in the root directory or the <code>config</code> directory of your project. You only need to specify the ones you wish to override from default. If you want to create global user settings, you can create a <code>biocypher_config.yaml</code> in your default BioCypher user directory (as found using <code>appdirs.user_config_dir('biocypher')</code>). For instance, on Mac OS, this would be <code>~/Library/Caches/biocypher/biocypher_config.yaml</code>. Finally, you can also point an instance of the BioCypher class to any YAML file using the biocypher_config_path parameter.</p> <p>Note</p> <p>It is important to follow the rules of indentation in the YAML file. BioCypher module configuration is found under the top-level keyword biocypher, while the settings for DBMS systems (e.g., Neo4j) are found under their respective keywords (e.g., neo4j).</p> <p>Quote characters</p> <p>If possible, avoid using quote characters in your YAML files. If you need to quote, for instance a tab delimiter (<code>\\t</code>), use single quotes (<code>'</code>), since double quotes (<code>\"</code>) allow parsing of escape characters in YAML, which can cause issues downstream. It is safe to use double quotes to quote a single quote character (<code>\"'\"</code>).</p> <p>Configuration files are read in the order <code>default -&gt; user level -&gt; project level</code>, with the later ones overriding the preceding. The following parameters are available:</p>"},{"location":"reference/biocypher-config-guide/#purpose","title":"Purpose","text":"<p>The configuration in BioCypher customizes its behavior by overriding default settings through a <code>biocypher_config.yaml</code> file. It ensures flexibility for different use cases by allowing you to define data sources, database connections, and output formats.</p>"},{"location":"reference/biocypher-config-guide/#convention-for-naming","title":"Convention for naming","text":"<p>It is important to follow the rules of indentation in the YAML file. BioCypher module configuration is found under the top-level keyword <code>biocypher</code>, while the settings for DBMS systems (e.g., Neo4j) are found under their respective keywords (e.g., <code>neo4j</code>).</p> <p>If possible, avoid using quote characters in your YAML files. If you need to quote, for instance a tab delimiter (<code>\\t</code>), use single quotes (<code>'</code>), since double quotes (<code>\"</code>) allow parsing of escape characters in YAML, which can cause issues downstream. It is safe to use double quotes to quote a single quote character (<code>\"'\"</code>).</p> <p>Quote characters</p> <p>If possible, avoid using quote characters in your YAML files. If you need to quote, for instance a tab delimiter (<code>\\t</code>), use single quotes (<code>'</code>), since double quotes (<code>\"</code>) allow parsing of escape characters in YAML, which can cause issues downstream. It is safe to use double quotes to quote a single quote character (<code>\"'\"</code>).</p> <p>Configuration files are read in the order <code>default -&gt; user level -&gt; project level</code>, with the later ones overriding the preceding. The following parameters are available:</p>"},{"location":"reference/biocypher-config-guide/#yaml-file-skeleton","title":"YAML file Skeleton","text":"biocypher_config.yaml<pre><code>biocypher:\n  #---- REQUIRED PARAMETERS\n\n  dbms: neo4j\n  schema_config_path: config/schema_config.yaml\n  offline: true\n  strict_mode: false\n  head_ontology:\n    url: https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl\n    root_node: entity\n    switch_label_and_id: true\n\n  #---- OPTIONAL PARAMETERS\n  log_to_disk: true\n\n  debug: true\n\n  log_directory: biocypher-log\n\n  output_directory: biocypher-out\n\n  cache_directory: .cache\n\n  #---- OPTIONAL TAIL ONTOLOGIES\n\n  # tail_ontologies:\n  #   so:\n  #     url: test/ontologies/so.owl\n  #     head_join_node: sequence variant\n  #     tail_join_node: sequence_variant\n  #     switch_label_and_id: true\n  #   mondo:\n  #     url: test/ontologies/mondo.owl\n  #     head_join_node: disease\n  #     tail_join_node: disease\n  #     switch_label_and_id: true\n\n#-------------------------------------------------------------------\n#-----------------       OUTPUT Configuration      -----------------\n#-------------------------------------------------------------------\n#---- NEO4J database management system\nneo4j:\n  database_name: neo4j\n  wipe: true\n\n  uri: neo4j://localhost:7687\n  user: neo4j\n  password: neo4j\n\n  delimiter: \";\"\n  array_delimiter: \"|\"\n  quote_character: \"'\"\n\n  multi_db: true\n\n  skip_duplicate_nodes: false\n  skip_bad_relationships: false\n\n  # import_call_bin_prefix: bin/\n  # import_call_file_prefix: path/to/files/\n\n#---- PostgreSQL database management system\npostgresql:\n  database_name: postgres\n\n  host: localhost # host\n  port: 5432 # port\n\n  user: postgres\n  password: postgres # password\n\n  quote_character: '\"'\n  delimiter: '\\t'\n  # import_call_bin_prefix: '' # path to \"psql\"\n  # import_call_file_prefix: '/path/to/files'\n\n#---- SQLite database management system\nsqlite:\n  ### SQLite configuration ###\n\n  # SQLite connection credentials\n  database_name: sqlite.db # DB name\n\n  # SQLite import batch writer settings\n  quote_character: '\"'\n  delimiter: '\\t'\n  # import_call_bin_prefix: '' # path to \"sqlite3\"\n  # import_call_file_prefix: '/path/to/files'\n\n#---- RDF (Resource Description Framework) data model\nrdf:\n  ### RDF configuration ###\n  rdf_format: turtle\n\n#---- NetworkX graph data model\nnetworkx:\n  ### NetworkX configuration ###\n  some_config: some_value # placeholder for technical reasons TODO\n\n#---- CSV (Comma-Separated Values) text file format\ncsv:\n  ### CSV/Pandas configuration ###\n  delimiter: \",\"\n</code></pre>"},{"location":"reference/biocypher-config-guide/#fields-reference","title":"Fields reference:","text":""},{"location":"reference/biocypher-config-guide/#biocypher-section-parameters","title":"Biocypher section parameters","text":""},{"location":"reference/biocypher-config-guide/#required-parameters","title":"Required parameters","text":""},{"location":"reference/biocypher-config-guide/#dbms","title":"<code>dbms</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#head_ontology","title":"<code>head_ontology</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#offline","title":"<code>offline</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#root_node","title":"<code>root_node</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#schema_config_path","title":"<code>schema_config_path</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#strict_mode","title":"<code>strict_mode</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#switch_label_and_id","title":"<code>switch_label_and_id</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#url","title":"<code>url</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#optional-parameters","title":"Optional parameters","text":""},{"location":"reference/biocypher-config-guide/#cache_directory","title":"<code>cache_directory</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#debug","title":"<code>debug</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#log_directory","title":"<code>log_directory</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#log_to_disk","title":"<code>log_to_disk</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#output_directory","title":"<code>output_directory</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#switch_label_and_id_1","title":"<code>switch_label_and_id</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#tail_join_node","title":"<code>tail_join_node</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#tail_ontologies","title":"<code>tail_ontologies</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#url_1","title":"<code>url</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#output-configuration-parameters","title":"Output configuration parameters","text":""},{"location":"reference/biocypher-config-guide/#neo4j-dbms","title":"NEO4j DBMS","text":""},{"location":"reference/biocypher-config-guide/#array_delimiter","title":"<code>array_delimiter</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#database_name","title":"<code>database_name</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#delimiter","title":"<code>delimiter</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#import_call_bin_prefix","title":"<code>import_call_bin_prefix</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#import_call_file_prefix","title":"<code>import_call_file_prefix</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#multi_db","title":"<code>multi_db</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#password","title":"<code>password</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#quote_character","title":"<code>quote_character</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#skip_duplicate_nodes","title":"<code>skip_duplicate_nodes</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#skip_bad_relationships","title":"<code>skip_bad_relationships</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#uri","title":"<code>uri</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#user","title":"<code>user</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#wipe","title":"<code>wipe</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#postgresql-dbms","title":"PostgreSQL DBMS","text":""},{"location":"reference/biocypher-config-guide/#database_name_1","title":"<code>database_name</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#delimiter_1","title":"<code>delimiter</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#host","title":"<code>host</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#import_call_bin_prefix_1","title":"<code>import_call_bin_prefix</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#import_call_file_prefix_1","title":"<code>import_call_file_prefix</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#password_1","title":"<code>password</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#port","title":"<code>port</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#quote_character_1","title":"<code>quote_character</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#user_1","title":"<code>user</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#sqlite-dbms","title":"SQLite DBMS","text":""},{"location":"reference/biocypher-config-guide/#database_name_2","title":"<code>database_name</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#delimiter_2","title":"<code>delimiter</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#import_call_bin_prefix_2","title":"<code>import_call_bin_prefix</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#import_call_file_prefix_2","title":"<code>import_call_file_prefix</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#quote_character_2","title":"<code>quote_character</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#rdf-data-model","title":"RDF data model","text":""},{"location":"reference/biocypher-config-guide/#rdf_format","title":"<code>rdf_format</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#networkx-graph-data-model","title":"NetworkX graph data model","text":""},{"location":"reference/biocypher-config-guide/#some_config","title":"<code>some_config</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#networkx-graph-data-model_1","title":"NetworkX graph data model","text":""},{"location":"reference/biocypher-config-guide/#some_config_1","title":"<code>some_config</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/biocypher-config-guide/#csv-file-format","title":"CSV file format","text":""},{"location":"reference/biocypher-config-guide/#delimiter_3","title":"<code>delimiter</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/schema-config-guide/","title":"Schema Configuration Reference","text":""},{"location":"reference/schema-config-guide/#purpose","title":"Purpose:","text":""},{"location":"reference/schema-config-guide/#convention-for-naming","title":"Convention for naming:","text":""},{"location":"reference/schema-config-guide/#skeleton","title":"Skeleton:","text":"<pre><code>#-------------------------------------------------------------------\n#---- Title: Schema Configuration file example\n#---- Authors: &lt;author 1&gt;, &lt;author 2&gt;\n#---- Description: Schema to load information related to proteins, and\n#                  and their interactions.\n#\n#-------------------------------------------------------------------\n\n#-------------------------------------------------------------------\n#-------------------------      NODES      -------------------------\n#-------------------------------------------------------------------\n#=========    PARENT NODES\nprotein:\n  represented_as: node\n  preferred_id: [uniprot, entrez]\n  input_label: [uniprot_protein, entrez_protein]\n  properties:\n    sequence: str\n    description: str\n    taxon: str\n    mass: int\n\n#=========    INHERITED NODES\nprotein isoform:\n  is_a: protein\n  inherit_properties: true\n  represented_as: node\n  preferred_id: uniprot\n  input_label: uniprot_isoform\n\n#-------------------------------------------------------------------\n#------------------      RELATIONSHIPS (EDGES)     -----------------\n#-------------------------------------------------------------------\n#=========    PARENT EDGES\nprotein protein interaction:\n  is_a: pairwise molecular interaction\n  represented_as: edge\n  preferred_id: intact\n  input_label: interacts_with\n  properties:\n      method: str\n      source: str\n\n#=========    INHERITED EDGES\n\n#=========    EDGES AS NODES\n\n\n#--------------------------------------------------------------------\n#---- Dictionary of custom keywords: add additional keywords if you\n#     need it. Please document each new keyword as in the following\n#     template. DO NOT DELETE the hash symbol (#) in each line.\n\n# &lt;keyword's name&gt;\n#    Description:\n#    Possible values:\n#        - possible value 1 [*datatype*]\n#        - possible value 2 [*datatype*]\n#\n</code></pre>"},{"location":"reference/schema-config-guide/#fields-reference","title":"Fields reference:","text":""},{"location":"reference/schema-config-guide/#exclude_properties","title":"<code>exclude_properties</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/schema-config-guide/#inherit_properties","title":"<code>inherit_properties</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/schema-config-guide/#input_label","title":"<code>input_label</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/schema-config-guide/#is_a","title":"<code>is_a</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/schema-config-guide/#label_as_edge","title":"<code>label_as_edge</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/schema-config-guide/#preferred_id","title":"<code>preferred_id</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/schema-config-guide/#properties","title":"<code>properties</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/schema-config-guide/#represented_as","title":"<code>represented_as</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/schema-config-guide/#source","title":"<code>source</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/schema-config-guide/#synonym_for","title":"<code>synonym_for</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/schema-config-guide/#target","title":"<code>target</code>","text":"<ul> <li>Description: describe briefly the purpose of this property.</li> <li>Possible values:</li> <li>possible value 1 [datatype]</li> <li>possible value 2 [datatype]</li> </ul>"},{"location":"reference/schema-config-guide/#add-custom-fields","title":"Add custom fields","text":"<p>Tip</p> <p>Do not forget to document your custom keywords at the end of the schema config file, this is especially useful if you share your schema configuration file with others. They will understand what is the purpose of those new keywords.</p> <p>You can use other keywords for local functionalities without interfering with the default ones. For instance, a particular user has added the <code>db_collection_name</code> field for its own purposes.</p> Example: schema configuration with a custom keyword<pre><code>#...\nprotein:\n  represented_as: node\n  preferred_id: uniprot\n  input_label: protein\n  db_collection_name: proteins\n  properties:\n    name: str\n    score: float\n    taxon: int\n    genes: str[]\n#...\n</code></pre>"},{"location":"reference/source/","title":"BioCypher","text":""},{"location":"reference/source/#modules","title":"Modules","text":""},{"location":"reference/source/#_corepy","title":"_core.py","text":"<p>BioCypher core module.</p> <p>Interfaces with the user and distributes tasks to submodules.</p>"},{"location":"reference/source/#biocypher._core.BioCypher","title":"<code>BioCypher</code>","text":"<p>Orchestration of BioCypher operations.</p> <p>Instantiate this class to interact with BioCypher.</p> <pre><code>dbms (str): The database management system to use. For supported\n    systems see SUPPORTED_DBMS.\n\noffline (bool): Whether to run in offline mode. In offline mode\n    the Knowledge Graph is written to files. In online mode, it\n    is written to a database or hold in memory.\n\nstrict_mode (bool): Whether to run in strict mode. If True, the\n    translator will raise an error if a node or edge does not\n    provide source, version, and licence information.\n\nbiocypher_config_path (str): Path to the BioCypher config file.\n\nschema_config_path (str): Path to the user schema config\n    file.\n\nhead_ontology (dict): The head ontology defined by URL ('url') and root\n    node ('root_node').\n\ntail_ontologies (dict): The tail ontologies defined by URL and\n    join nodes for both head and tail ontology.\n\noutput_directory (str): Path to the output directory. If not\n    provided, the default value 'biocypher-out' will be used.\n\ncache_directory (str): Path to the cache directory.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>class BioCypher:\n    \"\"\"Orchestration of BioCypher operations.\n\n    Instantiate this class to interact with BioCypher.\n\n    Args:\n    ----\n        dbms (str): The database management system to use. For supported\n            systems see SUPPORTED_DBMS.\n\n        offline (bool): Whether to run in offline mode. In offline mode\n            the Knowledge Graph is written to files. In online mode, it\n            is written to a database or hold in memory.\n\n        strict_mode (bool): Whether to run in strict mode. If True, the\n            translator will raise an error if a node or edge does not\n            provide source, version, and licence information.\n\n        biocypher_config_path (str): Path to the BioCypher config file.\n\n        schema_config_path (str): Path to the user schema config\n            file.\n\n        head_ontology (dict): The head ontology defined by URL ('url') and root\n            node ('root_node').\n\n        tail_ontologies (dict): The tail ontologies defined by URL and\n            join nodes for both head and tail ontology.\n\n        output_directory (str): Path to the output directory. If not\n            provided, the default value 'biocypher-out' will be used.\n\n        cache_directory (str): Path to the cache directory.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        dbms: str = None,\n        offline: bool = None,\n        strict_mode: bool = None,\n        biocypher_config_path: str = None,\n        schema_config_path: str = None,\n        head_ontology: dict = None,\n        tail_ontologies: dict = None,\n        output_directory: str = None,\n        cache_directory: str = None,\n        # legacy params\n        db_name: str = None,\n    ):\n        # Update configuration if custom path is provided\n        if biocypher_config_path:\n            _file_update(biocypher_config_path)\n\n        if db_name:\n            logger.warning(\n                \"The parameter `db_name` is deprecated. Please set the \"\n                \"`database_name` setting in the `biocypher_config.yaml` file \"\n                \"instead.\",\n            )\n            _config(**{db_name: {\"database_name\": db_name}})\n\n        # Load configuration\n        self.base_config = _config(\"biocypher\")\n\n        # Check for required configuration\n        for key in REQUIRED_CONFIG:\n            if key not in self.base_config:\n                msg = f\"Configuration key {key} is required.\"\n                raise ValueError(msg)\n\n        # Set configuration - mandatory\n        self._dbms = dbms or self.base_config[\"dbms\"]\n\n        if offline is None:\n            self._offline = self.base_config[\"offline\"]\n        else:\n            self._offline = offline\n\n        if strict_mode is None:\n            self._strict_mode = self.base_config[\"strict_mode\"]\n        else:\n            self._strict_mode = strict_mode\n\n        self._schema_config_path = schema_config_path or self.base_config.get(\n            \"schema_config_path\",\n        )\n\n        if not self._schema_config_path:\n            logger.warning(\"Running BioCypher without schema configuration.\")\n        else:\n            logger.info(\n                f\"Running BioCypher with schema configuration from {self._schema_config_path}.\",\n            )\n\n        self._head_ontology = head_ontology or self.base_config[\"head_ontology\"]\n\n        # Set configuration - optional\n        self._output_directory = output_directory or self.base_config.get(\n            \"output_directory\",\n        )\n        self._cache_directory = cache_directory or self.base_config.get(\n            \"cache_directory\",\n        )\n        self._tail_ontologies = tail_ontologies or self.base_config.get(\n            \"tail_ontologies\",\n        )\n\n        if self._dbms not in SUPPORTED_DBMS:\n            msg = f\"DBMS {self._dbms} not supported. Please select from {SUPPORTED_DBMS}.\"\n            raise ValueError(msg)\n\n        # Initialize\n        self._ontology_mapping = None\n        self._deduplicator = None\n        self._translator = None\n        self._downloader = None\n        self._ontology = None\n        self._writer = None\n        self._driver = None\n        self._in_memory_kg = None\n\n        self._in_memory_kg = None\n        self._nodes = None\n        self._edges = None\n\n    def _initialize_in_memory_kg(self) -&gt; None:\n        \"\"\"Create in-memory KG instance.\n\n        Set as instance variable `self._in_memory_kg`.\n        \"\"\"\n        if not self._in_memory_kg:\n            self._in_memory_kg = get_in_memory_kg(\n                dbms=self._dbms,\n                deduplicator=self._get_deduplicator(),\n            )\n\n    def add_nodes(self, nodes) -&gt; None:\n        \"\"\"Add new nodes to the internal representation.\n\n        Initially, receive nodes data from adaptor and create internal\n        representation for nodes.\n\n        Args:\n        ----\n            nodes(iterable): An iterable of nodes\n\n        \"\"\"\n        if isinstance(nodes, list):\n            self._nodes = list(itertools.chain(self._nodes, nodes))\n        else:\n            self._nodes = itertools.chain(self._nodes, nodes)\n\n    def add_edges(self, edges) -&gt; None:\n        \"\"\"Add new edges to the internal representation.\n\n        Initially, receive edges data from adaptor and create internal\n        representation for edges.\n\n        Args:\n        ----\n             edges(iterable): An iterable of edges.\n\n        \"\"\"\n        if isinstance(edges, list):\n            self._edges = list(itertools.chain(self._edges, edges))\n        else:\n            self._edges = itertools.chain(self._edges, edges)\n\n    def to_df(self):\n        \"\"\"Create DataFrame using internal representation.\n\n        TODO: to_df implies data frame, should be specifically that use case\n        \"\"\"\n        return self._to_KG()\n\n    def to_networkx(self):\n        \"\"\"Create networkx using internal representation.\"\"\"\n        return self._to_KG()\n\n    def _to_KG(self):\n        \"\"\"Convert the internal representation to knowledge graph.\n\n        The knowledge graph is returned based on the `dbms` parameter in\n        the biocypher configuration file.\n\n        Returns\n        -------\n             Any: knowledge graph.\n\n        \"\"\"\n        if not self._in_memory_kg:\n            self._initialize_in_memory_kg()\n        if not self._translator:\n            self._get_translator()\n        tnodes = self._translator.translate_entities(self._nodes)\n        tedges = self._translator.translate_entities(self._edges)\n        self._in_memory_kg.add_nodes(tnodes)\n        self._in_memory_kg.add_edges(tedges)\n        return self._in_memory_kg.get_kg()\n\n    def _get_deduplicator(self) -&gt; Deduplicator:\n        \"\"\"Create deduplicator if not exists and return.\"\"\"\n        if not self._deduplicator:\n            self._deduplicator = Deduplicator()\n\n        return self._deduplicator\n\n    def _get_ontology_mapping(self) -&gt; OntologyMapping:\n        \"\"\"Create ontology mapping if not exists and return.\"\"\"\n        if not self._schema_config_path:\n            self._ontology_mapping = OntologyMapping()\n\n        if not self._ontology_mapping:\n            self._ontology_mapping = OntologyMapping(\n                config_file=self._schema_config_path,\n            )\n\n        return self._ontology_mapping\n\n    def _get_ontology(self) -&gt; Ontology:\n        \"\"\"Create ontology if not exists and return.\"\"\"\n        if not self._ontology:\n            self._ontology = Ontology(\n                ontology_mapping=self._get_ontology_mapping(),\n                head_ontology=self._head_ontology,\n                tail_ontologies=self._tail_ontologies,\n            )\n\n        return self._ontology\n\n    def _get_translator(self) -&gt; Translator:\n        \"\"\"Create translator if not exists and return.\"\"\"\n        if not self._translator:\n            self._translator = Translator(\n                ontology=self._get_ontology(),\n                strict_mode=self._strict_mode,\n            )\n\n        return self._translator\n\n    def _get_writer(self):\n        \"\"\"Create writer if not online.\n\n        Set as instance variable `self._writer`.\n        \"\"\"\n        if self._offline:\n\n            def timestamp() -&gt; str:\n                return datetime.now().strftime(\"%Y%m%d%H%M%S\")\n\n            outdir = self._output_directory or os.path.join(\n                \"biocypher-out\",\n                timestamp(),\n            )\n            self._output_directory = os.path.abspath(outdir)\n\n            self._writer = get_writer(\n                dbms=self._dbms,\n                translator=self._get_translator(),\n                deduplicator=self._get_deduplicator(),\n                output_directory=self._output_directory,\n                strict_mode=self._strict_mode,\n            )\n        else:\n            msg = \"Cannot get writer in online mode.\"\n            raise NotImplementedError(msg)\n\n        return self._writer\n\n    def _get_driver(self):\n        \"\"\"Create driver if not exists.\n\n        Set as instance variable `self._driver`.\n        \"\"\"\n        if not self._offline:\n            self._driver = get_connector(\n                dbms=self._dbms,\n                translator=self._get_translator(),\n            )\n        else:\n            msg = \"Cannot get driver in offline mode.\"\n            raise NotImplementedError(msg)\n\n        return self._driver\n\n    def _get_in_memory_kg(self):\n        \"\"\"Create in-memory KG instance.\n\n        Set as instance variable `self._in_memory_kg`.\n        \"\"\"\n        if not self._in_memory_kg:\n            self._in_memory_kg = get_in_memory_kg(\n                dbms=self._dbms,\n                deduplicator=self._get_deduplicator(),\n            )\n\n        return self._in_memory_kg\n\n    def _add_nodes(\n        self,\n        nodes,\n        batch_size: int = int(1e6),\n        force: bool = False,\n    ):\n        \"\"\"Add nodes to the BioCypher KG.\n\n        First uses the `_translator` to translate the nodes to `BioCypherNode`\n        objects. Depending on the configuration the translated nodes are then\n        passed to the\n\n        - `_writer`: if `_offline` is set to `False`\n\n        - `_in_memory_kg`: if `_offline` is set to `False` and the `_dbms` is an\n            `IN_MEMORY_DBMS`\n\n        - `_driver`: if `_offline` is set to `True` and the `_dbms` is not an\n            `IN_MEMORY_DBMS`\n\n        \"\"\"\n        if not self._translator:\n            self._get_translator()\n        translated_nodes = self._translator.translate_entities(nodes)\n\n        if self._offline:\n            passed = self._get_writer().write_nodes(\n                translated_nodes,\n                batch_size=batch_size,\n                force=force,\n            )\n        elif self._is_online_and_in_memory():\n            passed = self._get_in_memory_kg().add_nodes(translated_nodes)\n        else:\n            passed = self._get_driver().add_biocypher_nodes(translated_nodes)\n\n        return passed\n\n    def _add_edges(self, edges, batch_size: int = int(1e6)):\n        \"\"\"Add edges to the BioCypher KG.\n\n        First uses the `_translator` to translate the edges to `BioCypherEdge`\n        objects. Depending on the configuration the translated edges are then\n        passed to the\n\n        - `_writer`: if `_offline` is set to `False`\n\n        - `_in_memory_kg`: if `_offline` is set to `False` and the `_dbms` is an\n            `IN_MEMORY_DBMS`\n\n        - `_driver`: if `_offline` is set to `True` and the `_dbms` is not an\n            `IN_MEMORY_DBMS`\n\n        \"\"\"\n        if not self._translator:\n            self._get_translator()\n        translated_edges = self._translator.translate_entities(edges)\n\n        if self._offline:\n            if not self._writer:\n                self._initialize_writer()\n            passed = self._writer.write_edges(\n                translated_edges,\n                batch_size=batch_size,\n            )\n        elif self._is_online_and_in_memory():\n            if not self._in_memory_kg:\n                self._initialize_in_memory_kg()\n            passed = self._in_memory_kg.add_edges(translated_edges)\n        else:\n            if not self._driver:\n                self._initialize_driver()\n            passed = self._driver.add_biocypher_nodes(translated_edges)\n\n        return passed\n\n    def _is_online_and_in_memory(self) -&gt; bool:\n        \"\"\"Return True if in online mode and in-memory dbms is used.\"\"\"\n        return (not self._offline) &amp; (self._dbms in IN_MEMORY_DBMS)\n\n    def write_nodes(\n        self,\n        nodes,\n        batch_size: int = int(1e6),\n        force: bool = False,\n    ) -&gt; bool:\n        \"\"\"Write nodes to database.\n\n        Either takes an iterable of tuples (if given, translates to\n        ``BioCypherNode`` objects) or an iterable of ``BioCypherNode`` objects.\n\n        Args:\n        ----\n            nodes (iterable): An iterable of nodes to write to the database.\n            batch_size (int): The batch size to use when writing to disk.\n            force (bool): Whether to force writing to the output directory even\n                if the node type is not present in the schema config file.\n\n        Returns:\n        -------\n            bool: True if successful.\n\n        \"\"\"\n        return self._add_nodes(nodes, batch_size=batch_size, force=force)\n\n    def write_edges(self, edges, batch_size: int = int(1e6)) -&gt; bool:\n        \"\"\"Write edges to database.\n\n        Either takes an iterable of tuples (if given, translates to\n        ``BioCypherEdge`` objects) or an iterable of ``BioCypherEdge`` objects.\n\n        Args:\n        ----\n            edges (iterable): An iterable of edges to write to the database.\n\n        Returns:\n        -------\n            bool: True if successful.\n\n        \"\"\"\n        return self._add_edges(edges, batch_size=batch_size)\n\n    def add(self, entities) -&gt; None:\n        \"\"\"Add entities to the in-memory database.\n\n        Accepts an iterable of tuples (if given, translates to\n        ``BioCypherNode`` or ``BioCypherEdge`` objects) or an iterable of\n        ``BioCypherNode`` or ``BioCypherEdge`` objects.\n\n        Args:\n        ----\n            entities (iterable): An iterable of entities to add to the database.\n                Can be 3-tuples (nodes) or 5-tuples (edges); also accepts\n                4-tuples for edges (deprecated).\n\n        Returns:\n        -------\n            None\n\n        \"\"\"\n        return self._add_nodes(entities)\n\n    def merge_nodes(self, nodes) -&gt; bool:\n        \"\"\"Merge nodes into database.\n\n        Either takes an iterable of tuples (if given, translates to\n        ``BioCypherNode`` objects) or an iterable of ``BioCypherNode`` objects.\n\n        Args:\n        ----\n            nodes (iterable): An iterable of nodes to merge into the database.\n\n        Returns:\n        -------\n            bool: True if successful.\n\n        \"\"\"\n        return self._add_nodes(nodes)\n\n    def merge_edges(self, edges) -&gt; bool:\n        \"\"\"Merge edges into database.\n\n        Either takes an iterable of tuples (if given, translates to\n        ``BioCypherEdge`` objects) or an iterable of ``BioCypherEdge`` objects.\n\n        Args:\n        ----\n            edges (iterable): An iterable of edges to merge into the database.\n\n        Returns:\n        -------\n            bool: True if successful.\n\n        \"\"\"\n        return self._add_edges(edges)\n\n    def get_kg(self):\n        \"\"\"Get the in-memory KG instance.\n\n        Depending on the specified `dbms` this could either be a list of Pandas\n        dataframes or a NetworkX DiGraph.\n        \"\"\"\n        if not self._is_online_and_in_memory():\n            msg = (f\"Getting the in-memory KG is only available in online mode for {IN_MEMORY_DBMS}.\",)\n            raise ValueError(msg)\n        if not self._in_memory_kg:\n            msg = \"No in-memory KG instance found. Please call `add()` first.\"\n            raise ValueError(msg)\n\n        if not self._in_memory_kg:\n            self._initialize_in_memory_kg()\n        return self._in_memory_kg.get_kg()\n\n    # DOWNLOAD AND CACHE MANAGEMENT METHODS ###\n\n    def _get_downloader(self, cache_dir: str | None = None):\n        \"\"\"Create downloader if not exists.\"\"\"\n        if not self._downloader:\n            self._downloader = Downloader(self._cache_directory)\n\n    def download(self, *resources) -&gt; None:\n        \"\"\"Download or load from cache the resources given by the adapter.\n\n        Args:\n        ----\n            resources (iterable): An iterable of resources to download or load\n                from cache.\n\n        Returns:\n        -------\n            None\n\n        \"\"\"\n        self._get_downloader()\n        return self._downloader.download(*resources)\n\n    # OVERVIEW AND CONVENIENCE METHODS ###\n\n    def log_missing_input_labels(self) -&gt; dict[str, list[str]] | None:\n        \"\"\"Log missing input labels.\n\n        Get the set of input labels encountered without an entry in the\n        `schema_config.yaml` and print them to the logger.\n\n        Returns\n        -------\n            Optional[Dict[str, List[str]]]: A dictionary of Biolink types\n            encountered without an entry in the `schema_config.yaml` file.\n\n        \"\"\"\n        mt = self._translator.get_missing_biolink_types()\n\n        if mt:\n            msg = (\n                \"Input entities not accounted for due to them not being \"\n                f\"present in the schema configuration file {self._schema_config_path} \"\n                \"(this is not necessarily a problem, if you did not intend \"\n                \"to include them in the database; see the log for details): \\n\"\n            )\n            for k, v in mt.items():\n                msg += f\"    {k}: {v} \\n\"\n\n            logger.info(msg)\n            return mt\n\n        else:\n            logger.info(\"No missing labels in input.\")\n            return None\n\n    def log_duplicates(self) -&gt; None:\n        \"\"\"Log duplicate nodes and edges.\n\n        Get the set of duplicate nodes and edges encountered and print them to\n        the logger.\n        \"\"\"\n        dn = self._deduplicator.get_duplicate_nodes()\n\n        if dn:\n            ntypes = dn[0]\n            nids = dn[1]\n\n            msg = \"Duplicate node types encountered (IDs in log): \\n\"\n            for typ in ntypes:\n                msg += f\"    {typ}\\n\"\n\n            logger.info(msg)\n\n            idmsg = \"Duplicate node IDs encountered: \\n\"\n            for _id in nids:\n                idmsg += f\"    {_id}\\n\"\n\n            logger.debug(idmsg)\n\n        else:\n            logger.info(\"No duplicate nodes in input.\")\n\n        de = self._deduplicator.get_duplicate_edges()\n\n        if de:\n            etypes = de[0]\n            eids = de[1]\n\n            msg = \"Duplicate edge types encountered (IDs in log): \\n\"\n            for typ in etypes:\n                msg += f\"    {typ}\\n\"\n\n            logger.info(msg)\n\n            idmsg = \"Duplicate edge IDs encountered: \\n\"\n            for _id in eids:\n                idmsg += f\"    {_id}\\n\"\n\n            logger.debug(idmsg)\n\n        else:\n            logger.info(\"No duplicate edges in input.\")\n\n    def show_ontology_structure(self, **kwargs) -&gt; None:\n        \"\"\"Show the ontology structure using treelib or write to GRAPHML file.\n\n        Args:\n        ----\n            to_disk (str): If specified, the ontology structure will be saved\n                to disk as a GRAPHML file, to be opened in your favourite\n                graph visualisation tool.\n\n            full (bool): If True, the full ontology structure will be shown,\n                including all nodes and edges. If False, only the nodes and\n                edges that are relevant to the extended schema will be shown.\n\n        \"\"\"\n        if not self._ontology:\n            self._get_ontology()\n\n        return self._ontology.show_ontology_structure(**kwargs)\n\n    def write_import_call(self) -&gt; str:\n        \"\"\"Write a shell script to import the database.\n\n        Shell script is written depending on the chosen DBMS.\n\n        Returns\n        -------\n            str: path toward the file holding the import call.\n\n        \"\"\"\n        if not self._offline:\n            msg = \"Cannot write import call in online mode.\"\n            raise NotImplementedError(msg)\n\n        return self._writer.write_import_call()\n\n    def write_schema_info(self, as_node: bool = False) -&gt; None:\n        \"\"\"Write an extended schema info to file or node.\n\n        Creates a YAML file or KG node that extends the `schema_config.yaml`\n        with run-time information of the built KG. For instance, include\n        information on whether something present in the actual knowledge graph,\n        whether it is a relationship (which is important in the case of\n        representing relationships as nodes) and the actual sources and\n        targets of edges. Since this file can be used in place of the original\n        `schema_config.yaml` file, it indicates that it is the extended schema\n        by setting `is_schema_info` to `true`.\n\n        We start by using the `extended_schema` dictionary from the ontology\n        class instance, which contains all expanded entities and relationships.\n        The information of whether something is a relationship can be gathered\n        from the deduplicator instance, which keeps track of all entities that\n        have been seen.\n\n        Args:\n        ----\n            as_node (bool): If True, the schema info is written as a KG node.\n                If False, the schema info is written to a YAML file.\n\n        \"\"\"\n        if (not self._offline) and self._dbms not in IN_MEMORY_DBMS:\n            msg = \"Cannot write schema info in online mode.\"\n            raise NotImplementedError(msg)\n\n        ontology = self._get_ontology()\n        schema = ontology.mapping.extended_schema.copy()\n        schema[\"is_schema_info\"] = True\n\n        deduplicator = self._get_deduplicator()\n        for node in deduplicator.entity_types:\n            if node in schema:\n                schema[node][\"present_in_knowledge_graph\"] = True\n                schema[node][\"is_relationship\"] = False\n            else:\n                logger.info(\n                    f\"Node {node} not present in extended schema. Skipping schema info.\",\n                )\n\n        # find 'label_as_edge' cases in schema entries\n        changed_labels = {}\n        for k, v in schema.items():\n            if not isinstance(v, dict):\n                continue\n            if \"label_as_edge\" in v:\n                if v[\"label_as_edge\"] in deduplicator.seen_relationships:\n                    changed_labels[v[\"label_as_edge\"]] = k\n\n        for edge in deduplicator.seen_relationships:\n            if edge in changed_labels:\n                edge = changed_labels[edge]\n            if edge in schema:\n                schema[edge][\"present_in_knowledge_graph\"] = True\n                schema[edge][\"is_relationship\"] = True\n                # TODO information about source and target nodes\n            else:\n                logger.info(\n                    f\"Edge {edge} not present in extended schema. Skipping schema info.\",\n                )\n\n        # write to output directory as YAML file\n        path = os.path.join(self._output_directory, \"schema_info.yaml\")\n        with open(path, \"w\") as f:\n            f.write(yaml.dump(schema))\n\n        if as_node:\n            # write as node\n            node = BioCypherNode(\n                node_id=\"schema_info\",\n                node_label=\"schema_info\",\n                properties={\"schema_info\": json.dumps(schema)},\n            )\n            self.write_nodes([node], force=True)\n\n            # override import call with added schema info node\n            self.write_import_call()\n\n        return schema\n\n    # TRANSLATION METHODS ###\n\n    def translate_term(self, term: str) -&gt; str:\n        \"\"\"Translate a term to its BioCypher equivalent.\n\n        Args:\n        ----\n            term (str): The term to translate.\n\n        Returns:\n        -------\n            str: The BioCypher equivalent of the term.\n\n        \"\"\"\n        # instantiate adapter if not exists\n        self.start_ontology()\n\n        return self._translator.translate_term(term)\n\n    def summary(self) -&gt; None:\n        \"\"\"Call convenience and reporting methods.\n\n        Shows ontology structure and logs duplicates and missing input types.\n        \"\"\"\n        self.show_ontology_structure()\n        self.log_duplicates()\n        self.log_missing_input_labels()\n\n    def reverse_translate_term(self, term: str) -&gt; str:\n        \"\"\"Reverse translate a term from its BioCypher equivalent.\n\n        Args:\n        ----\n            term (str): The BioCypher term to reverse translate.\n\n        Returns:\n        -------\n            str: The original term.\n\n        \"\"\"\n        # instantiate adapter if not exists\n        self.start_ontology()\n\n        return self._translator.reverse_translate_term(term)\n\n    def translate_query(self, query: str) -&gt; str:\n        \"\"\"Translate a query to its BioCypher equivalent.\n\n        Args:\n        ----\n            query (str): The query to translate.\n\n        Returns:\n        -------\n            str: The BioCypher equivalent of the query.\n\n        \"\"\"\n        # instantiate adapter if not exists\n        self.start_ontology()\n\n        return self._translator.translate(query)\n\n    def reverse_translate_query(self, query: str) -&gt; str:\n        \"\"\"Reverse translate a query from its BioCypher equivalent.\n\n        Args:\n        ----\n            query (str): The BioCypher query to reverse translate.\n\n        Returns:\n        -------\n            str: The original query.\n\n        \"\"\"\n        # instantiate adapter if not exists\n        self.start_ontology()\n\n        return self._translator.reverse_translate(query)\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher._add_edges","title":"<code>_add_edges(edges, batch_size=int(1000000.0))</code>","text":"<p>Add edges to the BioCypher KG.</p> <p>First uses the <code>_translator</code> to translate the edges to <code>BioCypherEdge</code> objects. Depending on the configuration the translated edges are then passed to the</p> <ul> <li> <p><code>_writer</code>: if <code>_offline</code> is set to <code>False</code></p> </li> <li> <p><code>_in_memory_kg</code>: if <code>_offline</code> is set to <code>False</code> and the <code>_dbms</code> is an     <code>IN_MEMORY_DBMS</code></p> </li> <li> <p><code>_driver</code>: if <code>_offline</code> is set to <code>True</code> and the <code>_dbms</code> is not an     <code>IN_MEMORY_DBMS</code></p> </li> </ul> Source code in <code>biocypher/_core.py</code> <pre><code>def _add_edges(self, edges, batch_size: int = int(1e6)):\n    \"\"\"Add edges to the BioCypher KG.\n\n    First uses the `_translator` to translate the edges to `BioCypherEdge`\n    objects. Depending on the configuration the translated edges are then\n    passed to the\n\n    - `_writer`: if `_offline` is set to `False`\n\n    - `_in_memory_kg`: if `_offline` is set to `False` and the `_dbms` is an\n        `IN_MEMORY_DBMS`\n\n    - `_driver`: if `_offline` is set to `True` and the `_dbms` is not an\n        `IN_MEMORY_DBMS`\n\n    \"\"\"\n    if not self._translator:\n        self._get_translator()\n    translated_edges = self._translator.translate_entities(edges)\n\n    if self._offline:\n        if not self._writer:\n            self._initialize_writer()\n        passed = self._writer.write_edges(\n            translated_edges,\n            batch_size=batch_size,\n        )\n    elif self._is_online_and_in_memory():\n        if not self._in_memory_kg:\n            self._initialize_in_memory_kg()\n        passed = self._in_memory_kg.add_edges(translated_edges)\n    else:\n        if not self._driver:\n            self._initialize_driver()\n        passed = self._driver.add_biocypher_nodes(translated_edges)\n\n    return passed\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher._add_nodes","title":"<code>_add_nodes(nodes, batch_size=int(1000000.0), force=False)</code>","text":"<p>Add nodes to the BioCypher KG.</p> <p>First uses the <code>_translator</code> to translate the nodes to <code>BioCypherNode</code> objects. Depending on the configuration the translated nodes are then passed to the</p> <ul> <li> <p><code>_writer</code>: if <code>_offline</code> is set to <code>False</code></p> </li> <li> <p><code>_in_memory_kg</code>: if <code>_offline</code> is set to <code>False</code> and the <code>_dbms</code> is an     <code>IN_MEMORY_DBMS</code></p> </li> <li> <p><code>_driver</code>: if <code>_offline</code> is set to <code>True</code> and the <code>_dbms</code> is not an     <code>IN_MEMORY_DBMS</code></p> </li> </ul> Source code in <code>biocypher/_core.py</code> <pre><code>def _add_nodes(\n    self,\n    nodes,\n    batch_size: int = int(1e6),\n    force: bool = False,\n):\n    \"\"\"Add nodes to the BioCypher KG.\n\n    First uses the `_translator` to translate the nodes to `BioCypherNode`\n    objects. Depending on the configuration the translated nodes are then\n    passed to the\n\n    - `_writer`: if `_offline` is set to `False`\n\n    - `_in_memory_kg`: if `_offline` is set to `False` and the `_dbms` is an\n        `IN_MEMORY_DBMS`\n\n    - `_driver`: if `_offline` is set to `True` and the `_dbms` is not an\n        `IN_MEMORY_DBMS`\n\n    \"\"\"\n    if not self._translator:\n        self._get_translator()\n    translated_nodes = self._translator.translate_entities(nodes)\n\n    if self._offline:\n        passed = self._get_writer().write_nodes(\n            translated_nodes,\n            batch_size=batch_size,\n            force=force,\n        )\n    elif self._is_online_and_in_memory():\n        passed = self._get_in_memory_kg().add_nodes(translated_nodes)\n    else:\n        passed = self._get_driver().add_biocypher_nodes(translated_nodes)\n\n    return passed\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher._get_deduplicator","title":"<code>_get_deduplicator()</code>","text":"<p>Create deduplicator if not exists and return.</p> Source code in <code>biocypher/_core.py</code> <pre><code>def _get_deduplicator(self) -&gt; Deduplicator:\n    \"\"\"Create deduplicator if not exists and return.\"\"\"\n    if not self._deduplicator:\n        self._deduplicator = Deduplicator()\n\n    return self._deduplicator\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher._get_downloader","title":"<code>_get_downloader(cache_dir=None)</code>","text":"<p>Create downloader if not exists.</p> Source code in <code>biocypher/_core.py</code> <pre><code>def _get_downloader(self, cache_dir: str | None = None):\n    \"\"\"Create downloader if not exists.\"\"\"\n    if not self._downloader:\n        self._downloader = Downloader(self._cache_directory)\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher._get_driver","title":"<code>_get_driver()</code>","text":"<p>Create driver if not exists.</p> <p>Set as instance variable <code>self._driver</code>.</p> Source code in <code>biocypher/_core.py</code> <pre><code>def _get_driver(self):\n    \"\"\"Create driver if not exists.\n\n    Set as instance variable `self._driver`.\n    \"\"\"\n    if not self._offline:\n        self._driver = get_connector(\n            dbms=self._dbms,\n            translator=self._get_translator(),\n        )\n    else:\n        msg = \"Cannot get driver in offline mode.\"\n        raise NotImplementedError(msg)\n\n    return self._driver\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher._get_in_memory_kg","title":"<code>_get_in_memory_kg()</code>","text":"<p>Create in-memory KG instance.</p> <p>Set as instance variable <code>self._in_memory_kg</code>.</p> Source code in <code>biocypher/_core.py</code> <pre><code>def _get_in_memory_kg(self):\n    \"\"\"Create in-memory KG instance.\n\n    Set as instance variable `self._in_memory_kg`.\n    \"\"\"\n    if not self._in_memory_kg:\n        self._in_memory_kg = get_in_memory_kg(\n            dbms=self._dbms,\n            deduplicator=self._get_deduplicator(),\n        )\n\n    return self._in_memory_kg\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher._get_ontology","title":"<code>_get_ontology()</code>","text":"<p>Create ontology if not exists and return.</p> Source code in <code>biocypher/_core.py</code> <pre><code>def _get_ontology(self) -&gt; Ontology:\n    \"\"\"Create ontology if not exists and return.\"\"\"\n    if not self._ontology:\n        self._ontology = Ontology(\n            ontology_mapping=self._get_ontology_mapping(),\n            head_ontology=self._head_ontology,\n            tail_ontologies=self._tail_ontologies,\n        )\n\n    return self._ontology\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher._get_ontology_mapping","title":"<code>_get_ontology_mapping()</code>","text":"<p>Create ontology mapping if not exists and return.</p> Source code in <code>biocypher/_core.py</code> <pre><code>def _get_ontology_mapping(self) -&gt; OntologyMapping:\n    \"\"\"Create ontology mapping if not exists and return.\"\"\"\n    if not self._schema_config_path:\n        self._ontology_mapping = OntologyMapping()\n\n    if not self._ontology_mapping:\n        self._ontology_mapping = OntologyMapping(\n            config_file=self._schema_config_path,\n        )\n\n    return self._ontology_mapping\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher._get_translator","title":"<code>_get_translator()</code>","text":"<p>Create translator if not exists and return.</p> Source code in <code>biocypher/_core.py</code> <pre><code>def _get_translator(self) -&gt; Translator:\n    \"\"\"Create translator if not exists and return.\"\"\"\n    if not self._translator:\n        self._translator = Translator(\n            ontology=self._get_ontology(),\n            strict_mode=self._strict_mode,\n        )\n\n    return self._translator\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher._get_writer","title":"<code>_get_writer()</code>","text":"<p>Create writer if not online.</p> <p>Set as instance variable <code>self._writer</code>.</p> Source code in <code>biocypher/_core.py</code> <pre><code>def _get_writer(self):\n    \"\"\"Create writer if not online.\n\n    Set as instance variable `self._writer`.\n    \"\"\"\n    if self._offline:\n\n        def timestamp() -&gt; str:\n            return datetime.now().strftime(\"%Y%m%d%H%M%S\")\n\n        outdir = self._output_directory or os.path.join(\n            \"biocypher-out\",\n            timestamp(),\n        )\n        self._output_directory = os.path.abspath(outdir)\n\n        self._writer = get_writer(\n            dbms=self._dbms,\n            translator=self._get_translator(),\n            deduplicator=self._get_deduplicator(),\n            output_directory=self._output_directory,\n            strict_mode=self._strict_mode,\n        )\n    else:\n        msg = \"Cannot get writer in online mode.\"\n        raise NotImplementedError(msg)\n\n    return self._writer\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher._initialize_in_memory_kg","title":"<code>_initialize_in_memory_kg()</code>","text":"<p>Create in-memory KG instance.</p> <p>Set as instance variable <code>self._in_memory_kg</code>.</p> Source code in <code>biocypher/_core.py</code> <pre><code>def _initialize_in_memory_kg(self) -&gt; None:\n    \"\"\"Create in-memory KG instance.\n\n    Set as instance variable `self._in_memory_kg`.\n    \"\"\"\n    if not self._in_memory_kg:\n        self._in_memory_kg = get_in_memory_kg(\n            dbms=self._dbms,\n            deduplicator=self._get_deduplicator(),\n        )\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher._is_online_and_in_memory","title":"<code>_is_online_and_in_memory()</code>","text":"<p>Return True if in online mode and in-memory dbms is used.</p> Source code in <code>biocypher/_core.py</code> <pre><code>def _is_online_and_in_memory(self) -&gt; bool:\n    \"\"\"Return True if in online mode and in-memory dbms is used.\"\"\"\n    return (not self._offline) &amp; (self._dbms in IN_MEMORY_DBMS)\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher._to_KG","title":"<code>_to_KG()</code>","text":"<p>Convert the internal representation to knowledge graph.</p> <p>The knowledge graph is returned based on the <code>dbms</code> parameter in the biocypher configuration file.</p>"},{"location":"reference/source/#biocypher._core.BioCypher._to_KG--returns","title":"Returns","text":"<pre><code> Any: knowledge graph.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def _to_KG(self):\n    \"\"\"Convert the internal representation to knowledge graph.\n\n    The knowledge graph is returned based on the `dbms` parameter in\n    the biocypher configuration file.\n\n    Returns\n    -------\n         Any: knowledge graph.\n\n    \"\"\"\n    if not self._in_memory_kg:\n        self._initialize_in_memory_kg()\n    if not self._translator:\n        self._get_translator()\n    tnodes = self._translator.translate_entities(self._nodes)\n    tedges = self._translator.translate_entities(self._edges)\n    self._in_memory_kg.add_nodes(tnodes)\n    self._in_memory_kg.add_edges(tedges)\n    return self._in_memory_kg.get_kg()\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher.add","title":"<code>add(entities)</code>","text":"<p>Add entities to the in-memory database.</p> <p>Accepts an iterable of tuples (if given, translates to <code>BioCypherNode</code> or <code>BioCypherEdge</code> objects) or an iterable of <code>BioCypherNode</code> or <code>BioCypherEdge</code> objects.</p> <pre><code>entities (iterable): An iterable of entities to add to the database.\n    Can be 3-tuples (nodes) or 5-tuples (edges); also accepts\n    4-tuples for edges (deprecated).\n</code></pre> <pre><code>None\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def add(self, entities) -&gt; None:\n    \"\"\"Add entities to the in-memory database.\n\n    Accepts an iterable of tuples (if given, translates to\n    ``BioCypherNode`` or ``BioCypherEdge`` objects) or an iterable of\n    ``BioCypherNode`` or ``BioCypherEdge`` objects.\n\n    Args:\n    ----\n        entities (iterable): An iterable of entities to add to the database.\n            Can be 3-tuples (nodes) or 5-tuples (edges); also accepts\n            4-tuples for edges (deprecated).\n\n    Returns:\n    -------\n        None\n\n    \"\"\"\n    return self._add_nodes(entities)\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher.add_edges","title":"<code>add_edges(edges)</code>","text":"<p>Add new edges to the internal representation.</p> <p>Initially, receive edges data from adaptor and create internal representation for edges.</p> <pre><code> edges(iterable): An iterable of edges.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def add_edges(self, edges) -&gt; None:\n    \"\"\"Add new edges to the internal representation.\n\n    Initially, receive edges data from adaptor and create internal\n    representation for edges.\n\n    Args:\n    ----\n         edges(iterable): An iterable of edges.\n\n    \"\"\"\n    if isinstance(edges, list):\n        self._edges = list(itertools.chain(self._edges, edges))\n    else:\n        self._edges = itertools.chain(self._edges, edges)\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher.add_nodes","title":"<code>add_nodes(nodes)</code>","text":"<p>Add new nodes to the internal representation.</p> <p>Initially, receive nodes data from adaptor and create internal representation for nodes.</p> <pre><code>nodes(iterable): An iterable of nodes\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def add_nodes(self, nodes) -&gt; None:\n    \"\"\"Add new nodes to the internal representation.\n\n    Initially, receive nodes data from adaptor and create internal\n    representation for nodes.\n\n    Args:\n    ----\n        nodes(iterable): An iterable of nodes\n\n    \"\"\"\n    if isinstance(nodes, list):\n        self._nodes = list(itertools.chain(self._nodes, nodes))\n    else:\n        self._nodes = itertools.chain(self._nodes, nodes)\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher.download","title":"<code>download(*resources)</code>","text":"<p>Download or load from cache the resources given by the adapter.</p> <pre><code>resources (iterable): An iterable of resources to download or load\n    from cache.\n</code></pre> <pre><code>None\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def download(self, *resources) -&gt; None:\n    \"\"\"Download or load from cache the resources given by the adapter.\n\n    Args:\n    ----\n        resources (iterable): An iterable of resources to download or load\n            from cache.\n\n    Returns:\n    -------\n        None\n\n    \"\"\"\n    self._get_downloader()\n    return self._downloader.download(*resources)\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher.get_kg","title":"<code>get_kg()</code>","text":"<p>Get the in-memory KG instance.</p> <p>Depending on the specified <code>dbms</code> this could either be a list of Pandas dataframes or a NetworkX DiGraph.</p> Source code in <code>biocypher/_core.py</code> <pre><code>def get_kg(self):\n    \"\"\"Get the in-memory KG instance.\n\n    Depending on the specified `dbms` this could either be a list of Pandas\n    dataframes or a NetworkX DiGraph.\n    \"\"\"\n    if not self._is_online_and_in_memory():\n        msg = (f\"Getting the in-memory KG is only available in online mode for {IN_MEMORY_DBMS}.\",)\n        raise ValueError(msg)\n    if not self._in_memory_kg:\n        msg = \"No in-memory KG instance found. Please call `add()` first.\"\n        raise ValueError(msg)\n\n    if not self._in_memory_kg:\n        self._initialize_in_memory_kg()\n    return self._in_memory_kg.get_kg()\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher.log_duplicates","title":"<code>log_duplicates()</code>","text":"<p>Log duplicate nodes and edges.</p> <p>Get the set of duplicate nodes and edges encountered and print them to the logger.</p> Source code in <code>biocypher/_core.py</code> <pre><code>def log_duplicates(self) -&gt; None:\n    \"\"\"Log duplicate nodes and edges.\n\n    Get the set of duplicate nodes and edges encountered and print them to\n    the logger.\n    \"\"\"\n    dn = self._deduplicator.get_duplicate_nodes()\n\n    if dn:\n        ntypes = dn[0]\n        nids = dn[1]\n\n        msg = \"Duplicate node types encountered (IDs in log): \\n\"\n        for typ in ntypes:\n            msg += f\"    {typ}\\n\"\n\n        logger.info(msg)\n\n        idmsg = \"Duplicate node IDs encountered: \\n\"\n        for _id in nids:\n            idmsg += f\"    {_id}\\n\"\n\n        logger.debug(idmsg)\n\n    else:\n        logger.info(\"No duplicate nodes in input.\")\n\n    de = self._deduplicator.get_duplicate_edges()\n\n    if de:\n        etypes = de[0]\n        eids = de[1]\n\n        msg = \"Duplicate edge types encountered (IDs in log): \\n\"\n        for typ in etypes:\n            msg += f\"    {typ}\\n\"\n\n        logger.info(msg)\n\n        idmsg = \"Duplicate edge IDs encountered: \\n\"\n        for _id in eids:\n            idmsg += f\"    {_id}\\n\"\n\n        logger.debug(idmsg)\n\n    else:\n        logger.info(\"No duplicate edges in input.\")\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher.log_missing_input_labels","title":"<code>log_missing_input_labels()</code>","text":"<p>Log missing input labels.</p> <p>Get the set of input labels encountered without an entry in the <code>schema_config.yaml</code> and print them to the logger.</p>"},{"location":"reference/source/#biocypher._core.BioCypher.log_missing_input_labels--returns","title":"Returns","text":"<pre><code>Optional[Dict[str, List[str]]]: A dictionary of Biolink types\nencountered without an entry in the `schema_config.yaml` file.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def log_missing_input_labels(self) -&gt; dict[str, list[str]] | None:\n    \"\"\"Log missing input labels.\n\n    Get the set of input labels encountered without an entry in the\n    `schema_config.yaml` and print them to the logger.\n\n    Returns\n    -------\n        Optional[Dict[str, List[str]]]: A dictionary of Biolink types\n        encountered without an entry in the `schema_config.yaml` file.\n\n    \"\"\"\n    mt = self._translator.get_missing_biolink_types()\n\n    if mt:\n        msg = (\n            \"Input entities not accounted for due to them not being \"\n            f\"present in the schema configuration file {self._schema_config_path} \"\n            \"(this is not necessarily a problem, if you did not intend \"\n            \"to include them in the database; see the log for details): \\n\"\n        )\n        for k, v in mt.items():\n            msg += f\"    {k}: {v} \\n\"\n\n        logger.info(msg)\n        return mt\n\n    else:\n        logger.info(\"No missing labels in input.\")\n        return None\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher.merge_edges","title":"<code>merge_edges(edges)</code>","text":"<p>Merge edges into database.</p> <p>Either takes an iterable of tuples (if given, translates to <code>BioCypherEdge</code> objects) or an iterable of <code>BioCypherEdge</code> objects.</p> <pre><code>edges (iterable): An iterable of edges to merge into the database.\n</code></pre> <pre><code>bool: True if successful.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def merge_edges(self, edges) -&gt; bool:\n    \"\"\"Merge edges into database.\n\n    Either takes an iterable of tuples (if given, translates to\n    ``BioCypherEdge`` objects) or an iterable of ``BioCypherEdge`` objects.\n\n    Args:\n    ----\n        edges (iterable): An iterable of edges to merge into the database.\n\n    Returns:\n    -------\n        bool: True if successful.\n\n    \"\"\"\n    return self._add_edges(edges)\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher.merge_nodes","title":"<code>merge_nodes(nodes)</code>","text":"<p>Merge nodes into database.</p> <p>Either takes an iterable of tuples (if given, translates to <code>BioCypherNode</code> objects) or an iterable of <code>BioCypherNode</code> objects.</p> <pre><code>nodes (iterable): An iterable of nodes to merge into the database.\n</code></pre> <pre><code>bool: True if successful.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def merge_nodes(self, nodes) -&gt; bool:\n    \"\"\"Merge nodes into database.\n\n    Either takes an iterable of tuples (if given, translates to\n    ``BioCypherNode`` objects) or an iterable of ``BioCypherNode`` objects.\n\n    Args:\n    ----\n        nodes (iterable): An iterable of nodes to merge into the database.\n\n    Returns:\n    -------\n        bool: True if successful.\n\n    \"\"\"\n    return self._add_nodes(nodes)\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher.reverse_translate_query","title":"<code>reverse_translate_query(query)</code>","text":"<p>Reverse translate a query from its BioCypher equivalent.</p> <pre><code>query (str): The BioCypher query to reverse translate.\n</code></pre> <pre><code>str: The original query.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def reverse_translate_query(self, query: str) -&gt; str:\n    \"\"\"Reverse translate a query from its BioCypher equivalent.\n\n    Args:\n    ----\n        query (str): The BioCypher query to reverse translate.\n\n    Returns:\n    -------\n        str: The original query.\n\n    \"\"\"\n    # instantiate adapter if not exists\n    self.start_ontology()\n\n    return self._translator.reverse_translate(query)\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher.reverse_translate_term","title":"<code>reverse_translate_term(term)</code>","text":"<p>Reverse translate a term from its BioCypher equivalent.</p> <pre><code>term (str): The BioCypher term to reverse translate.\n</code></pre> <pre><code>str: The original term.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def reverse_translate_term(self, term: str) -&gt; str:\n    \"\"\"Reverse translate a term from its BioCypher equivalent.\n\n    Args:\n    ----\n        term (str): The BioCypher term to reverse translate.\n\n    Returns:\n    -------\n        str: The original term.\n\n    \"\"\"\n    # instantiate adapter if not exists\n    self.start_ontology()\n\n    return self._translator.reverse_translate_term(term)\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher.show_ontology_structure","title":"<code>show_ontology_structure(**kwargs)</code>","text":"<p>Show the ontology structure using treelib or write to GRAPHML file.</p> <pre><code>to_disk (str): If specified, the ontology structure will be saved\n    to disk as a GRAPHML file, to be opened in your favourite\n    graph visualisation tool.\n\nfull (bool): If True, the full ontology structure will be shown,\n    including all nodes and edges. If False, only the nodes and\n    edges that are relevant to the extended schema will be shown.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def show_ontology_structure(self, **kwargs) -&gt; None:\n    \"\"\"Show the ontology structure using treelib or write to GRAPHML file.\n\n    Args:\n    ----\n        to_disk (str): If specified, the ontology structure will be saved\n            to disk as a GRAPHML file, to be opened in your favourite\n            graph visualisation tool.\n\n        full (bool): If True, the full ontology structure will be shown,\n            including all nodes and edges. If False, only the nodes and\n            edges that are relevant to the extended schema will be shown.\n\n    \"\"\"\n    if not self._ontology:\n        self._get_ontology()\n\n    return self._ontology.show_ontology_structure(**kwargs)\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher.summary","title":"<code>summary()</code>","text":"<p>Call convenience and reporting methods.</p> <p>Shows ontology structure and logs duplicates and missing input types.</p> Source code in <code>biocypher/_core.py</code> <pre><code>def summary(self) -&gt; None:\n    \"\"\"Call convenience and reporting methods.\n\n    Shows ontology structure and logs duplicates and missing input types.\n    \"\"\"\n    self.show_ontology_structure()\n    self.log_duplicates()\n    self.log_missing_input_labels()\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher.to_df","title":"<code>to_df()</code>","text":"<p>Create DataFrame using internal representation.</p> <p>TODO: to_df implies data frame, should be specifically that use case</p> Source code in <code>biocypher/_core.py</code> <pre><code>def to_df(self):\n    \"\"\"Create DataFrame using internal representation.\n\n    TODO: to_df implies data frame, should be specifically that use case\n    \"\"\"\n    return self._to_KG()\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher.to_networkx","title":"<code>to_networkx()</code>","text":"<p>Create networkx using internal representation.</p> Source code in <code>biocypher/_core.py</code> <pre><code>def to_networkx(self):\n    \"\"\"Create networkx using internal representation.\"\"\"\n    return self._to_KG()\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher.translate_query","title":"<code>translate_query(query)</code>","text":"<p>Translate a query to its BioCypher equivalent.</p> <pre><code>query (str): The query to translate.\n</code></pre> <pre><code>str: The BioCypher equivalent of the query.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def translate_query(self, query: str) -&gt; str:\n    \"\"\"Translate a query to its BioCypher equivalent.\n\n    Args:\n    ----\n        query (str): The query to translate.\n\n    Returns:\n    -------\n        str: The BioCypher equivalent of the query.\n\n    \"\"\"\n    # instantiate adapter if not exists\n    self.start_ontology()\n\n    return self._translator.translate(query)\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher.translate_term","title":"<code>translate_term(term)</code>","text":"<p>Translate a term to its BioCypher equivalent.</p> <pre><code>term (str): The term to translate.\n</code></pre> <pre><code>str: The BioCypher equivalent of the term.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def translate_term(self, term: str) -&gt; str:\n    \"\"\"Translate a term to its BioCypher equivalent.\n\n    Args:\n    ----\n        term (str): The term to translate.\n\n    Returns:\n    -------\n        str: The BioCypher equivalent of the term.\n\n    \"\"\"\n    # instantiate adapter if not exists\n    self.start_ontology()\n\n    return self._translator.translate_term(term)\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher.write_edges","title":"<code>write_edges(edges, batch_size=int(1000000.0))</code>","text":"<p>Write edges to database.</p> <p>Either takes an iterable of tuples (if given, translates to <code>BioCypherEdge</code> objects) or an iterable of <code>BioCypherEdge</code> objects.</p> <pre><code>edges (iterable): An iterable of edges to write to the database.\n</code></pre> <pre><code>bool: True if successful.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def write_edges(self, edges, batch_size: int = int(1e6)) -&gt; bool:\n    \"\"\"Write edges to database.\n\n    Either takes an iterable of tuples (if given, translates to\n    ``BioCypherEdge`` objects) or an iterable of ``BioCypherEdge`` objects.\n\n    Args:\n    ----\n        edges (iterable): An iterable of edges to write to the database.\n\n    Returns:\n    -------\n        bool: True if successful.\n\n    \"\"\"\n    return self._add_edges(edges, batch_size=batch_size)\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher.write_import_call","title":"<code>write_import_call()</code>","text":"<p>Write a shell script to import the database.</p> <p>Shell script is written depending on the chosen DBMS.</p>"},{"location":"reference/source/#biocypher._core.BioCypher.write_import_call--returns","title":"Returns","text":"<pre><code>str: path toward the file holding the import call.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def write_import_call(self) -&gt; str:\n    \"\"\"Write a shell script to import the database.\n\n    Shell script is written depending on the chosen DBMS.\n\n    Returns\n    -------\n        str: path toward the file holding the import call.\n\n    \"\"\"\n    if not self._offline:\n        msg = \"Cannot write import call in online mode.\"\n        raise NotImplementedError(msg)\n\n    return self._writer.write_import_call()\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher.write_nodes","title":"<code>write_nodes(nodes, batch_size=int(1000000.0), force=False)</code>","text":"<p>Write nodes to database.</p> <p>Either takes an iterable of tuples (if given, translates to <code>BioCypherNode</code> objects) or an iterable of <code>BioCypherNode</code> objects.</p> <pre><code>nodes (iterable): An iterable of nodes to write to the database.\nbatch_size (int): The batch size to use when writing to disk.\nforce (bool): Whether to force writing to the output directory even\n    if the node type is not present in the schema config file.\n</code></pre> <pre><code>bool: True if successful.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def write_nodes(\n    self,\n    nodes,\n    batch_size: int = int(1e6),\n    force: bool = False,\n) -&gt; bool:\n    \"\"\"Write nodes to database.\n\n    Either takes an iterable of tuples (if given, translates to\n    ``BioCypherNode`` objects) or an iterable of ``BioCypherNode`` objects.\n\n    Args:\n    ----\n        nodes (iterable): An iterable of nodes to write to the database.\n        batch_size (int): The batch size to use when writing to disk.\n        force (bool): Whether to force writing to the output directory even\n            if the node type is not present in the schema config file.\n\n    Returns:\n    -------\n        bool: True if successful.\n\n    \"\"\"\n    return self._add_nodes(nodes, batch_size=batch_size, force=force)\n</code></pre>"},{"location":"reference/source/#biocypher._core.BioCypher.write_schema_info","title":"<code>write_schema_info(as_node=False)</code>","text":"<p>Write an extended schema info to file or node.</p> <p>Creates a YAML file or KG node that extends the <code>schema_config.yaml</code> with run-time information of the built KG. For instance, include information on whether something present in the actual knowledge graph, whether it is a relationship (which is important in the case of representing relationships as nodes) and the actual sources and targets of edges. Since this file can be used in place of the original <code>schema_config.yaml</code> file, it indicates that it is the extended schema by setting <code>is_schema_info</code> to <code>true</code>.</p> <p>We start by using the <code>extended_schema</code> dictionary from the ontology class instance, which contains all expanded entities and relationships. The information of whether something is a relationship can be gathered from the deduplicator instance, which keeps track of all entities that have been seen.</p> <pre><code>as_node (bool): If True, the schema info is written as a KG node.\n    If False, the schema info is written to a YAML file.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def write_schema_info(self, as_node: bool = False) -&gt; None:\n    \"\"\"Write an extended schema info to file or node.\n\n    Creates a YAML file or KG node that extends the `schema_config.yaml`\n    with run-time information of the built KG. For instance, include\n    information on whether something present in the actual knowledge graph,\n    whether it is a relationship (which is important in the case of\n    representing relationships as nodes) and the actual sources and\n    targets of edges. Since this file can be used in place of the original\n    `schema_config.yaml` file, it indicates that it is the extended schema\n    by setting `is_schema_info` to `true`.\n\n    We start by using the `extended_schema` dictionary from the ontology\n    class instance, which contains all expanded entities and relationships.\n    The information of whether something is a relationship can be gathered\n    from the deduplicator instance, which keeps track of all entities that\n    have been seen.\n\n    Args:\n    ----\n        as_node (bool): If True, the schema info is written as a KG node.\n            If False, the schema info is written to a YAML file.\n\n    \"\"\"\n    if (not self._offline) and self._dbms not in IN_MEMORY_DBMS:\n        msg = \"Cannot write schema info in online mode.\"\n        raise NotImplementedError(msg)\n\n    ontology = self._get_ontology()\n    schema = ontology.mapping.extended_schema.copy()\n    schema[\"is_schema_info\"] = True\n\n    deduplicator = self._get_deduplicator()\n    for node in deduplicator.entity_types:\n        if node in schema:\n            schema[node][\"present_in_knowledge_graph\"] = True\n            schema[node][\"is_relationship\"] = False\n        else:\n            logger.info(\n                f\"Node {node} not present in extended schema. Skipping schema info.\",\n            )\n\n    # find 'label_as_edge' cases in schema entries\n    changed_labels = {}\n    for k, v in schema.items():\n        if not isinstance(v, dict):\n            continue\n        if \"label_as_edge\" in v:\n            if v[\"label_as_edge\"] in deduplicator.seen_relationships:\n                changed_labels[v[\"label_as_edge\"]] = k\n\n    for edge in deduplicator.seen_relationships:\n        if edge in changed_labels:\n            edge = changed_labels[edge]\n        if edge in schema:\n            schema[edge][\"present_in_knowledge_graph\"] = True\n            schema[edge][\"is_relationship\"] = True\n            # TODO information about source and target nodes\n        else:\n            logger.info(\n                f\"Edge {edge} not present in extended schema. Skipping schema info.\",\n            )\n\n    # write to output directory as YAML file\n    path = os.path.join(self._output_directory, \"schema_info.yaml\")\n    with open(path, \"w\") as f:\n        f.write(yaml.dump(schema))\n\n    if as_node:\n        # write as node\n        node = BioCypherNode(\n            node_id=\"schema_info\",\n            node_label=\"schema_info\",\n            properties={\"schema_info\": json.dumps(schema)},\n        )\n        self.write_nodes([node], force=True)\n\n        # override import call with added schema info node\n        self.write_import_call()\n\n    return schema\n</code></pre>"},{"location":"reference/source/#_createpy","title":"_create.py","text":"<p>BioCypher 'create' module. Handles the creation of BioCypher node and edge dataclasses.</p>"},{"location":"reference/source/#biocypher._create.BioCypherEdge","title":"<code>BioCypherEdge</code>  <code>dataclass</code>","text":"<p>Handoff class to represent biomedical relationships in Neo4j.</p> <p>Has source and target ids, label, property dict; ids and label (in the Neo4j sense of a label, ie, the entity descriptor after the colon, such as \":TARGETS\") are non-optional and called source_id, target_id, and relationship_label to avoid confusion with properties called \"label\", which usually denotes the human-readable form. Relationship labels are written in UPPERCASE and as verbs, as per Neo4j consensus.</p> <p>Args:</p> <pre><code>source_id (string): consensus \"best\" id for biological entity\n\ntarget_id (string): consensus \"best\" id for biological entity\n\nrelationship_label (string): type of interaction, UPPERCASE\n\nproperties (dict): collection of all other properties of the\nrespective edge\n</code></pre> Source code in <code>biocypher/_create.py</code> <pre><code>@dataclass(frozen=True)\nclass BioCypherEdge:\n    \"\"\"\n    Handoff class to represent biomedical relationships in Neo4j.\n\n    Has source and target ids, label, property dict; ids and label (in\n    the Neo4j sense of a label, ie, the entity descriptor after the\n    colon, such as \":TARGETS\") are non-optional and called source_id,\n    target_id, and relationship_label to avoid confusion with properties\n    called \"label\", which usually denotes the human-readable form.\n    Relationship labels are written in UPPERCASE and as verbs, as per\n    Neo4j consensus.\n\n    Args:\n\n        source_id (string): consensus \"best\" id for biological entity\n\n        target_id (string): consensus \"best\" id for biological entity\n\n        relationship_label (string): type of interaction, UPPERCASE\n\n        properties (dict): collection of all other properties of the\n        respective edge\n\n    \"\"\"\n\n    source_id: str\n    target_id: str\n    relationship_label: str\n    relationship_id: str = None\n    properties: dict = field(default_factory=dict)\n\n    def __post_init__(self):\n        \"\"\"\n        Check for reserved keywords.\n        \"\"\"\n\n        if \":TYPE\" in self.properties.keys():\n            logger.debug(\n                \"Keyword ':TYPE' is reserved for Neo4j. Removing from properties.\",\n                # \"Renaming to 'type'.\"\n            )\n            # self.properties[\"type\"] = self.properties[\":TYPE\"]\n            del self.properties[\":TYPE\"]\n        elif \"id\" in self.properties.keys():\n            logger.debug(\n                \"Keyword 'id' is reserved for Neo4j. Removing from properties.\",\n                # \"Renaming to 'type'.\"\n            )\n            # self.properties[\"type\"] = self.properties[\":TYPE\"]\n            del self.properties[\"id\"]\n        elif \"_ID\" in self.properties.keys():\n            logger.debug(\n                \"Keyword '_ID' is reserved for Postgres. Removing from properties.\",\n                # \"Renaming to 'type'.\"\n            )\n            # self.properties[\"type\"] = self.properties[\":TYPE\"]\n            del self.properties[\"_ID\"]\n\n    def get_id(self) -&gt; Union[str, None]:\n        \"\"\"\n        Returns primary node identifier or None.\n\n        Returns:\n            str: node_id\n        \"\"\"\n\n        return self.relationship_id\n\n    def get_source_id(self) -&gt; str:\n        \"\"\"\n        Returns primary node identifier of relationship source.\n\n        Returns:\n            str: source_id\n        \"\"\"\n        return self.source_id\n\n    def get_target_id(self) -&gt; str:\n        \"\"\"\n        Returns primary node identifier of relationship target.\n\n        Returns:\n            str: target_id\n        \"\"\"\n        return self.target_id\n\n    def get_label(self) -&gt; str:\n        \"\"\"\n        Returns relationship label.\n\n        Returns:\n            str: relationship_label\n        \"\"\"\n        return self.relationship_label\n\n    def get_type(self) -&gt; str:\n        \"\"\"\n        Returns relationship label.\n\n        Returns:\n            str: relationship_label\n        \"\"\"\n        return self.relationship_label\n\n    def get_properties(self) -&gt; dict:\n        \"\"\"\n        Returns all other relationship properties apart from primary ids\n        and label as key-value pairs.\n\n        Returns:\n            dict: properties\n        \"\"\"\n        return self.properties\n\n    def get_dict(self) -&gt; dict:\n        \"\"\"\n        Return dict of ids, label, and properties.\n\n        Returns:\n            dict: source_id, target_id and relationship_label as\n                top-level key-value pairs, properties as second-level\n                dict.\n        \"\"\"\n        return {\n            \"relationship_id\": self.relationship_id or None,\n            \"source_id\": self.source_id,\n            \"target_id\": self.target_id,\n            \"relationship_label\": self.relationship_label,\n            \"properties\": self.properties,\n        }\n</code></pre>"},{"location":"reference/source/#biocypher._create.BioCypherEdge.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Check for reserved keywords.</p> Source code in <code>biocypher/_create.py</code> <pre><code>def __post_init__(self):\n    \"\"\"\n    Check for reserved keywords.\n    \"\"\"\n\n    if \":TYPE\" in self.properties.keys():\n        logger.debug(\n            \"Keyword ':TYPE' is reserved for Neo4j. Removing from properties.\",\n            # \"Renaming to 'type'.\"\n        )\n        # self.properties[\"type\"] = self.properties[\":TYPE\"]\n        del self.properties[\":TYPE\"]\n    elif \"id\" in self.properties.keys():\n        logger.debug(\n            \"Keyword 'id' is reserved for Neo4j. Removing from properties.\",\n            # \"Renaming to 'type'.\"\n        )\n        # self.properties[\"type\"] = self.properties[\":TYPE\"]\n        del self.properties[\"id\"]\n    elif \"_ID\" in self.properties.keys():\n        logger.debug(\n            \"Keyword '_ID' is reserved for Postgres. Removing from properties.\",\n            # \"Renaming to 'type'.\"\n        )\n        # self.properties[\"type\"] = self.properties[\":TYPE\"]\n        del self.properties[\"_ID\"]\n</code></pre>"},{"location":"reference/source/#biocypher._create.BioCypherEdge.get_dict","title":"<code>get_dict()</code>","text":"<p>Return dict of ids, label, and properties.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>source_id, target_id and relationship_label as top-level key-value pairs, properties as second-level dict.</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_dict(self) -&gt; dict:\n    \"\"\"\n    Return dict of ids, label, and properties.\n\n    Returns:\n        dict: source_id, target_id and relationship_label as\n            top-level key-value pairs, properties as second-level\n            dict.\n    \"\"\"\n    return {\n        \"relationship_id\": self.relationship_id or None,\n        \"source_id\": self.source_id,\n        \"target_id\": self.target_id,\n        \"relationship_label\": self.relationship_label,\n        \"properties\": self.properties,\n    }\n</code></pre>"},{"location":"reference/source/#biocypher._create.BioCypherEdge.get_id","title":"<code>get_id()</code>","text":"<p>Returns primary node identifier or None.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>Union[str, None]</code> <p>node_id</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_id(self) -&gt; Union[str, None]:\n    \"\"\"\n    Returns primary node identifier or None.\n\n    Returns:\n        str: node_id\n    \"\"\"\n\n    return self.relationship_id\n</code></pre>"},{"location":"reference/source/#biocypher._create.BioCypherEdge.get_label","title":"<code>get_label()</code>","text":"<p>Returns relationship label.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>relationship_label</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_label(self) -&gt; str:\n    \"\"\"\n    Returns relationship label.\n\n    Returns:\n        str: relationship_label\n    \"\"\"\n    return self.relationship_label\n</code></pre>"},{"location":"reference/source/#biocypher._create.BioCypherEdge.get_properties","title":"<code>get_properties()</code>","text":"<p>Returns all other relationship properties apart from primary ids and label as key-value pairs.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>properties</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_properties(self) -&gt; dict:\n    \"\"\"\n    Returns all other relationship properties apart from primary ids\n    and label as key-value pairs.\n\n    Returns:\n        dict: properties\n    \"\"\"\n    return self.properties\n</code></pre>"},{"location":"reference/source/#biocypher._create.BioCypherEdge.get_source_id","title":"<code>get_source_id()</code>","text":"<p>Returns primary node identifier of relationship source.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>source_id</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_source_id(self) -&gt; str:\n    \"\"\"\n    Returns primary node identifier of relationship source.\n\n    Returns:\n        str: source_id\n    \"\"\"\n    return self.source_id\n</code></pre>"},{"location":"reference/source/#biocypher._create.BioCypherEdge.get_target_id","title":"<code>get_target_id()</code>","text":"<p>Returns primary node identifier of relationship target.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>target_id</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_target_id(self) -&gt; str:\n    \"\"\"\n    Returns primary node identifier of relationship target.\n\n    Returns:\n        str: target_id\n    \"\"\"\n    return self.target_id\n</code></pre>"},{"location":"reference/source/#biocypher._create.BioCypherEdge.get_type","title":"<code>get_type()</code>","text":"<p>Returns relationship label.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>relationship_label</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_type(self) -&gt; str:\n    \"\"\"\n    Returns relationship label.\n\n    Returns:\n        str: relationship_label\n    \"\"\"\n    return self.relationship_label\n</code></pre>"},{"location":"reference/source/#biocypher._create.BioCypherNode","title":"<code>BioCypherNode</code>  <code>dataclass</code>","text":"<p>Handoff class to represent biomedical entities as Neo4j nodes.</p> <p>Has id, label, property dict; id and label (in the Neo4j sense of a label, ie, the entity descriptor after the colon, such as \":Protein\") are non-optional and called node_id and node_label to avoid confusion with \"label\" properties. Node labels are written in PascalCase and as nouns, as per Neo4j consensus.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>string</code> <p>consensus \"best\" id for biological entity</p> required <code>node_label</code> <code>string</code> <p>primary type of entity, capitalised</p> required <code>**properties</code> <code>kwargs</code> <p>collection of all other properties to be passed to neo4j for the respective node (dict)</p> <code>dict()</code> Todo <ul> <li>check and correct small inconsistencies such as capitalisation     of ID names (\"uniprot\" vs \"UniProt\")</li> <li>check for correct ID patterns (eg \"ENSG\" + string of numbers,     uniprot length)</li> <li>ID conversion using pypath translation facilities for now</li> </ul> Source code in <code>biocypher/_create.py</code> <pre><code>@dataclass(frozen=True)\nclass BioCypherNode:\n    \"\"\"\n    Handoff class to represent biomedical entities as Neo4j nodes.\n\n    Has id, label, property dict; id and label (in the Neo4j sense of a\n    label, ie, the entity descriptor after the colon, such as\n    \":Protein\") are non-optional and called node_id and node_label to\n    avoid confusion with \"label\" properties. Node labels are written in\n    PascalCase and as nouns, as per Neo4j consensus.\n\n    Args:\n        node_id (string): consensus \"best\" id for biological entity\n        node_label (string): primary type of entity, capitalised\n        **properties (kwargs): collection of all other properties to be\n            passed to neo4j for the respective node (dict)\n\n    Todo:\n        - check and correct small inconsistencies such as capitalisation\n            of ID names (\"uniprot\" vs \"UniProt\")\n        - check for correct ID patterns (eg \"ENSG\" + string of numbers,\n            uniprot length)\n        - ID conversion using pypath translation facilities for now\n    \"\"\"\n\n    node_id: str\n    node_label: str\n    preferred_id: str = \"id\"\n    properties: dict = field(default_factory=dict)\n\n    def __post_init__(self):\n        \"\"\"\n        Add id field to properties.\n\n        Check for reserved keywords.\n\n        Replace unwanted characters in properties.\n        \"\"\"\n        self.properties[\"id\"] = self.node_id\n        self.properties[\"preferred_id\"] = self.preferred_id or None\n        # TODO actually make None possible here; as is, \"id\" is the default in\n        # the dataclass as well as in the configuration file\n\n        if \":TYPE\" in self.properties.keys():\n            logger.warning(\n                \"Keyword ':TYPE' is reserved for Neo4j. Removing from properties.\",\n                # \"Renaming to 'type'.\"\n            )\n            # self.properties[\"type\"] = self.properties[\":TYPE\"]\n            del self.properties[\":TYPE\"]\n\n        for k, v in self.properties.items():\n            if isinstance(v, str):\n                self.properties[k] = (\n                    v.replace(\n                        os.linesep,\n                        \" \",\n                    )\n                    .replace(\n                        \"\\n\",\n                        \" \",\n                    )\n                    .replace(\n                        \"\\r\",\n                        \" \",\n                    )\n                )\n\n            elif isinstance(v, list):\n                self.properties[k] = [\n                    val.replace(\n                        os.linesep,\n                        \" \",\n                    )\n                    .replace(\n                        \"\\n\",\n                        \" \",\n                    )\n                    .replace(\"\\r\", \" \")\n                    for val in v\n                ]\n\n    def get_id(self) -&gt; str:\n        \"\"\"\n        Returns primary node identifier.\n\n        Returns:\n            str: node_id\n        \"\"\"\n        return self.node_id\n\n    def get_label(self) -&gt; str:\n        \"\"\"\n        Returns primary node label.\n\n        Returns:\n            str: node_label\n        \"\"\"\n        return self.node_label\n\n    def get_type(self) -&gt; str:\n        \"\"\"\n        Returns primary node label.\n\n        Returns:\n            str: node_label\n        \"\"\"\n        return self.node_label\n\n    def get_preferred_id(self) -&gt; str:\n        \"\"\"\n        Returns preferred id.\n\n        Returns:\n            str: preferred_id\n        \"\"\"\n        return self.preferred_id\n\n    def get_properties(self) -&gt; dict:\n        \"\"\"\n        Returns all other node properties apart from primary id and\n        label as key-value pairs.\n\n        Returns:\n            dict: properties\n        \"\"\"\n        return self.properties\n\n    def get_dict(self) -&gt; dict:\n        \"\"\"\n        Return dict of id, labels, and properties.\n\n        Returns:\n            dict: node_id and node_label as top-level key-value pairs,\n            properties as second-level dict.\n        \"\"\"\n        return {\n            \"node_id\": self.node_id,\n            \"node_label\": self.node_label,\n            \"properties\": self.properties,\n        }\n</code></pre>"},{"location":"reference/source/#biocypher._create.BioCypherNode.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Add id field to properties.</p> <p>Check for reserved keywords.</p> <p>Replace unwanted characters in properties.</p> Source code in <code>biocypher/_create.py</code> <pre><code>def __post_init__(self):\n    \"\"\"\n    Add id field to properties.\n\n    Check for reserved keywords.\n\n    Replace unwanted characters in properties.\n    \"\"\"\n    self.properties[\"id\"] = self.node_id\n    self.properties[\"preferred_id\"] = self.preferred_id or None\n    # TODO actually make None possible here; as is, \"id\" is the default in\n    # the dataclass as well as in the configuration file\n\n    if \":TYPE\" in self.properties.keys():\n        logger.warning(\n            \"Keyword ':TYPE' is reserved for Neo4j. Removing from properties.\",\n            # \"Renaming to 'type'.\"\n        )\n        # self.properties[\"type\"] = self.properties[\":TYPE\"]\n        del self.properties[\":TYPE\"]\n\n    for k, v in self.properties.items():\n        if isinstance(v, str):\n            self.properties[k] = (\n                v.replace(\n                    os.linesep,\n                    \" \",\n                )\n                .replace(\n                    \"\\n\",\n                    \" \",\n                )\n                .replace(\n                    \"\\r\",\n                    \" \",\n                )\n            )\n\n        elif isinstance(v, list):\n            self.properties[k] = [\n                val.replace(\n                    os.linesep,\n                    \" \",\n                )\n                .replace(\n                    \"\\n\",\n                    \" \",\n                )\n                .replace(\"\\r\", \" \")\n                for val in v\n            ]\n</code></pre>"},{"location":"reference/source/#biocypher._create.BioCypherNode.get_dict","title":"<code>get_dict()</code>","text":"<p>Return dict of id, labels, and properties.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>node_id and node_label as top-level key-value pairs,</p> <code>dict</code> <p>properties as second-level dict.</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_dict(self) -&gt; dict:\n    \"\"\"\n    Return dict of id, labels, and properties.\n\n    Returns:\n        dict: node_id and node_label as top-level key-value pairs,\n        properties as second-level dict.\n    \"\"\"\n    return {\n        \"node_id\": self.node_id,\n        \"node_label\": self.node_label,\n        \"properties\": self.properties,\n    }\n</code></pre>"},{"location":"reference/source/#biocypher._create.BioCypherNode.get_id","title":"<code>get_id()</code>","text":"<p>Returns primary node identifier.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>node_id</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_id(self) -&gt; str:\n    \"\"\"\n    Returns primary node identifier.\n\n    Returns:\n        str: node_id\n    \"\"\"\n    return self.node_id\n</code></pre>"},{"location":"reference/source/#biocypher._create.BioCypherNode.get_label","title":"<code>get_label()</code>","text":"<p>Returns primary node label.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>node_label</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_label(self) -&gt; str:\n    \"\"\"\n    Returns primary node label.\n\n    Returns:\n        str: node_label\n    \"\"\"\n    return self.node_label\n</code></pre>"},{"location":"reference/source/#biocypher._create.BioCypherNode.get_preferred_id","title":"<code>get_preferred_id()</code>","text":"<p>Returns preferred id.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>preferred_id</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_preferred_id(self) -&gt; str:\n    \"\"\"\n    Returns preferred id.\n\n    Returns:\n        str: preferred_id\n    \"\"\"\n    return self.preferred_id\n</code></pre>"},{"location":"reference/source/#biocypher._create.BioCypherNode.get_properties","title":"<code>get_properties()</code>","text":"<p>Returns all other node properties apart from primary id and label as key-value pairs.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>properties</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_properties(self) -&gt; dict:\n    \"\"\"\n    Returns all other node properties apart from primary id and\n    label as key-value pairs.\n\n    Returns:\n        dict: properties\n    \"\"\"\n    return self.properties\n</code></pre>"},{"location":"reference/source/#biocypher._create.BioCypherNode.get_type","title":"<code>get_type()</code>","text":"<p>Returns primary node label.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>node_label</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_type(self) -&gt; str:\n    \"\"\"\n    Returns primary node label.\n\n    Returns:\n        str: node_label\n    \"\"\"\n    return self.node_label\n</code></pre>"},{"location":"reference/source/#biocypher._create.BioCypherRelAsNode","title":"<code>BioCypherRelAsNode</code>  <code>dataclass</code>","text":"<p>Class to represent relationships as nodes (with in- and outgoing edges) as a triplet of a BioCypherNode and two BioCypherEdges. Main usage in type checking (instances where the receiving function needs to check whether it receives a relationship as a single edge or as a triplet).</p> <p>Args:</p> <pre><code>node (BioCypherNode): node representing the relationship\n\nsource_edge (BioCypherEdge): edge representing the source of the\n    relationship\n\ntarget_edge (BioCypherEdge): edge representing the target of the\n    relationship\n</code></pre> Source code in <code>biocypher/_create.py</code> <pre><code>@dataclass(frozen=True)\nclass BioCypherRelAsNode:\n    \"\"\"\n    Class to represent relationships as nodes (with in- and outgoing\n    edges) as a triplet of a BioCypherNode and two BioCypherEdges. Main\n    usage in type checking (instances where the receiving function needs\n    to check whether it receives a relationship as a single edge or as\n    a triplet).\n\n    Args:\n\n        node (BioCypherNode): node representing the relationship\n\n        source_edge (BioCypherEdge): edge representing the source of the\n            relationship\n\n        target_edge (BioCypherEdge): edge representing the target of the\n            relationship\n\n    \"\"\"\n\n    node: BioCypherNode\n    source_edge: BioCypherEdge\n    target_edge: BioCypherEdge\n\n    def __post_init__(self):\n        if not isinstance(self.node, BioCypherNode):\n            raise TypeError(\n                f\"BioCypherRelAsNode.node must be a BioCypherNode, \" f\"not {type(self.node)}.\",\n            )\n\n        if not isinstance(self.source_edge, BioCypherEdge):\n            raise TypeError(\n                f\"BioCypherRelAsNode.source_edge must be a BioCypherEdge, \" f\"not {type(self.source_edge)}.\",\n            )\n\n        if not isinstance(self.target_edge, BioCypherEdge):\n            raise TypeError(\n                f\"BioCypherRelAsNode.target_edge must be a BioCypherEdge, \" f\"not {type(self.target_edge)}.\",\n            )\n\n    def get_node(self) -&gt; BioCypherNode:\n        return self.node\n\n    def get_source_edge(self) -&gt; BioCypherEdge:\n        return self.source_edge\n\n    def get_target_edge(self) -&gt; BioCypherEdge:\n        return self.target_edge\n</code></pre>"},{"location":"reference/source/#_deduplicatepy","title":"_deduplicate.py","text":""},{"location":"reference/source/#biocypher._deduplicate.Deduplicator","title":"<code>Deduplicator</code>","text":"<p>Singleton class responsible of deduplicating BioCypher inputs. Maintains sets/dictionaries of node and edge types and their unique identifiers.</p> <p>Nodes identifiers should be globally unique (represented as a set), while edge identifiers are only unique per edge type (represented as a dict of sets, keyed by edge type).</p> <p>Stores collection of duplicate node and edge identifiers and types for troubleshooting and to avoid overloading the log.</p> Source code in <code>biocypher/_deduplicate.py</code> <pre><code>class Deduplicator:\n    \"\"\"\n    Singleton class responsible of deduplicating BioCypher inputs. Maintains\n    sets/dictionaries of node and edge types and their unique identifiers.\n\n    Nodes identifiers should be globally unique (represented as a set), while\n    edge identifiers are only unique per edge type (represented as a dict of\n    sets, keyed by edge type).\n\n    Stores collection of duplicate node and edge identifiers and types for\n    troubleshooting and to avoid overloading the log.\n    \"\"\"\n\n    def __init__(self):\n        self.seen_entity_ids = set()\n        self.duplicate_entity_ids = set()\n\n        self.entity_types = set()\n        self.duplicate_entity_types = set()\n\n        self.seen_relationships = {}\n        self.duplicate_relationship_ids = set()\n        self.duplicate_relationship_types = set()\n\n    def node_seen(self, entity: BioCypherNode) -&gt; bool:\n        \"\"\"\n        Adds a node to the instance and checks if it has been seen before.\n\n        Args:\n            node: BioCypherNode to be added.\n\n        Returns:\n            True if the node has been seen before, False otherwise.\n        \"\"\"\n        if entity.get_label() not in self.entity_types:\n            self.entity_types.add(entity.get_label())\n\n        if entity.get_id() in self.seen_entity_ids:\n            self.duplicate_entity_ids.add(entity.get_id())\n            if entity.get_label() not in self.duplicate_entity_types:\n                logger.warning(f\"Duplicate node type {entity.get_label()} found. \")\n                self.duplicate_entity_types.add(entity.get_label())\n            return True\n\n        self.seen_entity_ids.add(entity.get_id())\n        return False\n\n    def edge_seen(self, relationship: BioCypherEdge) -&gt; bool:\n        \"\"\"\n        Adds an edge to the instance and checks if it has been seen before.\n\n        Args:\n            edge: BioCypherEdge to be added.\n\n        Returns:\n            True if the edge has been seen before, False otherwise.\n        \"\"\"\n        if relationship.get_type() not in self.seen_relationships:\n            self.seen_relationships[relationship.get_type()] = set()\n\n        # concatenate source and target if no id is present\n        if not relationship.get_id():\n            _id = f\"{relationship.get_source_id()}_{relationship.get_target_id()}\"\n        else:\n            _id = relationship.get_id()\n\n        if _id in self.seen_relationships[relationship.get_type()]:\n            self.duplicate_relationship_ids.add(_id)\n            if relationship.get_type() not in self.duplicate_relationship_types:\n                logger.warning(f\"Duplicate edge type {relationship.get_type()} found. \")\n                self.duplicate_relationship_types.add(relationship.get_type())\n            return True\n\n        self.seen_relationships[relationship.get_type()].add(_id)\n        return False\n\n    def rel_as_node_seen(self, rel_as_node: BioCypherRelAsNode) -&gt; bool:\n        \"\"\"\n        Adds a rel_as_node to the instance (one entity and two relationships)\n        and checks if it has been seen before. Only the node is relevant for\n        identifying the rel_as_node as a duplicate.\n\n        Args:\n            rel_as_node: BioCypherRelAsNode to be added.\n\n        Returns:\n            True if the rel_as_node has been seen before, False otherwise.\n        \"\"\"\n        node = rel_as_node.get_node()\n\n        if node.get_label() not in self.seen_relationships:\n            self.seen_relationships[node.get_label()] = set()\n\n        # rel as node always has an id\n        _id = node.get_id()\n\n        if _id in self.seen_relationships[node.get_type()]:\n            self.duplicate_relationship_ids.add(_id)\n            if node.get_type() not in self.duplicate_relationship_types:\n                logger.warning(f\"Duplicate edge type {node.get_type()} found. \")\n                self.duplicate_relationship_types.add(node.get_type())\n            return True\n\n        self.seen_relationships[node.get_type()].add(_id)\n        return False\n\n    def get_duplicate_nodes(self):\n        \"\"\"\n        Function to return a list of duplicate nodes.\n\n        Returns:\n            list: list of duplicate nodes\n        \"\"\"\n\n        if self.duplicate_entity_types:\n            return (self.duplicate_entity_types, self.duplicate_entity_ids)\n        else:\n            return None\n\n    def get_duplicate_edges(self):\n        \"\"\"\n        Function to return a list of duplicate edges.\n\n        Returns:\n            list: list of duplicate edges\n        \"\"\"\n\n        if self.duplicate_relationship_types:\n            return (\n                self.duplicate_relationship_types,\n                self.duplicate_relationship_ids,\n            )\n        else:\n            return None\n</code></pre>"},{"location":"reference/source/#biocypher._deduplicate.Deduplicator.edge_seen","title":"<code>edge_seen(relationship)</code>","text":"<p>Adds an edge to the instance and checks if it has been seen before.</p> <p>Parameters:</p> Name Type Description Default <code>edge</code> <p>BioCypherEdge to be added.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the edge has been seen before, False otherwise.</p> Source code in <code>biocypher/_deduplicate.py</code> <pre><code>def edge_seen(self, relationship: BioCypherEdge) -&gt; bool:\n    \"\"\"\n    Adds an edge to the instance and checks if it has been seen before.\n\n    Args:\n        edge: BioCypherEdge to be added.\n\n    Returns:\n        True if the edge has been seen before, False otherwise.\n    \"\"\"\n    if relationship.get_type() not in self.seen_relationships:\n        self.seen_relationships[relationship.get_type()] = set()\n\n    # concatenate source and target if no id is present\n    if not relationship.get_id():\n        _id = f\"{relationship.get_source_id()}_{relationship.get_target_id()}\"\n    else:\n        _id = relationship.get_id()\n\n    if _id in self.seen_relationships[relationship.get_type()]:\n        self.duplicate_relationship_ids.add(_id)\n        if relationship.get_type() not in self.duplicate_relationship_types:\n            logger.warning(f\"Duplicate edge type {relationship.get_type()} found. \")\n            self.duplicate_relationship_types.add(relationship.get_type())\n        return True\n\n    self.seen_relationships[relationship.get_type()].add(_id)\n    return False\n</code></pre>"},{"location":"reference/source/#biocypher._deduplicate.Deduplicator.get_duplicate_edges","title":"<code>get_duplicate_edges()</code>","text":"<p>Function to return a list of duplicate edges.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>list of duplicate edges</p> Source code in <code>biocypher/_deduplicate.py</code> <pre><code>def get_duplicate_edges(self):\n    \"\"\"\n    Function to return a list of duplicate edges.\n\n    Returns:\n        list: list of duplicate edges\n    \"\"\"\n\n    if self.duplicate_relationship_types:\n        return (\n            self.duplicate_relationship_types,\n            self.duplicate_relationship_ids,\n        )\n    else:\n        return None\n</code></pre>"},{"location":"reference/source/#biocypher._deduplicate.Deduplicator.get_duplicate_nodes","title":"<code>get_duplicate_nodes()</code>","text":"<p>Function to return a list of duplicate nodes.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>list of duplicate nodes</p> Source code in <code>biocypher/_deduplicate.py</code> <pre><code>def get_duplicate_nodes(self):\n    \"\"\"\n    Function to return a list of duplicate nodes.\n\n    Returns:\n        list: list of duplicate nodes\n    \"\"\"\n\n    if self.duplicate_entity_types:\n        return (self.duplicate_entity_types, self.duplicate_entity_ids)\n    else:\n        return None\n</code></pre>"},{"location":"reference/source/#biocypher._deduplicate.Deduplicator.node_seen","title":"<code>node_seen(entity)</code>","text":"<p>Adds a node to the instance and checks if it has been seen before.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <p>BioCypherNode to be added.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the node has been seen before, False otherwise.</p> Source code in <code>biocypher/_deduplicate.py</code> <pre><code>def node_seen(self, entity: BioCypherNode) -&gt; bool:\n    \"\"\"\n    Adds a node to the instance and checks if it has been seen before.\n\n    Args:\n        node: BioCypherNode to be added.\n\n    Returns:\n        True if the node has been seen before, False otherwise.\n    \"\"\"\n    if entity.get_label() not in self.entity_types:\n        self.entity_types.add(entity.get_label())\n\n    if entity.get_id() in self.seen_entity_ids:\n        self.duplicate_entity_ids.add(entity.get_id())\n        if entity.get_label() not in self.duplicate_entity_types:\n            logger.warning(f\"Duplicate node type {entity.get_label()} found. \")\n            self.duplicate_entity_types.add(entity.get_label())\n        return True\n\n    self.seen_entity_ids.add(entity.get_id())\n    return False\n</code></pre>"},{"location":"reference/source/#biocypher._deduplicate.Deduplicator.rel_as_node_seen","title":"<code>rel_as_node_seen(rel_as_node)</code>","text":"<p>Adds a rel_as_node to the instance (one entity and two relationships) and checks if it has been seen before. Only the node is relevant for identifying the rel_as_node as a duplicate.</p> <p>Parameters:</p> Name Type Description Default <code>rel_as_node</code> <code>BioCypherRelAsNode</code> <p>BioCypherRelAsNode to be added.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the rel_as_node has been seen before, False otherwise.</p> Source code in <code>biocypher/_deduplicate.py</code> <pre><code>def rel_as_node_seen(self, rel_as_node: BioCypherRelAsNode) -&gt; bool:\n    \"\"\"\n    Adds a rel_as_node to the instance (one entity and two relationships)\n    and checks if it has been seen before. Only the node is relevant for\n    identifying the rel_as_node as a duplicate.\n\n    Args:\n        rel_as_node: BioCypherRelAsNode to be added.\n\n    Returns:\n        True if the rel_as_node has been seen before, False otherwise.\n    \"\"\"\n    node = rel_as_node.get_node()\n\n    if node.get_label() not in self.seen_relationships:\n        self.seen_relationships[node.get_label()] = set()\n\n    # rel as node always has an id\n    _id = node.get_id()\n\n    if _id in self.seen_relationships[node.get_type()]:\n        self.duplicate_relationship_ids.add(_id)\n        if node.get_type() not in self.duplicate_relationship_types:\n            logger.warning(f\"Duplicate edge type {node.get_type()} found. \")\n            self.duplicate_relationship_types.add(node.get_type())\n        return True\n\n    self.seen_relationships[node.get_type()].add(_id)\n    return False\n</code></pre>"},{"location":"reference/source/#_getpy","title":"_get.py","text":"<p>BioCypher get module. Used to download and cache data from external sources.</p>"},{"location":"reference/source/#biocypher._get.APIRequest","title":"<code>APIRequest</code>","text":"<p>               Bases: <code>Resource</code></p> Source code in <code>biocypher/_get.py</code> <pre><code>class APIRequest(Resource):\n    def __init__(self, name: str, url_s: str | list[str], lifetime: int = 0):\n        \"\"\"\n        Represents basic information for an API Request.\n\n        Args:\n            name(str): The name of the API Request.\n\n            url_s(str|list): The URL of the API endpoint.\n\n            lifetime(int): The lifetime of the API Request in days. If 0, the\n                API Request is cached indefinitely.\n\n        \"\"\"\n        super().__init__(name, url_s, lifetime)\n</code></pre>"},{"location":"reference/source/#biocypher._get.APIRequest.__init__","title":"<code>__init__(name, url_s, lifetime=0)</code>","text":"<p>Represents basic information for an API Request.</p> <p>Parameters:</p> Name Type Description Default <code>name(str)</code> <p>The name of the API Request.</p> required <code>url_s(str|list)</code> <p>The URL of the API endpoint.</p> required <code>lifetime(int)</code> <p>The lifetime of the API Request in days. If 0, the API Request is cached indefinitely.</p> required Source code in <code>biocypher/_get.py</code> <pre><code>def __init__(self, name: str, url_s: str | list[str], lifetime: int = 0):\n    \"\"\"\n    Represents basic information for an API Request.\n\n    Args:\n        name(str): The name of the API Request.\n\n        url_s(str|list): The URL of the API endpoint.\n\n        lifetime(int): The lifetime of the API Request in days. If 0, the\n            API Request is cached indefinitely.\n\n    \"\"\"\n    super().__init__(name, url_s, lifetime)\n</code></pre>"},{"location":"reference/source/#biocypher._get.Downloader","title":"<code>Downloader</code>","text":"Source code in <code>biocypher/_get.py</code> <pre><code>class Downloader:\n    def __init__(self, cache_dir: Optional[str] = None) -&gt; None:\n        \"\"\"\n        The Downloader is a class that manages resources that can be downloaded\n        and cached locally. It manages the lifetime of downloaded resources by\n        keeping a JSON record of the download date of each resource.\n\n        Args:\n            cache_dir (str): The directory where the resources are cached. If\n                not given, a temporary directory is created.\n        \"\"\"\n        self.cache_dir = cache_dir or TemporaryDirectory().name\n        self.cache_file = os.path.join(self.cache_dir, \"cache.json\")\n        self.cache_dict = self._load_cache_dict()\n\n    def download(self, *resources: Resource):\n        \"\"\"\n        Download one or multiple resources. Load from cache if the resource is\n        already downloaded and the cache is not expired.\n\n        Args:\n            resources (Resource): The resource(s) to download or load from\n                cache.\n\n        Returns:\n            list[str]: The path or paths to the resource(s) that were downloaded\n                or loaded from cache.\n\n        \"\"\"\n        paths = []\n        for resource in resources:\n            paths.append(self._download_or_cache(resource))\n\n        # flatten list if it is nested\n        if is_nested(paths):\n            paths = [path for sublist in paths for path in sublist]\n\n        return paths\n\n    def _download_or_cache(self, resource: Resource, cache: bool = True):\n        \"\"\"\n        Download a resource if it is not cached or exceeded its lifetime.\n\n        Args:\n            resource (Resource): The resource to download.\n        Returns:\n            list[str]: The path or paths to the downloaded resource(s).\n\n        \"\"\"\n        expired = self._is_cache_expired(resource)\n\n        if expired or not cache:\n            self._delete_expired_cache(resource)\n            if isinstance(resource, FileDownload):\n                logger.info(f\"Asking for download of resource {resource.name}.\")\n                paths = self._download_files(cache, resource)\n            elif isinstance(resource, APIRequest):\n                logger.info(f\"Asking for download of api request {resource.name}.\")\n                paths = self._download_api_request(resource)\n            else:\n                raise TypeError(f\"Unknown resource type: {type(resource)}\")\n        else:\n            paths = self.get_cached_version(resource)\n        self._update_cache_record(resource)\n        return paths\n\n    def _is_cache_expired(self, resource: Resource) -&gt; bool:\n        \"\"\"\n        Check if resource or API request cache is expired.\n\n        Args:\n            resource (Resource): The resource to download.\n\n        Returns:\n            bool: cache is expired or not.\n        \"\"\"\n        cache_record = self._get_cache_record(resource)\n        if cache_record:\n            download_time = datetime.strptime(cache_record.get(\"date_downloaded\"), \"%Y-%m-%d %H:%M:%S.%f\")\n            lifetime = timedelta(days=resource.lifetime)\n            expired = download_time + lifetime &lt; datetime.now()\n        else:\n            expired = True\n        return expired\n\n    def _delete_expired_cache(self, resource: Resource):\n        cache_resource_path = self.cache_dir + \"/\" + resource.name\n        if os.path.exists(cache_resource_path) and os.path.isdir(cache_resource_path):\n            shutil.rmtree(cache_resource_path)\n\n    def _download_files(self, cache, file_download: FileDownload):\n        \"\"\"\n        Download a resource given it is a file or a directory and return the\n        path.\n\n        Args:\n            cache (bool): Whether to cache the resource or not.\n            file_download (FileDownload): The resource to download.\n\n        Returns:\n            list[str]: The path or paths to the downloaded resource(s).\n        \"\"\"\n        if file_download.is_dir:\n            files = self._get_files(file_download)\n            file_download.url_s = [file_download.url_s + \"/\" + file for file in files]\n            file_download.is_dir = False\n            paths = self._download_or_cache(file_download, cache)\n        elif isinstance(file_download.url_s, list):\n            paths = []\n            for url in file_download.url_s:\n                fname = url[url.rfind(\"/\") + 1 :].split(\"?\")[0]\n                path = self._retrieve(\n                    url=url,\n                    fname=fname,\n                    path=os.path.join(self.cache_dir, file_download.name),\n                )\n                paths.append(path)\n        else:\n            paths = []\n            fname = file_download.url_s[file_download.url_s.rfind(\"/\") + 1 :].split(\"?\")[0]\n            results = self._retrieve(\n                url=file_download.url_s,\n                fname=fname,\n                path=os.path.join(self.cache_dir, file_download.name),\n            )\n            if isinstance(results, list):\n                paths.extend(results)\n            else:\n                paths.append(results)\n\n        # sometimes a compressed file contains multiple files\n        # TODO ask for a list of files in the archive to be used from the\n        # adapter\n        return paths\n\n    def _download_api_request(self, api_request: APIRequest):\n        \"\"\"\n        Download an API request and return the path.\n\n        Args:\n            api_request(APIRequest): The API request result that is being cached.\n        Returns:\n            list[str]: The path to the cached API request.\n\n        \"\"\"\n        urls = api_request.url_s if isinstance(api_request.url_s, list) else [api_request.url_s]\n        paths = []\n        for url in urls:\n            fname = url[url.rfind(\"/\") + 1 :].rsplit(\".\", 1)[0]\n            logger.info(f\"Asking for caching API of {api_request.name} {fname}.\")\n            response = requests.get(url=url)\n\n            if response.status_code != 200:\n                response.raise_for_status()\n            response_data = response.json()\n            api_path = os.path.join(self.cache_dir, api_request.name, f\"{fname}.json\")\n\n            os.makedirs(os.path.dirname(api_path), exist_ok=True)\n            with open(api_path, \"w\") as f:\n                json.dump(response_data, f)\n                logger.info(f\"Caching API request to {api_path}.\")\n            paths.append(api_path)\n        return paths\n\n    def get_cached_version(self, resource: Resource) -&gt; list[str]:\n        \"\"\"Get the cached version of a resource.\n\n        Args:\n            resource(Resource): The resource to get the cached version of.\n\n        Returns:\n            list[str]: The paths to the cached resource(s).\n        \"\"\"\n        cached_location = os.path.join(self.cache_dir, resource.name)\n        logger.info(f\"Use cached version from {cached_location}.\")\n        paths = []\n        for file in os.listdir(cached_location):\n            paths.append(os.path.join(cached_location, file))\n        return paths\n\n    def _retrieve(\n        self,\n        url: str,\n        fname: str,\n        path: str,\n        known_hash: str = None,\n    ):\n        \"\"\"\n        Retrieve a file from a URL using Pooch. Infer type of file from\n        extension and use appropriate processor.\n\n        Args:\n            url (str): The URL to retrieve the file from.\n\n            fname (str): The name of the file.\n\n            path (str): The path to the file.\n        \"\"\"\n        if fname.endswith(\".zip\"):\n            return pooch.retrieve(\n                url=url,\n                known_hash=known_hash,\n                fname=fname,\n                path=path,\n                processor=pooch.Unzip(),\n                progressbar=True,\n            )\n\n        elif fname.endswith(\".tar.gz\"):\n            return pooch.retrieve(\n                url=url,\n                known_hash=known_hash,\n                fname=fname,\n                path=path,\n                processor=pooch.Untar(),\n                progressbar=True,\n            )\n\n        elif fname.endswith(\".gz\"):\n            return pooch.retrieve(\n                url=url,\n                known_hash=known_hash,\n                fname=fname,\n                path=path,\n                processor=pooch.Decompress(),\n                progressbar=True,\n            )\n\n        else:\n            return pooch.retrieve(\n                url=url,\n                known_hash=known_hash,\n                fname=fname,\n                path=path,\n                progressbar=True,\n            )\n\n    def _get_files(self, file_download: FileDownload):\n        \"\"\"\n        Get the files contained in a directory file.\n\n        Args:\n            file_download (FileDownload): The directory file.\n\n        Returns:\n            list: The files contained in the directory.\n        \"\"\"\n        if file_download.url_s.startswith(\"ftp://\"):\n            # remove protocol\n            url = file_download.url_s[6:]\n            # get base url\n            url = url[: url.find(\"/\")]\n            # get directory (remove initial slash as well)\n            dir = file_download.url_s[7 + len(url) :]\n            # get files\n            ftp = ftplib.FTP(url)\n            ftp.login()\n            ftp.cwd(dir)\n            files = ftp.nlst()\n            ftp.quit()\n        else:\n            raise NotImplementedError(\"Only FTP directories are supported at the moment.\")\n\n        return files\n\n    def _load_cache_dict(self):\n        \"\"\"\n        Load the cache dictionary from the cache file. Create an empty cache\n        file if it does not exist.\n        \"\"\"\n        if not os.path.exists(self.cache_dir):\n            logger.info(f\"Creating cache directory {self.cache_dir}.\")\n            os.makedirs(self.cache_dir)\n\n        if not os.path.exists(self.cache_file):\n            logger.info(f\"Creating cache file {self.cache_file}.\")\n            with open(self.cache_file, \"w\") as f:\n                json.dump({}, f)\n\n        with open(self.cache_file, \"r\") as f:\n            logger.info(f\"Loading cache file {self.cache_file}.\")\n            return json.load(f)\n\n    def _get_cache_record(self, resource: Resource):\n        \"\"\"\n        Get the cache record of a resource.\n\n        Args:\n            resource (Resource): The resource to get the cache record of.\n\n        Returns:\n            The cache record of the resource.\n        \"\"\"\n        return self.cache_dict.get(resource.name, {})\n\n    def _update_cache_record(self, resource: Resource):\n        \"\"\"\n        Update the cache record of a resource.\n\n        Args:\n            resource (Resource): The resource to update the cache record of.\n        \"\"\"\n        cache_record = {}\n        cache_record[\"url\"] = to_list(resource.url_s)\n        cache_record[\"date_downloaded\"] = str(datetime.now())\n        cache_record[\"lifetime\"] = resource.lifetime\n        self.cache_dict[resource.name] = cache_record\n        with open(self.cache_file, \"w\") as f:\n            json.dump(self.cache_dict, f, default=str)\n</code></pre>"},{"location":"reference/source/#biocypher._get.Downloader.__init__","title":"<code>__init__(cache_dir=None)</code>","text":"<p>The Downloader is a class that manages resources that can be downloaded and cached locally. It manages the lifetime of downloaded resources by keeping a JSON record of the download date of each resource.</p> <p>Parameters:</p> Name Type Description Default <code>cache_dir</code> <code>str</code> <p>The directory where the resources are cached. If not given, a temporary directory is created.</p> <code>None</code> Source code in <code>biocypher/_get.py</code> <pre><code>def __init__(self, cache_dir: Optional[str] = None) -&gt; None:\n    \"\"\"\n    The Downloader is a class that manages resources that can be downloaded\n    and cached locally. It manages the lifetime of downloaded resources by\n    keeping a JSON record of the download date of each resource.\n\n    Args:\n        cache_dir (str): The directory where the resources are cached. If\n            not given, a temporary directory is created.\n    \"\"\"\n    self.cache_dir = cache_dir or TemporaryDirectory().name\n    self.cache_file = os.path.join(self.cache_dir, \"cache.json\")\n    self.cache_dict = self._load_cache_dict()\n</code></pre>"},{"location":"reference/source/#biocypher._get.Downloader._download_api_request","title":"<code>_download_api_request(api_request)</code>","text":"<p>Download an API request and return the path.</p> <p>Parameters:</p> Name Type Description Default <code>api_request(APIRequest)</code> <p>The API request result that is being cached.</p> required <p>Returns:     list[str]: The path to the cached API request.</p> Source code in <code>biocypher/_get.py</code> <pre><code>def _download_api_request(self, api_request: APIRequest):\n    \"\"\"\n    Download an API request and return the path.\n\n    Args:\n        api_request(APIRequest): The API request result that is being cached.\n    Returns:\n        list[str]: The path to the cached API request.\n\n    \"\"\"\n    urls = api_request.url_s if isinstance(api_request.url_s, list) else [api_request.url_s]\n    paths = []\n    for url in urls:\n        fname = url[url.rfind(\"/\") + 1 :].rsplit(\".\", 1)[0]\n        logger.info(f\"Asking for caching API of {api_request.name} {fname}.\")\n        response = requests.get(url=url)\n\n        if response.status_code != 200:\n            response.raise_for_status()\n        response_data = response.json()\n        api_path = os.path.join(self.cache_dir, api_request.name, f\"{fname}.json\")\n\n        os.makedirs(os.path.dirname(api_path), exist_ok=True)\n        with open(api_path, \"w\") as f:\n            json.dump(response_data, f)\n            logger.info(f\"Caching API request to {api_path}.\")\n        paths.append(api_path)\n    return paths\n</code></pre>"},{"location":"reference/source/#biocypher._get.Downloader._download_files","title":"<code>_download_files(cache, file_download)</code>","text":"<p>Download a resource given it is a file or a directory and return the path.</p> <p>Parameters:</p> Name Type Description Default <code>cache</code> <code>bool</code> <p>Whether to cache the resource or not.</p> required <code>file_download</code> <code>FileDownload</code> <p>The resource to download.</p> required <p>Returns:</p> Type Description <p>list[str]: The path or paths to the downloaded resource(s).</p> Source code in <code>biocypher/_get.py</code> <pre><code>def _download_files(self, cache, file_download: FileDownload):\n    \"\"\"\n    Download a resource given it is a file or a directory and return the\n    path.\n\n    Args:\n        cache (bool): Whether to cache the resource or not.\n        file_download (FileDownload): The resource to download.\n\n    Returns:\n        list[str]: The path or paths to the downloaded resource(s).\n    \"\"\"\n    if file_download.is_dir:\n        files = self._get_files(file_download)\n        file_download.url_s = [file_download.url_s + \"/\" + file for file in files]\n        file_download.is_dir = False\n        paths = self._download_or_cache(file_download, cache)\n    elif isinstance(file_download.url_s, list):\n        paths = []\n        for url in file_download.url_s:\n            fname = url[url.rfind(\"/\") + 1 :].split(\"?\")[0]\n            path = self._retrieve(\n                url=url,\n                fname=fname,\n                path=os.path.join(self.cache_dir, file_download.name),\n            )\n            paths.append(path)\n    else:\n        paths = []\n        fname = file_download.url_s[file_download.url_s.rfind(\"/\") + 1 :].split(\"?\")[0]\n        results = self._retrieve(\n            url=file_download.url_s,\n            fname=fname,\n            path=os.path.join(self.cache_dir, file_download.name),\n        )\n        if isinstance(results, list):\n            paths.extend(results)\n        else:\n            paths.append(results)\n\n    # sometimes a compressed file contains multiple files\n    # TODO ask for a list of files in the archive to be used from the\n    # adapter\n    return paths\n</code></pre>"},{"location":"reference/source/#biocypher._get.Downloader._download_or_cache","title":"<code>_download_or_cache(resource, cache=True)</code>","text":"<p>Download a resource if it is not cached or exceeded its lifetime.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>Resource</code> <p>The resource to download.</p> required <p>Returns:     list[str]: The path or paths to the downloaded resource(s).</p> Source code in <code>biocypher/_get.py</code> <pre><code>def _download_or_cache(self, resource: Resource, cache: bool = True):\n    \"\"\"\n    Download a resource if it is not cached or exceeded its lifetime.\n\n    Args:\n        resource (Resource): The resource to download.\n    Returns:\n        list[str]: The path or paths to the downloaded resource(s).\n\n    \"\"\"\n    expired = self._is_cache_expired(resource)\n\n    if expired or not cache:\n        self._delete_expired_cache(resource)\n        if isinstance(resource, FileDownload):\n            logger.info(f\"Asking for download of resource {resource.name}.\")\n            paths = self._download_files(cache, resource)\n        elif isinstance(resource, APIRequest):\n            logger.info(f\"Asking for download of api request {resource.name}.\")\n            paths = self._download_api_request(resource)\n        else:\n            raise TypeError(f\"Unknown resource type: {type(resource)}\")\n    else:\n        paths = self.get_cached_version(resource)\n    self._update_cache_record(resource)\n    return paths\n</code></pre>"},{"location":"reference/source/#biocypher._get.Downloader._get_cache_record","title":"<code>_get_cache_record(resource)</code>","text":"<p>Get the cache record of a resource.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>Resource</code> <p>The resource to get the cache record of.</p> required <p>Returns:</p> Type Description <p>The cache record of the resource.</p> Source code in <code>biocypher/_get.py</code> <pre><code>def _get_cache_record(self, resource: Resource):\n    \"\"\"\n    Get the cache record of a resource.\n\n    Args:\n        resource (Resource): The resource to get the cache record of.\n\n    Returns:\n        The cache record of the resource.\n    \"\"\"\n    return self.cache_dict.get(resource.name, {})\n</code></pre>"},{"location":"reference/source/#biocypher._get.Downloader._get_files","title":"<code>_get_files(file_download)</code>","text":"<p>Get the files contained in a directory file.</p> <p>Parameters:</p> Name Type Description Default <code>file_download</code> <code>FileDownload</code> <p>The directory file.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>The files contained in the directory.</p> Source code in <code>biocypher/_get.py</code> <pre><code>def _get_files(self, file_download: FileDownload):\n    \"\"\"\n    Get the files contained in a directory file.\n\n    Args:\n        file_download (FileDownload): The directory file.\n\n    Returns:\n        list: The files contained in the directory.\n    \"\"\"\n    if file_download.url_s.startswith(\"ftp://\"):\n        # remove protocol\n        url = file_download.url_s[6:]\n        # get base url\n        url = url[: url.find(\"/\")]\n        # get directory (remove initial slash as well)\n        dir = file_download.url_s[7 + len(url) :]\n        # get files\n        ftp = ftplib.FTP(url)\n        ftp.login()\n        ftp.cwd(dir)\n        files = ftp.nlst()\n        ftp.quit()\n    else:\n        raise NotImplementedError(\"Only FTP directories are supported at the moment.\")\n\n    return files\n</code></pre>"},{"location":"reference/source/#biocypher._get.Downloader._is_cache_expired","title":"<code>_is_cache_expired(resource)</code>","text":"<p>Check if resource or API request cache is expired.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>Resource</code> <p>The resource to download.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>cache is expired or not.</p> Source code in <code>biocypher/_get.py</code> <pre><code>def _is_cache_expired(self, resource: Resource) -&gt; bool:\n    \"\"\"\n    Check if resource or API request cache is expired.\n\n    Args:\n        resource (Resource): The resource to download.\n\n    Returns:\n        bool: cache is expired or not.\n    \"\"\"\n    cache_record = self._get_cache_record(resource)\n    if cache_record:\n        download_time = datetime.strptime(cache_record.get(\"date_downloaded\"), \"%Y-%m-%d %H:%M:%S.%f\")\n        lifetime = timedelta(days=resource.lifetime)\n        expired = download_time + lifetime &lt; datetime.now()\n    else:\n        expired = True\n    return expired\n</code></pre>"},{"location":"reference/source/#biocypher._get.Downloader._load_cache_dict","title":"<code>_load_cache_dict()</code>","text":"<p>Load the cache dictionary from the cache file. Create an empty cache file if it does not exist.</p> Source code in <code>biocypher/_get.py</code> <pre><code>def _load_cache_dict(self):\n    \"\"\"\n    Load the cache dictionary from the cache file. Create an empty cache\n    file if it does not exist.\n    \"\"\"\n    if not os.path.exists(self.cache_dir):\n        logger.info(f\"Creating cache directory {self.cache_dir}.\")\n        os.makedirs(self.cache_dir)\n\n    if not os.path.exists(self.cache_file):\n        logger.info(f\"Creating cache file {self.cache_file}.\")\n        with open(self.cache_file, \"w\") as f:\n            json.dump({}, f)\n\n    with open(self.cache_file, \"r\") as f:\n        logger.info(f\"Loading cache file {self.cache_file}.\")\n        return json.load(f)\n</code></pre>"},{"location":"reference/source/#biocypher._get.Downloader._retrieve","title":"<code>_retrieve(url, fname, path, known_hash=None)</code>","text":"<p>Retrieve a file from a URL using Pooch. Infer type of file from extension and use appropriate processor.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to retrieve the file from.</p> required <code>fname</code> <code>str</code> <p>The name of the file.</p> required <code>path</code> <code>str</code> <p>The path to the file.</p> required Source code in <code>biocypher/_get.py</code> <pre><code>def _retrieve(\n    self,\n    url: str,\n    fname: str,\n    path: str,\n    known_hash: str = None,\n):\n    \"\"\"\n    Retrieve a file from a URL using Pooch. Infer type of file from\n    extension and use appropriate processor.\n\n    Args:\n        url (str): The URL to retrieve the file from.\n\n        fname (str): The name of the file.\n\n        path (str): The path to the file.\n    \"\"\"\n    if fname.endswith(\".zip\"):\n        return pooch.retrieve(\n            url=url,\n            known_hash=known_hash,\n            fname=fname,\n            path=path,\n            processor=pooch.Unzip(),\n            progressbar=True,\n        )\n\n    elif fname.endswith(\".tar.gz\"):\n        return pooch.retrieve(\n            url=url,\n            known_hash=known_hash,\n            fname=fname,\n            path=path,\n            processor=pooch.Untar(),\n            progressbar=True,\n        )\n\n    elif fname.endswith(\".gz\"):\n        return pooch.retrieve(\n            url=url,\n            known_hash=known_hash,\n            fname=fname,\n            path=path,\n            processor=pooch.Decompress(),\n            progressbar=True,\n        )\n\n    else:\n        return pooch.retrieve(\n            url=url,\n            known_hash=known_hash,\n            fname=fname,\n            path=path,\n            progressbar=True,\n        )\n</code></pre>"},{"location":"reference/source/#biocypher._get.Downloader._update_cache_record","title":"<code>_update_cache_record(resource)</code>","text":"<p>Update the cache record of a resource.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>Resource</code> <p>The resource to update the cache record of.</p> required Source code in <code>biocypher/_get.py</code> <pre><code>def _update_cache_record(self, resource: Resource):\n    \"\"\"\n    Update the cache record of a resource.\n\n    Args:\n        resource (Resource): The resource to update the cache record of.\n    \"\"\"\n    cache_record = {}\n    cache_record[\"url\"] = to_list(resource.url_s)\n    cache_record[\"date_downloaded\"] = str(datetime.now())\n    cache_record[\"lifetime\"] = resource.lifetime\n    self.cache_dict[resource.name] = cache_record\n    with open(self.cache_file, \"w\") as f:\n        json.dump(self.cache_dict, f, default=str)\n</code></pre>"},{"location":"reference/source/#biocypher._get.Downloader.download","title":"<code>download(*resources)</code>","text":"<p>Download one or multiple resources. Load from cache if the resource is already downloaded and the cache is not expired.</p> <p>Parameters:</p> Name Type Description Default <code>resources</code> <code>Resource</code> <p>The resource(s) to download or load from cache.</p> <code>()</code> <p>Returns:</p> Type Description <p>list[str]: The path or paths to the resource(s) that were downloaded or loaded from cache.</p> Source code in <code>biocypher/_get.py</code> <pre><code>def download(self, *resources: Resource):\n    \"\"\"\n    Download one or multiple resources. Load from cache if the resource is\n    already downloaded and the cache is not expired.\n\n    Args:\n        resources (Resource): The resource(s) to download or load from\n            cache.\n\n    Returns:\n        list[str]: The path or paths to the resource(s) that were downloaded\n            or loaded from cache.\n\n    \"\"\"\n    paths = []\n    for resource in resources:\n        paths.append(self._download_or_cache(resource))\n\n    # flatten list if it is nested\n    if is_nested(paths):\n        paths = [path for sublist in paths for path in sublist]\n\n    return paths\n</code></pre>"},{"location":"reference/source/#biocypher._get.Downloader.get_cached_version","title":"<code>get_cached_version(resource)</code>","text":"<p>Get the cached version of a resource.</p> <p>Parameters:</p> Name Type Description Default <code>resource(Resource)</code> <p>The resource to get the cached version of.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: The paths to the cached resource(s).</p> Source code in <code>biocypher/_get.py</code> <pre><code>def get_cached_version(self, resource: Resource) -&gt; list[str]:\n    \"\"\"Get the cached version of a resource.\n\n    Args:\n        resource(Resource): The resource to get the cached version of.\n\n    Returns:\n        list[str]: The paths to the cached resource(s).\n    \"\"\"\n    cached_location = os.path.join(self.cache_dir, resource.name)\n    logger.info(f\"Use cached version from {cached_location}.\")\n    paths = []\n    for file in os.listdir(cached_location):\n        paths.append(os.path.join(cached_location, file))\n    return paths\n</code></pre>"},{"location":"reference/source/#biocypher._get.FileDownload","title":"<code>FileDownload</code>","text":"<p>               Bases: <code>Resource</code></p> Source code in <code>biocypher/_get.py</code> <pre><code>class FileDownload(Resource):\n    def __init__(\n        self,\n        name: str,\n        url_s: str | list[str],\n        lifetime: int = 0,\n        is_dir: bool = False,\n    ):\n        \"\"\"\n        Represents basic information for a File Download.\n\n        Args:\n            name(str): The name of the File Download.\n\n            url_s(str|list[str]): The URL(s) of the File Download.\n\n            lifetime(int): The lifetime of the File Download in days. If 0, the\n                File Download is cached indefinitely.\n\n            is_dir (bool): Whether the URL points to a directory or not.\n        \"\"\"\n\n        super().__init__(name, url_s, lifetime)\n        self.is_dir = is_dir\n</code></pre>"},{"location":"reference/source/#biocypher._get.FileDownload.__init__","title":"<code>__init__(name, url_s, lifetime=0, is_dir=False)</code>","text":"<p>Represents basic information for a File Download.</p> <p>Parameters:</p> Name Type Description Default <code>name(str)</code> <p>The name of the File Download.</p> required <code>url_s(str|list[str])</code> <p>The URL(s) of the File Download.</p> required <code>lifetime(int)</code> <p>The lifetime of the File Download in days. If 0, the File Download is cached indefinitely.</p> required <code>is_dir</code> <code>bool</code> <p>Whether the URL points to a directory or not.</p> <code>False</code> Source code in <code>biocypher/_get.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    url_s: str | list[str],\n    lifetime: int = 0,\n    is_dir: bool = False,\n):\n    \"\"\"\n    Represents basic information for a File Download.\n\n    Args:\n        name(str): The name of the File Download.\n\n        url_s(str|list[str]): The URL(s) of the File Download.\n\n        lifetime(int): The lifetime of the File Download in days. If 0, the\n            File Download is cached indefinitely.\n\n        is_dir (bool): Whether the URL points to a directory or not.\n    \"\"\"\n\n    super().__init__(name, url_s, lifetime)\n    self.is_dir = is_dir\n</code></pre>"},{"location":"reference/source/#biocypher._get.Resource","title":"<code>Resource</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>biocypher/_get.py</code> <pre><code>class Resource(ABC):\n    def __init__(\n        self,\n        name: str,\n        url_s: str | list[str],\n        lifetime: int = 0,\n    ):\n        \"\"\"\n        A Resource is a file, a list of files, an API request, or a list of API\n        requests, any of which can be downloaded from the given URL(s) and\n        cached locally. This class implements checks of the minimum requirements\n        for a resource, to be implemented by a biocypher adapter.\n\n        Args:\n            name (str): The name of the resource.\n\n            url_s (str | list[str]): The URL or URLs of the resource.\n\n            lifetime (int): The lifetime of the resource in days. If 0, the\n                resource is considered to be permanent.\n        \"\"\"\n        self.name = name\n        self.url_s = url_s\n        self.lifetime = lifetime\n</code></pre>"},{"location":"reference/source/#biocypher._get.Resource.__init__","title":"<code>__init__(name, url_s, lifetime=0)</code>","text":"<p>A Resource is a file, a list of files, an API request, or a list of API requests, any of which can be downloaded from the given URL(s) and cached locally. This class implements checks of the minimum requirements for a resource, to be implemented by a biocypher adapter.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the resource.</p> required <code>url_s</code> <code>str | list[str]</code> <p>The URL or URLs of the resource.</p> required <code>lifetime</code> <code>int</code> <p>The lifetime of the resource in days. If 0, the resource is considered to be permanent.</p> <code>0</code> Source code in <code>biocypher/_get.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    url_s: str | list[str],\n    lifetime: int = 0,\n):\n    \"\"\"\n    A Resource is a file, a list of files, an API request, or a list of API\n    requests, any of which can be downloaded from the given URL(s) and\n    cached locally. This class implements checks of the minimum requirements\n    for a resource, to be implemented by a biocypher adapter.\n\n    Args:\n        name (str): The name of the resource.\n\n        url_s (str | list[str]): The URL or URLs of the resource.\n\n        lifetime (int): The lifetime of the resource in days. If 0, the\n            resource is considered to be permanent.\n    \"\"\"\n    self.name = name\n    self.url_s = url_s\n    self.lifetime = lifetime\n</code></pre>"},{"location":"reference/source/#_loggerpy","title":"_.logger.py","text":"<p>Configuration of the module logger.</p>"},{"location":"reference/source/#biocypher._logger.get_logger","title":"<code>get_logger(name='biocypher')</code>","text":"<p>Access the module logger, create a new one if does not exist yet.</p> <p>Method providing central logger instance to main module. Is called only from main submodule, :mod:<code>biocypher.driver</code>. In child modules, the standard Python logging facility is called (using <code>logging.getLogger(__name__)</code>), automatically inheriting the handlers from the central logger.</p> <p>The file handler creates a log file named after the current date and time. Levels to output to file and console can be set here.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the logger instance.</p> <code>'biocypher'</code> <p>Returns:</p> Type Description <code>Logger</code> <p>An instance of the Python mod:<code>logging.Logger</code>.</p> Source code in <code>biocypher/_logger.py</code> <pre><code>def get_logger(name: str = \"biocypher\") -&gt; logging.Logger:\n    \"\"\"\n    Access the module logger, create a new one if does not exist yet.\n\n    Method providing central logger instance to main module. Is called\n    only from main submodule, :mod:`biocypher.driver`. In child modules,\n    the standard Python logging facility is called\n    (using ``logging.getLogger(__name__)``), automatically inheriting\n    the handlers from the central logger.\n\n    The file handler creates a log file named after the current date and\n    time. Levels to output to file and console can be set here.\n\n    Args:\n        name:\n            Name of the logger instance.\n\n    Returns:\n        An instance of the Python :py:mod:`logging.Logger`.\n    \"\"\"\n\n    if not logging.getLogger(name).hasHandlers():\n        # create logger\n        logger = logging.getLogger(name)\n        logger.setLevel(logging.DEBUG)\n        logger.propagate = True\n\n        # formatting\n        file_formatter = logging.Formatter(\n            \"%(asctime)s\\t%(levelname)s\\tmodule:%(module)s\\n%(message)s\",\n        )\n        stdout_formatter = logging.Formatter(\"%(levelname)s -- %(message)s\")\n\n        # file name and creation\n        now = datetime.now()\n        date_time = now.strftime(\"%Y%m%d-%H%M%S\")\n\n        log_to_disk = _config.config(\"biocypher\").get(\"log_to_disk\")\n\n        if log_to_disk:\n            logdir = _config.config(\"biocypher\").get(\"log_directory\") or \"biocypher-log\"\n            os.makedirs(logdir, exist_ok=True)\n            logfile = os.path.join(logdir, f\"biocypher-{date_time}.log\")\n\n            # file handler\n            file_handler = logging.FileHandler(logfile)\n\n            if _config.config(\"biocypher\").get(\"debug\"):\n                file_handler.setLevel(logging.DEBUG)\n            else:\n                file_handler.setLevel(logging.INFO)\n\n            file_handler.setFormatter(file_formatter)\n\n            logger.addHandler(file_handler)\n\n        # handlers\n        # stream handler\n        stdout_handler = logging.StreamHandler()\n        stdout_handler.setLevel(logging.INFO)\n        stdout_handler.setFormatter(stdout_formatter)\n\n        # add handlers\n        logger.addHandler(stdout_handler)\n\n        # startup message\n        logger.info(f\"This is BioCypher v{__version__}.\")\n        if log_to_disk:\n            logger.info(f\"Logging into `{logfile}`.\")\n        else:\n            logger.info(\"Logging into stdout.\")\n\n    return logging.getLogger(name)\n</code></pre>"},{"location":"reference/source/#biocypher._logger.log","title":"<code>log()</code>","text":"<p>Browse the log file.</p> Source code in <code>biocypher/_logger.py</code> <pre><code>def log():\n    \"\"\"\n    Browse the log file.\n    \"\"\"\n\n    with open(logfile()) as fp:\n        pydoc.pager(fp.read())\n</code></pre>"},{"location":"reference/source/#biocypher._logger.logfile","title":"<code>logfile()</code>","text":"<p>Path to the log file.</p> Source code in <code>biocypher/_logger.py</code> <pre><code>def logfile() -&gt; str:\n    \"\"\"\n    Path to the log file.\n    \"\"\"\n\n    return get_logger().handlers[0].baseFilename\n</code></pre>"},{"location":"reference/source/#_mappingpy","title":"_mapping.py","text":"<p>BioCypher 'mapping' module. Handles the mapping of user-defined schema to the underlying ontology.</p>"},{"location":"reference/source/#biocypher._mapping.OntologyMapping","title":"<code>OntologyMapping</code>","text":"<p>Class to store the ontology mapping and extensions.</p> Source code in <code>biocypher/_mapping.py</code> <pre><code>class OntologyMapping:\n    \"\"\"\n    Class to store the ontology mapping and extensions.\n    \"\"\"\n\n    def __init__(self, config_file: str = None):\n        self.schema = self._read_config(config_file)\n\n        self.extended_schema = self._extend_schema()\n\n    def _read_config(self, config_file: str = None):\n        \"\"\"\n        Read the configuration file and store the ontology mapping and extensions.\n        \"\"\"\n        if config_file is None:\n            schema_config = {}\n\n        # load yaml file from web\n        elif config_file.startswith(\"http\"):\n            with urlopen(config_file) as f:\n                schema_config = yaml.safe_load(f)\n\n        # get graph state from config (assume file is local)\n        else:\n            with open(config_file, \"r\") as f:\n                schema_config = yaml.safe_load(f)\n\n        return schema_config\n\n    def _extend_schema(self, d: Optional[dict] = None) -&gt; dict:\n        \"\"\"\n        Get leaves of the tree hierarchy from the data structure dict\n        contained in the `schema_config.yaml`. Creates virtual leaves\n        (as children) from entries that provide more than one preferred\n        id type (and corresponding inputs).\n\n        Args:\n            d:\n                Data structure dict from yaml file.\n\n        \"\"\"\n\n        d = d or self.schema\n\n        extended_schema = dict()\n\n        # first pass: get parent leaves with direct representation in ontology\n        for k, v in d.items():\n            # k is not an entity\n            if \"represented_as\" not in v:\n                continue\n\n            # preferred_id optional: if not provided, use `id`\n            if not v.get(\"preferred_id\"):\n                v[\"preferred_id\"] = \"id\"\n\n            # k is an entity that is present in the ontology\n            if \"is_a\" not in v:\n                extended_schema[k] = v\n\n        # second pass: \"vertical\" inheritance\n        d = self._vertical_property_inheritance(d)\n        for k, v in d.items():\n            if \"is_a\" in v:\n                # prevent loops\n                if k == v[\"is_a\"]:\n                    logger.warning(\n                        f\"Loop detected in ontology mapping: {k} -&gt; {v}. \"\n                        \"Removing item. Please fix the inheritance if you want \"\n                        \"to use this item.\"\n                    )\n                    continue\n\n                extended_schema[k] = v\n\n        # \"horizontal\" inheritance: create siblings for multiple identifiers or\n        # sources -&gt; virtual leaves or implicit children\n        mi_leaves = {}\n        ms_leaves = {}\n        for k, v in d.items():\n            # k is not an entity\n            if \"represented_as\" not in v:\n                continue\n\n            if isinstance(v.get(\"preferred_id\"), list):\n                mi_leaves = self._horizontal_inheritance_pid(k, v)\n                extended_schema.update(mi_leaves)\n\n            elif isinstance(v.get(\"source\"), list):\n                ms_leaves = self._horizontal_inheritance_source(k, v)\n                extended_schema.update(ms_leaves)\n\n        return extended_schema\n\n    def _vertical_property_inheritance(self, d):\n        \"\"\"\n        Inherit properties from parents to children and update `d` accordingly.\n        \"\"\"\n        for k, v in d.items():\n            # k is not an entity\n            if \"represented_as\" not in v:\n                continue\n\n            # k is an entity that is present in the ontology\n            if \"is_a\" not in v:\n                continue\n\n            # \"vertical\" inheritance: inherit properties from parent\n            if v.get(\"inherit_properties\", False):\n                # get direct ancestor\n                if isinstance(v[\"is_a\"], list):\n                    parent = v[\"is_a\"][0]\n                else:\n                    parent = v[\"is_a\"]\n\n                # ensure child has properties and exclude_properties\n                if \"properties\" not in v:\n                    v[\"properties\"] = {}\n                if \"exclude_properties\" not in v:\n                    v[\"exclude_properties\"] = {}\n\n                # update properties of child\n                parent_props = self.schema[parent].get(\"properties\", {})\n                if parent_props:\n                    v[\"properties\"].update(parent_props)\n\n                parent_excl_props = self.schema[parent].get(\"exclude_properties\", {})\n                if parent_excl_props:\n                    v[\"exclude_properties\"].update(parent_excl_props)\n\n                # update schema (d)\n                d[k] = v\n\n        return d\n\n    def _horizontal_inheritance_pid(self, key, value):\n        \"\"\"\n        Create virtual leaves for multiple preferred id types or sources.\n\n        If we create virtual leaves, input_label/label_in_input always has to be\n        a list.\n        \"\"\"\n\n        leaves = {}\n\n        preferred_id = value[\"preferred_id\"]\n        input_label = value.get(\"input_label\") or value[\"label_in_input\"]\n        represented_as = value[\"represented_as\"]\n\n        # adjust lengths\n        max_l = max(\n            [\n                len(_misc.to_list(preferred_id)),\n                len(_misc.to_list(input_label)),\n                len(_misc.to_list(represented_as)),\n            ],\n        )\n\n        # adjust pid length if necessary\n        if isinstance(preferred_id, str):\n            pids = [preferred_id] * max_l\n        else:\n            pids = preferred_id\n\n        # adjust rep length if necessary\n        if isinstance(represented_as, str):\n            reps = [represented_as] * max_l\n        else:\n            reps = represented_as\n\n        for pid, lab, rep in zip(pids, input_label, reps):\n            skey = pid + \".\" + key\n            svalue = {\n                \"preferred_id\": pid,\n                \"input_label\": lab,\n                \"represented_as\": rep,\n                # mark as virtual\n                \"virtual\": True,\n            }\n\n            # inherit is_a if exists\n            if \"is_a\" in value.keys():\n                # treat as multiple inheritance\n                if isinstance(value[\"is_a\"], list):\n                    v = list(value[\"is_a\"])\n                    v.insert(0, key)\n                    svalue[\"is_a\"] = v\n\n                else:\n                    svalue[\"is_a\"] = [key, value[\"is_a\"]]\n\n            else:\n                # set parent as is_a\n                svalue[\"is_a\"] = key\n\n            # inherit everything except core attributes\n            for k, v in value.items():\n                if k not in [\n                    \"is_a\",\n                    \"preferred_id\",\n                    \"input_label\",\n                    \"label_in_input\",\n                    \"represented_as\",\n                ]:\n                    svalue[k] = v\n\n            leaves[skey] = svalue\n\n        return leaves\n\n    def _horizontal_inheritance_source(self, key, value):\n        \"\"\"\n        Create virtual leaves for multiple sources.\n\n        If we create virtual leaves, input_label/label_in_input always has to be\n        a list.\n        \"\"\"\n\n        leaves = {}\n\n        source = value[\"source\"]\n        input_label = value.get(\"input_label\") or value[\"label_in_input\"]\n        represented_as = value[\"represented_as\"]\n\n        # adjust lengths\n        src_l = len(source)\n\n        # adjust label length if necessary\n        if isinstance(input_label, str):\n            labels = [input_label] * src_l\n        else:\n            labels = input_label\n\n        # adjust rep length if necessary\n        if isinstance(represented_as, str):\n            reps = [represented_as] * src_l\n        else:\n            reps = represented_as\n\n        for src, lab, rep in zip(source, labels, reps):\n            skey = src + \".\" + key\n            svalue = {\n                \"source\": src,\n                \"input_label\": lab,\n                \"represented_as\": rep,\n                # mark as virtual\n                \"virtual\": True,\n            }\n\n            # inherit is_a if exists\n            if \"is_a\" in value.keys():\n                # treat as multiple inheritance\n                if isinstance(value[\"is_a\"], list):\n                    v = list(value[\"is_a\"])\n                    v.insert(0, key)\n                    svalue[\"is_a\"] = v\n\n                else:\n                    svalue[\"is_a\"] = [key, value[\"is_a\"]]\n\n            else:\n                # set parent as is_a\n                svalue[\"is_a\"] = key\n\n            # inherit everything except core attributes\n            for k, v in value.items():\n                if k not in [\n                    \"is_a\",\n                    \"source\",\n                    \"input_label\",\n                    \"label_in_input\",\n                    \"represented_as\",\n                ]:\n                    svalue[k] = v\n\n            leaves[skey] = svalue\n\n        return leaves\n</code></pre>"},{"location":"reference/source/#biocypher._mapping.OntologyMapping._extend_schema","title":"<code>_extend_schema(d=None)</code>","text":"<p>Get leaves of the tree hierarchy from the data structure dict contained in the <code>schema_config.yaml</code>. Creates virtual leaves (as children) from entries that provide more than one preferred id type (and corresponding inputs).</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>Optional[dict]</code> <p>Data structure dict from yaml file.</p> <code>None</code> Source code in <code>biocypher/_mapping.py</code> <pre><code>def _extend_schema(self, d: Optional[dict] = None) -&gt; dict:\n    \"\"\"\n    Get leaves of the tree hierarchy from the data structure dict\n    contained in the `schema_config.yaml`. Creates virtual leaves\n    (as children) from entries that provide more than one preferred\n    id type (and corresponding inputs).\n\n    Args:\n        d:\n            Data structure dict from yaml file.\n\n    \"\"\"\n\n    d = d or self.schema\n\n    extended_schema = dict()\n\n    # first pass: get parent leaves with direct representation in ontology\n    for k, v in d.items():\n        # k is not an entity\n        if \"represented_as\" not in v:\n            continue\n\n        # preferred_id optional: if not provided, use `id`\n        if not v.get(\"preferred_id\"):\n            v[\"preferred_id\"] = \"id\"\n\n        # k is an entity that is present in the ontology\n        if \"is_a\" not in v:\n            extended_schema[k] = v\n\n    # second pass: \"vertical\" inheritance\n    d = self._vertical_property_inheritance(d)\n    for k, v in d.items():\n        if \"is_a\" in v:\n            # prevent loops\n            if k == v[\"is_a\"]:\n                logger.warning(\n                    f\"Loop detected in ontology mapping: {k} -&gt; {v}. \"\n                    \"Removing item. Please fix the inheritance if you want \"\n                    \"to use this item.\"\n                )\n                continue\n\n            extended_schema[k] = v\n\n    # \"horizontal\" inheritance: create siblings for multiple identifiers or\n    # sources -&gt; virtual leaves or implicit children\n    mi_leaves = {}\n    ms_leaves = {}\n    for k, v in d.items():\n        # k is not an entity\n        if \"represented_as\" not in v:\n            continue\n\n        if isinstance(v.get(\"preferred_id\"), list):\n            mi_leaves = self._horizontal_inheritance_pid(k, v)\n            extended_schema.update(mi_leaves)\n\n        elif isinstance(v.get(\"source\"), list):\n            ms_leaves = self._horizontal_inheritance_source(k, v)\n            extended_schema.update(ms_leaves)\n\n    return extended_schema\n</code></pre>"},{"location":"reference/source/#biocypher._mapping.OntologyMapping._horizontal_inheritance_pid","title":"<code>_horizontal_inheritance_pid(key, value)</code>","text":"<p>Create virtual leaves for multiple preferred id types or sources.</p> <p>If we create virtual leaves, input_label/label_in_input always has to be a list.</p> Source code in <code>biocypher/_mapping.py</code> <pre><code>def _horizontal_inheritance_pid(self, key, value):\n    \"\"\"\n    Create virtual leaves for multiple preferred id types or sources.\n\n    If we create virtual leaves, input_label/label_in_input always has to be\n    a list.\n    \"\"\"\n\n    leaves = {}\n\n    preferred_id = value[\"preferred_id\"]\n    input_label = value.get(\"input_label\") or value[\"label_in_input\"]\n    represented_as = value[\"represented_as\"]\n\n    # adjust lengths\n    max_l = max(\n        [\n            len(_misc.to_list(preferred_id)),\n            len(_misc.to_list(input_label)),\n            len(_misc.to_list(represented_as)),\n        ],\n    )\n\n    # adjust pid length if necessary\n    if isinstance(preferred_id, str):\n        pids = [preferred_id] * max_l\n    else:\n        pids = preferred_id\n\n    # adjust rep length if necessary\n    if isinstance(represented_as, str):\n        reps = [represented_as] * max_l\n    else:\n        reps = represented_as\n\n    for pid, lab, rep in zip(pids, input_label, reps):\n        skey = pid + \".\" + key\n        svalue = {\n            \"preferred_id\": pid,\n            \"input_label\": lab,\n            \"represented_as\": rep,\n            # mark as virtual\n            \"virtual\": True,\n        }\n\n        # inherit is_a if exists\n        if \"is_a\" in value.keys():\n            # treat as multiple inheritance\n            if isinstance(value[\"is_a\"], list):\n                v = list(value[\"is_a\"])\n                v.insert(0, key)\n                svalue[\"is_a\"] = v\n\n            else:\n                svalue[\"is_a\"] = [key, value[\"is_a\"]]\n\n        else:\n            # set parent as is_a\n            svalue[\"is_a\"] = key\n\n        # inherit everything except core attributes\n        for k, v in value.items():\n            if k not in [\n                \"is_a\",\n                \"preferred_id\",\n                \"input_label\",\n                \"label_in_input\",\n                \"represented_as\",\n            ]:\n                svalue[k] = v\n\n        leaves[skey] = svalue\n\n    return leaves\n</code></pre>"},{"location":"reference/source/#biocypher._mapping.OntologyMapping._horizontal_inheritance_source","title":"<code>_horizontal_inheritance_source(key, value)</code>","text":"<p>Create virtual leaves for multiple sources.</p> <p>If we create virtual leaves, input_label/label_in_input always has to be a list.</p> Source code in <code>biocypher/_mapping.py</code> <pre><code>def _horizontal_inheritance_source(self, key, value):\n    \"\"\"\n    Create virtual leaves for multiple sources.\n\n    If we create virtual leaves, input_label/label_in_input always has to be\n    a list.\n    \"\"\"\n\n    leaves = {}\n\n    source = value[\"source\"]\n    input_label = value.get(\"input_label\") or value[\"label_in_input\"]\n    represented_as = value[\"represented_as\"]\n\n    # adjust lengths\n    src_l = len(source)\n\n    # adjust label length if necessary\n    if isinstance(input_label, str):\n        labels = [input_label] * src_l\n    else:\n        labels = input_label\n\n    # adjust rep length if necessary\n    if isinstance(represented_as, str):\n        reps = [represented_as] * src_l\n    else:\n        reps = represented_as\n\n    for src, lab, rep in zip(source, labels, reps):\n        skey = src + \".\" + key\n        svalue = {\n            \"source\": src,\n            \"input_label\": lab,\n            \"represented_as\": rep,\n            # mark as virtual\n            \"virtual\": True,\n        }\n\n        # inherit is_a if exists\n        if \"is_a\" in value.keys():\n            # treat as multiple inheritance\n            if isinstance(value[\"is_a\"], list):\n                v = list(value[\"is_a\"])\n                v.insert(0, key)\n                svalue[\"is_a\"] = v\n\n            else:\n                svalue[\"is_a\"] = [key, value[\"is_a\"]]\n\n        else:\n            # set parent as is_a\n            svalue[\"is_a\"] = key\n\n        # inherit everything except core attributes\n        for k, v in value.items():\n            if k not in [\n                \"is_a\",\n                \"source\",\n                \"input_label\",\n                \"label_in_input\",\n                \"represented_as\",\n            ]:\n                svalue[k] = v\n\n        leaves[skey] = svalue\n\n    return leaves\n</code></pre>"},{"location":"reference/source/#biocypher._mapping.OntologyMapping._read_config","title":"<code>_read_config(config_file=None)</code>","text":"<p>Read the configuration file and store the ontology mapping and extensions.</p> Source code in <code>biocypher/_mapping.py</code> <pre><code>def _read_config(self, config_file: str = None):\n    \"\"\"\n    Read the configuration file and store the ontology mapping and extensions.\n    \"\"\"\n    if config_file is None:\n        schema_config = {}\n\n    # load yaml file from web\n    elif config_file.startswith(\"http\"):\n        with urlopen(config_file) as f:\n            schema_config = yaml.safe_load(f)\n\n    # get graph state from config (assume file is local)\n    else:\n        with open(config_file, \"r\") as f:\n            schema_config = yaml.safe_load(f)\n\n    return schema_config\n</code></pre>"},{"location":"reference/source/#biocypher._mapping.OntologyMapping._vertical_property_inheritance","title":"<code>_vertical_property_inheritance(d)</code>","text":"<p>Inherit properties from parents to children and update <code>d</code> accordingly.</p> Source code in <code>biocypher/_mapping.py</code> <pre><code>def _vertical_property_inheritance(self, d):\n    \"\"\"\n    Inherit properties from parents to children and update `d` accordingly.\n    \"\"\"\n    for k, v in d.items():\n        # k is not an entity\n        if \"represented_as\" not in v:\n            continue\n\n        # k is an entity that is present in the ontology\n        if \"is_a\" not in v:\n            continue\n\n        # \"vertical\" inheritance: inherit properties from parent\n        if v.get(\"inherit_properties\", False):\n            # get direct ancestor\n            if isinstance(v[\"is_a\"], list):\n                parent = v[\"is_a\"][0]\n            else:\n                parent = v[\"is_a\"]\n\n            # ensure child has properties and exclude_properties\n            if \"properties\" not in v:\n                v[\"properties\"] = {}\n            if \"exclude_properties\" not in v:\n                v[\"exclude_properties\"] = {}\n\n            # update properties of child\n            parent_props = self.schema[parent].get(\"properties\", {})\n            if parent_props:\n                v[\"properties\"].update(parent_props)\n\n            parent_excl_props = self.schema[parent].get(\"exclude_properties\", {})\n            if parent_excl_props:\n                v[\"exclude_properties\"].update(parent_excl_props)\n\n            # update schema (d)\n            d[k] = v\n\n    return d\n</code></pre>"},{"location":"reference/source/#_metadatapy","title":"_metadata.py","text":"<p>Package metadata (version, authors, etc).</p>"},{"location":"reference/source/#biocypher._metadata.get_metadata","title":"<code>get_metadata()</code>","text":"<p>Basic package metadata.</p> <p>Retrieves package metadata from the current project directory or from the installed package.</p> Source code in <code>biocypher/_metadata.py</code> <pre><code>def get_metadata():\n    \"\"\"\n    Basic package metadata.\n\n    Retrieves package metadata from the current project directory or from\n    the installed package.\n    \"\"\"\n\n    here = pathlib.Path(__file__).parent\n    pyproj_toml = \"pyproject.toml\"\n    meta = {}\n\n    for project_dir in (here, here.parent):\n        toml_path = str(project_dir.joinpath(pyproj_toml).absolute())\n\n        if os.path.exists(toml_path):\n            pyproject = toml.load(toml_path)\n\n            meta = {\n                \"name\": pyproject[\"tool\"][\"poetry\"][\"name\"],\n                \"version\": pyproject[\"tool\"][\"poetry\"][\"version\"],\n                \"author\": pyproject[\"tool\"][\"poetry\"][\"authors\"],\n                \"license\": pyproject[\"tool\"][\"poetry\"][\"license\"],\n                \"full_metadata\": pyproject,\n            }\n\n            break\n\n    if not meta:\n        try:\n            meta = {k.lower(): v for k, v in importlib.metadata.metadata(here.name).items()}\n\n        except importlib.metadata.PackageNotFoundError:\n            pass\n\n    meta[\"version\"] = meta.get(\"version\", None) or _VERSION\n\n    return meta\n</code></pre>"},{"location":"reference/source/#_miscpy","title":"_misc.py","text":"<p>Handy functions for use in various places.</p>"},{"location":"reference/source/#biocypher._misc._get_inheritance_tree","title":"<code>_get_inheritance_tree(inheritance_graph)</code>","text":"<p>Transforms an inheritance_graph into an inheritance_tree.</p> <p>Parameters:</p> Name Type Description Default <code>inheritance_graph</code> <code>Union[dict, Graph]</code> <p>A dict or nx.Graph representing the inheritance graph.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dict representing the inheritance tree.</p> Source code in <code>biocypher/_misc.py</code> <pre><code>def _get_inheritance_tree(inheritance_graph: Union[dict, nx.Graph]) -&gt; dict:\n    \"\"\"Transforms an inheritance_graph into an inheritance_tree.\n\n    Args:\n        inheritance_graph: A dict or nx.Graph representing the inheritance graph.\n\n    Returns:\n        A dict representing the inheritance tree.\n    \"\"\"\n    if isinstance(inheritance_graph, nx.Graph):\n        inheritance_tree = nx.to_dict_of_lists(inheritance_graph)\n\n        multiple_parents_present = _multiple_inheritance_present(inheritance_tree)\n        if multiple_parents_present:\n            logger.warning(\n                \"The ontology contains multiple inheritance (one child node \"\n                \"has multiple parent nodes). This is not visualized in the \"\n                \"following hierarchy tree (the child node is only added once). \"\n                \"If you wish to browse all relationships of the parsed \"\n                \"ontologies, write a graphml file to disk using \"\n                \"`to_disk = &lt;directory&gt;` and view this file.\"\n            )\n\n        # unlist values\n        inheritance_tree = {k: v[0] for k, v in inheritance_tree.items() if v}\n        return inheritance_tree\n    elif not _multiple_inheritance_present(inheritance_graph):\n        return inheritance_graph\n</code></pre>"},{"location":"reference/source/#biocypher._misc._multiple_inheritance_present","title":"<code>_multiple_inheritance_present(inheritance_tree)</code>","text":"<p>Checks if multiple inheritance is present in the inheritance_tree.</p> Source code in <code>biocypher/_misc.py</code> <pre><code>def _multiple_inheritance_present(inheritance_tree: dict) -&gt; bool:\n    \"\"\"Checks if multiple inheritance is present in the inheritance_tree.\"\"\"\n    return any(len(value) &gt; 1 for value in inheritance_tree.values())\n</code></pre>"},{"location":"reference/source/#biocypher._misc.create_tree_visualisation","title":"<code>create_tree_visualisation(inheritance_graph)</code>","text":"<p>Creates a visualisation of the inheritance tree using treelib.</p> Source code in <code>biocypher/_misc.py</code> <pre><code>def create_tree_visualisation(inheritance_graph: Union[dict, nx.Graph]) -&gt; Tree:\n    \"\"\"\n    Creates a visualisation of the inheritance tree using treelib.\n    \"\"\"\n    inheritance_tree = _get_inheritance_tree(inheritance_graph)\n    classes, root = _find_root_node(inheritance_tree)\n\n    tree = Tree()\n    tree.create_node(root, root)\n    while classes:\n        for child in classes:\n            parent = inheritance_tree[child]\n            if parent in tree.nodes.keys() or parent == root:\n                tree.create_node(child, child, parent=parent)\n\n        for node in tree.nodes.keys():\n            if node in classes:\n                classes.remove(node)\n\n    return tree\n</code></pre>"},{"location":"reference/source/#biocypher._misc.ensure_iterable","title":"<code>ensure_iterable(value)</code>","text":"<p>Returns iterables, except strings, wraps simple types into tuple.</p> Source code in <code>biocypher/_misc.py</code> <pre><code>def ensure_iterable(value: Any) -&gt; Iterable:\n    \"\"\"\n    Returns iterables, except strings, wraps simple types into tuple.\n    \"\"\"\n\n    return value if isinstance(value, LIST_LIKE) else (value,)\n</code></pre>"},{"location":"reference/source/#biocypher._misc.is_nested","title":"<code>is_nested(lst)</code>","text":"<p>Check if a list is nested.</p> <p>Parameters:</p> Name Type Description Default <code>lst</code> <code>list</code> <p>The list to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the list is nested, False otherwise.</p> Source code in <code>biocypher/_misc.py</code> <pre><code>def is_nested(lst) -&gt; bool:\n    \"\"\"\n    Check if a list is nested.\n\n    Args:\n        lst (list): The list to check.\n\n    Returns:\n        bool: True if the list is nested, False otherwise.\n    \"\"\"\n    for item in lst:\n        if isinstance(item, list):\n            return True\n    return False\n</code></pre>"},{"location":"reference/source/#biocypher._misc.pascalcase_to_sentencecase","title":"<code>pascalcase_to_sentencecase(s)</code>","text":"<p>Convert PascalCase to sentence case.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>Input string in PascalCase</p> required <p>Returns:</p> Type Description <code>str</code> <p>string in sentence case form</p> Source code in <code>biocypher/_misc.py</code> <pre><code>def pascalcase_to_sentencecase(s: str) -&gt; str:\n    \"\"\"\n    Convert PascalCase to sentence case.\n\n    Args:\n        s: Input string in PascalCase\n\n    Returns:\n        string in sentence case form\n    \"\"\"\n    return from_pascal(s, sep=\" \")\n</code></pre>"},{"location":"reference/source/#biocypher._misc.sentencecase_to_pascalcase","title":"<code>sentencecase_to_pascalcase(s, sep='\\\\s')</code>","text":"<p>Convert sentence case to PascalCase.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>Input string in sentence case</p> required <p>Returns:</p> Type Description <code>str</code> <p>string in PascalCase form</p> Source code in <code>biocypher/_misc.py</code> <pre><code>def sentencecase_to_pascalcase(s: str, sep: str = r\"\\s\") -&gt; str:\n    \"\"\"\n    Convert sentence case to PascalCase.\n\n    Args:\n        s: Input string in sentence case\n\n    Returns:\n        string in PascalCase form\n    \"\"\"\n    return re.sub(\n        r\"(?:^|[\" + sep + \"])([a-zA-Z])\",\n        lambda match: match.group(1).upper(),\n        s,\n    )\n</code></pre>"},{"location":"reference/source/#biocypher._misc.sentencecase_to_snakecase","title":"<code>sentencecase_to_snakecase(s)</code>","text":"<p>Convert sentence case to snake_case.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>Input string in sentence case</p> required <p>Returns:</p> Type Description <code>str</code> <p>string in snake_case form</p> Source code in <code>biocypher/_misc.py</code> <pre><code>def sentencecase_to_snakecase(s: str) -&gt; str:\n    \"\"\"\n    Convert sentence case to snake_case.\n\n    Args:\n        s: Input string in sentence case\n\n    Returns:\n        string in snake_case form\n    \"\"\"\n    return stringcase.snakecase(s).lower()\n</code></pre>"},{"location":"reference/source/#biocypher._misc.snakecase_to_sentencecase","title":"<code>snakecase_to_sentencecase(s)</code>","text":"<p>Convert snake_case to sentence case.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>Input string in snake_case</p> required <p>Returns:</p> Type Description <code>str</code> <p>string in sentence case form</p> Source code in <code>biocypher/_misc.py</code> <pre><code>def snakecase_to_sentencecase(s: str) -&gt; str:\n    \"\"\"\n    Convert snake_case to sentence case.\n\n    Args:\n        s: Input string in snake_case\n\n    Returns:\n        string in sentence case form\n    \"\"\"\n    return stringcase.sentencecase(s).lower()\n</code></pre>"},{"location":"reference/source/#biocypher._misc.to_list","title":"<code>to_list(value)</code>","text":"<p>Ensures that <code>value</code> is a list.</p> Source code in <code>biocypher/_misc.py</code> <pre><code>def to_list(value: Any) -&gt; list:\n    \"\"\"\n    Ensures that ``value`` is a list.\n    \"\"\"\n\n    if isinstance(value, LIST_LIKE):\n        value = list(value)\n\n    else:\n        value = [value]\n\n    return value\n</code></pre>"},{"location":"reference/source/#biocypher._misc.to_lower_sentence_case","title":"<code>to_lower_sentence_case(s)</code>","text":"<p>Convert any string to lower sentence case. Works with snake_case, PascalCase, and sentence case.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>Input string</p> required <p>Returns:</p> Type Description <code>str</code> <p>string in lower sentence case form</p> Source code in <code>biocypher/_misc.py</code> <pre><code>def to_lower_sentence_case(s: str) -&gt; str:\n    \"\"\"\n    Convert any string to lower sentence case. Works with snake_case,\n    PascalCase, and sentence case.\n\n    Args:\n        s: Input string\n\n    Returns:\n        string in lower sentence case form\n    \"\"\"\n    if \"_\" in s:\n        return snakecase_to_sentencecase(s)\n    elif \" \" in s:\n        return s.lower()\n    elif s[0].isupper():\n        return pascalcase_to_sentencecase(s)\n    else:\n        return s\n</code></pre>"},{"location":"reference/source/#_ontologypy","title":"_ontology.py","text":"<p>BioCypher 'ontology' module. Contains classes and functions to handle parsing and representation of single ontologies as well as their hybridisation and other advanced operations.</p>"},{"location":"reference/source/#biocypher._ontology.Ontology","title":"<code>Ontology</code>","text":"<p>A class that represents the ontological \"backbone\" of a BioCypher knowledge graph. The ontology can be built from a single resource, or hybridised from a combination of resources, with one resource being the \"head\" ontology, while an arbitrary number of other resources can become \"tail\" ontologies at arbitrary fusion points inside the \"head\" ontology.</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>class Ontology:\n    \"\"\"A class that represents the ontological \"backbone\" of a BioCypher knowledge\n    graph. The ontology can be built from a single resource, or hybridised from\n    a combination of resources, with one resource being the \"head\" ontology,\n    while an arbitrary number of other resources can become \"tail\" ontologies at\n    arbitrary fusion points inside the \"head\" ontology.\n    \"\"\"\n\n    def __init__(\n        self,\n        head_ontology: dict,\n        ontology_mapping: Optional[\"OntologyMapping\"] = None,\n        tail_ontologies: dict | None = None,\n    ):\n        \"\"\"Initialize the Ontology class.\n\n        Args:\n        ----\n            head_ontology (OntologyAdapter): The head ontology.\n\n            tail_ontologies (list): A list of OntologyAdapters that will be\n                added to the head ontology. Defaults to None.\n\n        \"\"\"\n        self._head_ontology_meta = head_ontology\n        self.mapping = ontology_mapping\n        self._tail_ontology_meta = tail_ontologies\n\n        self._tail_ontologies = None\n        self._nx_graph = None\n\n        # keep track of nodes that have been extended\n        self._extended_nodes = set()\n\n        self._main()\n\n    def _main(self) -&gt; None:\n        \"\"\"Main method to be run on instantiation. Loads the ontologies, joins\n        them, and returns the hybrid ontology. Loads only the head ontology\n        if nothing else is given. Adds user extensions and properties from\n        the mapping.\n        \"\"\"\n        self._load_ontologies()\n\n        if self._tail_ontologies:\n            for adapter in self._tail_ontologies.values():\n                head_join_node = self._get_head_join_node(adapter)\n                self._join_ontologies(adapter, head_join_node)\n        else:\n            self._nx_graph = self._head_ontology.get_nx_graph()\n\n        if self.mapping:\n            self._extend_ontology()\n\n            # experimental: add connections of disjoint classes to entity\n            # self._connect_biolink_classes()\n\n            self._add_properties()\n\n    def _load_ontologies(self) -&gt; None:\n        \"\"\"For each ontology, load the OntologyAdapter object and store it as an\n        instance variable (head) or a dictionary (tail).\n        \"\"\"\n        logger.info(\"Loading ontologies...\")\n\n        self._head_ontology = OntologyAdapter(\n            ontology_file=self._head_ontology_meta[\"url\"],\n            root_label=self._head_ontology_meta[\"root_node\"],\n            ontology_file_format=self._head_ontology_meta.get(\"format\", None),\n            switch_label_and_id=self._head_ontology_meta.get(\"switch_label_and_id\", True),\n        )\n\n        if self._tail_ontology_meta:\n            self._tail_ontologies = {}\n            for key, value in self._tail_ontology_meta.items():\n                self._tail_ontologies[key] = OntologyAdapter(\n                    ontology_file=value[\"url\"],\n                    root_label=value[\"tail_join_node\"],\n                    head_join_node_label=value[\"head_join_node\"],\n                    ontology_file_format=value.get(\"format\", None),\n                    merge_nodes=value.get(\"merge_nodes\", True),\n                    switch_label_and_id=value.get(\"switch_label_and_id\", True),\n                )\n\n    def _get_head_join_node(self, adapter: OntologyAdapter) -&gt; str:\n        \"\"\"Tries to find the head join node of the given ontology adapter in the\n        head ontology. If the join node is not found, the method will raise an\n        error.\n\n        Args:\n        ----\n            adapter (OntologyAdapter): The ontology adapter of which to find the\n                join node in the head ontology.\n\n        \"\"\"\n        head_join_node = None\n        user_defined_head_join_node_label = adapter.get_head_join_node()\n        head_join_node_label_in_bc_format = to_lower_sentence_case(user_defined_head_join_node_label.replace(\"_\", \" \"))\n\n        if self._head_ontology._switch_label_and_id:\n            head_join_node = head_join_node_label_in_bc_format\n        elif not self._head_ontology._switch_label_and_id:\n            for node_id, data in self._head_ontology.get_nx_graph().nodes(data=True):\n                if \"label\" in data and data[\"label\"] == head_join_node_label_in_bc_format:\n                    head_join_node = node_id\n                    break\n\n        if head_join_node not in self._head_ontology.get_nx_graph().nodes:\n            head_ontology = self._head_ontology._rdf_to_nx(\n                self._head_ontology.get_rdf_graph(),\n                self._head_ontology._root_label,\n                self._head_ontology._switch_label_and_id,\n                rename_nodes=False,\n            )\n            raise ValueError(\n                f\"Head join node '{head_join_node}' not found in head ontology. \"\n                f\"The head ontology contains the following nodes: {head_ontology.nodes}.\",\n            )\n        return head_join_node\n\n    def _join_ontologies(self, adapter: OntologyAdapter, head_join_node) -&gt; None:\n        \"\"\"Joins the ontologies by adding the tail ontology as a subgraph to the\n        head ontology at the specified join nodes.\n\n        Args:\n        ----\n            adapter (OntologyAdapter): The ontology adapter of the tail ontology\n                to be added to the head ontology.\n\n        \"\"\"\n        if not self._nx_graph:\n            self._nx_graph = self._head_ontology.get_nx_graph().copy()\n\n        tail_join_node = adapter.get_root_node()\n        tail_ontology = adapter.get_nx_graph()\n\n        # subtree of tail ontology at join node\n        tail_ontology_subtree = nx.dfs_tree(tail_ontology.reverse(), tail_join_node).reverse()\n\n        # transfer node attributes from tail ontology to subtree\n        for node in tail_ontology_subtree.nodes:\n            tail_ontology_subtree.nodes[node].update(tail_ontology.nodes[node])\n\n        # if merge_nodes is False, create parent of tail join node from head\n        # join node\n        if not adapter._merge_nodes:\n            # add head join node from head ontology to tail ontology subtree\n            # as parent of tail join node\n            tail_ontology_subtree.add_node(\n                head_join_node,\n                **self._head_ontology.get_nx_graph().nodes[head_join_node],\n            )\n            tail_ontology_subtree.add_edge(tail_join_node, head_join_node)\n\n        # else rename tail join node to match head join node if necessary\n        elif tail_join_node != head_join_node:\n            tail_ontology_subtree = nx.relabel_nodes(tail_ontology_subtree, {tail_join_node: head_join_node})\n\n        # combine head ontology and tail subtree\n        self._nx_graph = nx.compose(self._nx_graph, tail_ontology_subtree)\n\n    def _extend_ontology(self) -&gt; None:\n        \"\"\"Add the user extensions to the ontology.\n\n        Tries to find the parent in the ontology, adds it if necessary, and adds\n        the child and a directed edge from child to parent. Can handle multiple\n        parents.\n        \"\"\"\n        if not self._nx_graph:\n            self._nx_graph = self._head_ontology.get_nx_graph().copy()\n\n        for key, value in self.mapping.extended_schema.items():\n            if not value.get(\"is_a\"):\n                if self._nx_graph.has_node(value.get(\"synonym_for\")):\n                    continue\n\n                if not self._nx_graph.has_node(key):\n                    msg = (\n                        f\"Node {key} not found in ontology, but also has no inheritance definition. Please check your \"\n                        \"schema for spelling errors, first letter not in lower case, use of underscores, a missing \"\n                        \"`is_a` definition (SubClassOf a root node), or missing labels in class or super-classes.\"\n                    )\n                    logger.error(msg)\n                    raise ValueError(msg)\n\n                continue\n\n            parents = to_list(value.get(\"is_a\"))\n            child = key\n\n            while parents:\n                parent = parents.pop(0)\n\n                if parent not in self._nx_graph.nodes:\n                    self._nx_graph.add_node(parent)\n                    self._nx_graph.nodes[parent][\"label\"] = sentencecase_to_pascalcase(parent)\n\n                    # mark parent as user extension\n                    self._nx_graph.nodes[parent][\"user_extension\"] = True\n                    self._extended_nodes.add(parent)\n\n                if child not in self._nx_graph.nodes:\n                    self._nx_graph.add_node(child)\n                    self._nx_graph.nodes[child][\"label\"] = sentencecase_to_pascalcase(child)\n\n                    # mark child as user extension\n                    self._nx_graph.nodes[child][\"user_extension\"] = True\n                    self._extended_nodes.add(child)\n\n                self._nx_graph.add_edge(child, parent)\n\n                child = parent\n\n    def _connect_biolink_classes(self) -&gt; None:\n        \"\"\"Experimental: Adds edges from disjoint classes to the entity node.\"\"\"\n        if not self._nx_graph:\n            self._nx_graph = self._head_ontology.get_nx_graph().copy()\n\n        if \"entity\" not in self._nx_graph.nodes:\n            return\n\n        # biolink classes that are disjoint from entity\n        disjoint_classes = [\n            \"frequency qualifier mixin\",\n            \"chemical entity to entity association mixin\",\n            \"ontology class\",\n            \"relationship quantifier\",\n            \"physical essence or occurrent\",\n            \"gene or gene product\",\n            \"subject of investigation\",\n        ]\n\n        for node in disjoint_classes:\n            if not self._nx_graph.nodes.get(node):\n                self._nx_graph.add_node(node)\n                self._nx_graph.nodes[node][\"label\"] = sentencecase_to_pascalcase(node)\n\n            self._nx_graph.add_edge(node, \"entity\")\n\n    def _add_properties(self) -&gt; None:\n        \"\"\"For each entity in the mapping, update the ontology with the properties\n        specified in the mapping. Updates synonym information in the graph,\n        setting the synonym as the primary node label.\n        \"\"\"\n        for key, value in self.mapping.extended_schema.items():\n            if key in self._nx_graph.nodes:\n                self._nx_graph.nodes[key].update(value)\n\n            if value.get(\"synonym_for\"):\n                # change node label to synonym\n                if value[\"synonym_for\"] not in self._nx_graph.nodes:\n                    raise ValueError(f\"Node {value['synonym_for']} not found in ontology.\")\n\n                self._nx_graph = nx.relabel_nodes(self._nx_graph, {value[\"synonym_for\"]: key})\n\n    def get_ancestors(self, node_label: str) -&gt; list:\n        \"\"\"Get the ancestors of a node in the ontology.\n\n        Args:\n        ----\n            node_label (str): The label of the node in the ontology.\n\n        Returns:\n        -------\n            list: A list of the ancestors of the node.\n\n        \"\"\"\n        return nx.dfs_tree(self._nx_graph, node_label)\n\n    def show_ontology_structure(self, to_disk: str = None, full: bool = False):\n        \"\"\"Show the ontology structure using treelib or write to GRAPHML file.\n\n        Args:\n        ----\n            to_disk (str): If specified, the ontology structure will be saved\n                to disk as a GRAPHML file at the location (directory) specified\n                by the `to_disk` string, to be opened in your favourite graph\n                visualisation tool.\n\n            full (bool): If True, the full ontology structure will be shown,\n                including all nodes and edges. If False, only the nodes and\n                edges that are relevant to the extended schema will be shown.\n\n        \"\"\"\n        if not full and not self.mapping.extended_schema:\n            raise ValueError(\n                \"You are attempting to visualise a subset of the loaded\"\n                \"ontology, but have not provided a schema configuration. \"\n                \"To display a partial ontology graph, please provide a schema \"\n                \"configuration file; to visualise the full graph, please use \"\n                \"the parameter `full = True`.\",\n            )\n\n        if not self._nx_graph:\n            raise ValueError(\"Ontology not loaded.\")\n\n        if not self._tail_ontologies:\n            msg = f\"Showing ontology structure based on {self._head_ontology._ontology_file}\"\n\n        else:\n            msg = f\"Showing ontology structure based on {len(self._tail_ontology_meta) + 1} ontologies: \"\n\n        logger.info(msg)\n\n        if not full:\n            # set of leaves and their intermediate parents up to the root\n            filter_nodes = set(self.mapping.extended_schema.keys())\n\n            for node in self.mapping.extended_schema.keys():\n                filter_nodes.update(self.get_ancestors(node).nodes)\n\n            # filter graph\n            G = self._nx_graph.subgraph(filter_nodes)\n\n        else:\n            G = self._nx_graph\n\n        if not to_disk:\n            # create tree\n            tree = create_tree_visualisation(G)\n\n            # add synonym information\n            for node in self.mapping.extended_schema:\n                if not isinstance(self.mapping.extended_schema[node], dict):\n                    continue\n                if self.mapping.extended_schema[node].get(\"synonym_for\"):\n                    tree.nodes[node].tag = f\"{node} = {self.mapping.extended_schema[node].get('synonym_for')}\"\n\n            logger.info(f\"\\n{tree}\")\n\n            return tree\n\n        else:\n            # convert lists/dicts to strings for vis only\n            for node in G.nodes:\n                # rename node and use former id as label\n                label = G.nodes[node].get(\"label\")\n\n                if not label:\n                    label = node\n\n                G = nx.relabel_nodes(G, {node: label})\n                G.nodes[label][\"label\"] = node\n\n                for attrib in G.nodes[label]:\n                    if type(G.nodes[label][attrib]) in [list, dict]:\n                        G.nodes[label][attrib] = str(G.nodes[label][attrib])\n\n            path = os.path.join(to_disk, \"ontology_structure.graphml\")\n\n            logger.info(f\"Writing ontology structure to {path}.\")\n\n            nx.write_graphml(G, path)\n\n            return True\n\n    def get_dict(self) -&gt; dict:\n        \"\"\"Returns a dictionary compatible with a BioCypher node for compatibility\n        with the Neo4j driver.\n        \"\"\"\n        d = {\n            \"node_id\": self._get_current_id(),\n            \"node_label\": \"BioCypher\",\n            \"properties\": {\n                \"schema\": \"self.ontology_mapping.extended_schema\",\n            },\n        }\n\n        return d\n\n    def _get_current_id(self):\n        \"\"\"Instantiate a version ID for the current session. For now does simple\n        versioning using datetime.\n\n        Can later implement incremental versioning, versioning from\n        config file, or manual specification via argument.\n        \"\"\"\n        now = datetime.now()\n        return now.strftime(\"v%Y%m%d-%H%M%S\")\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.Ontology.__init__","title":"<code>__init__(head_ontology, ontology_mapping=None, tail_ontologies=None)</code>","text":"<p>Initialize the Ontology class.</p> <pre><code>head_ontology (OntologyAdapter): The head ontology.\n\ntail_ontologies (list): A list of OntologyAdapters that will be\n    added to the head ontology. Defaults to None.\n</code></pre> Source code in <code>biocypher/_ontology.py</code> <pre><code>def __init__(\n    self,\n    head_ontology: dict,\n    ontology_mapping: Optional[\"OntologyMapping\"] = None,\n    tail_ontologies: dict | None = None,\n):\n    \"\"\"Initialize the Ontology class.\n\n    Args:\n    ----\n        head_ontology (OntologyAdapter): The head ontology.\n\n        tail_ontologies (list): A list of OntologyAdapters that will be\n            added to the head ontology. Defaults to None.\n\n    \"\"\"\n    self._head_ontology_meta = head_ontology\n    self.mapping = ontology_mapping\n    self._tail_ontology_meta = tail_ontologies\n\n    self._tail_ontologies = None\n    self._nx_graph = None\n\n    # keep track of nodes that have been extended\n    self._extended_nodes = set()\n\n    self._main()\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.Ontology._add_properties","title":"<code>_add_properties()</code>","text":"<p>For each entity in the mapping, update the ontology with the properties specified in the mapping. Updates synonym information in the graph, setting the synonym as the primary node label.</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>def _add_properties(self) -&gt; None:\n    \"\"\"For each entity in the mapping, update the ontology with the properties\n    specified in the mapping. Updates synonym information in the graph,\n    setting the synonym as the primary node label.\n    \"\"\"\n    for key, value in self.mapping.extended_schema.items():\n        if key in self._nx_graph.nodes:\n            self._nx_graph.nodes[key].update(value)\n\n        if value.get(\"synonym_for\"):\n            # change node label to synonym\n            if value[\"synonym_for\"] not in self._nx_graph.nodes:\n                raise ValueError(f\"Node {value['synonym_for']} not found in ontology.\")\n\n            self._nx_graph = nx.relabel_nodes(self._nx_graph, {value[\"synonym_for\"]: key})\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.Ontology._connect_biolink_classes","title":"<code>_connect_biolink_classes()</code>","text":"<p>Experimental: Adds edges from disjoint classes to the entity node.</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>def _connect_biolink_classes(self) -&gt; None:\n    \"\"\"Experimental: Adds edges from disjoint classes to the entity node.\"\"\"\n    if not self._nx_graph:\n        self._nx_graph = self._head_ontology.get_nx_graph().copy()\n\n    if \"entity\" not in self._nx_graph.nodes:\n        return\n\n    # biolink classes that are disjoint from entity\n    disjoint_classes = [\n        \"frequency qualifier mixin\",\n        \"chemical entity to entity association mixin\",\n        \"ontology class\",\n        \"relationship quantifier\",\n        \"physical essence or occurrent\",\n        \"gene or gene product\",\n        \"subject of investigation\",\n    ]\n\n    for node in disjoint_classes:\n        if not self._nx_graph.nodes.get(node):\n            self._nx_graph.add_node(node)\n            self._nx_graph.nodes[node][\"label\"] = sentencecase_to_pascalcase(node)\n\n        self._nx_graph.add_edge(node, \"entity\")\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.Ontology._extend_ontology","title":"<code>_extend_ontology()</code>","text":"<p>Add the user extensions to the ontology.</p> <p>Tries to find the parent in the ontology, adds it if necessary, and adds the child and a directed edge from child to parent. Can handle multiple parents.</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>def _extend_ontology(self) -&gt; None:\n    \"\"\"Add the user extensions to the ontology.\n\n    Tries to find the parent in the ontology, adds it if necessary, and adds\n    the child and a directed edge from child to parent. Can handle multiple\n    parents.\n    \"\"\"\n    if not self._nx_graph:\n        self._nx_graph = self._head_ontology.get_nx_graph().copy()\n\n    for key, value in self.mapping.extended_schema.items():\n        if not value.get(\"is_a\"):\n            if self._nx_graph.has_node(value.get(\"synonym_for\")):\n                continue\n\n            if not self._nx_graph.has_node(key):\n                msg = (\n                    f\"Node {key} not found in ontology, but also has no inheritance definition. Please check your \"\n                    \"schema for spelling errors, first letter not in lower case, use of underscores, a missing \"\n                    \"`is_a` definition (SubClassOf a root node), or missing labels in class or super-classes.\"\n                )\n                logger.error(msg)\n                raise ValueError(msg)\n\n            continue\n\n        parents = to_list(value.get(\"is_a\"))\n        child = key\n\n        while parents:\n            parent = parents.pop(0)\n\n            if parent not in self._nx_graph.nodes:\n                self._nx_graph.add_node(parent)\n                self._nx_graph.nodes[parent][\"label\"] = sentencecase_to_pascalcase(parent)\n\n                # mark parent as user extension\n                self._nx_graph.nodes[parent][\"user_extension\"] = True\n                self._extended_nodes.add(parent)\n\n            if child not in self._nx_graph.nodes:\n                self._nx_graph.add_node(child)\n                self._nx_graph.nodes[child][\"label\"] = sentencecase_to_pascalcase(child)\n\n                # mark child as user extension\n                self._nx_graph.nodes[child][\"user_extension\"] = True\n                self._extended_nodes.add(child)\n\n            self._nx_graph.add_edge(child, parent)\n\n            child = parent\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.Ontology._get_current_id","title":"<code>_get_current_id()</code>","text":"<p>Instantiate a version ID for the current session. For now does simple versioning using datetime.</p> <p>Can later implement incremental versioning, versioning from config file, or manual specification via argument.</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>def _get_current_id(self):\n    \"\"\"Instantiate a version ID for the current session. For now does simple\n    versioning using datetime.\n\n    Can later implement incremental versioning, versioning from\n    config file, or manual specification via argument.\n    \"\"\"\n    now = datetime.now()\n    return now.strftime(\"v%Y%m%d-%H%M%S\")\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.Ontology._get_head_join_node","title":"<code>_get_head_join_node(adapter)</code>","text":"<p>Tries to find the head join node of the given ontology adapter in the head ontology. If the join node is not found, the method will raise an error.</p> <pre><code>adapter (OntologyAdapter): The ontology adapter of which to find the\n    join node in the head ontology.\n</code></pre> Source code in <code>biocypher/_ontology.py</code> <pre><code>def _get_head_join_node(self, adapter: OntologyAdapter) -&gt; str:\n    \"\"\"Tries to find the head join node of the given ontology adapter in the\n    head ontology. If the join node is not found, the method will raise an\n    error.\n\n    Args:\n    ----\n        adapter (OntologyAdapter): The ontology adapter of which to find the\n            join node in the head ontology.\n\n    \"\"\"\n    head_join_node = None\n    user_defined_head_join_node_label = adapter.get_head_join_node()\n    head_join_node_label_in_bc_format = to_lower_sentence_case(user_defined_head_join_node_label.replace(\"_\", \" \"))\n\n    if self._head_ontology._switch_label_and_id:\n        head_join_node = head_join_node_label_in_bc_format\n    elif not self._head_ontology._switch_label_and_id:\n        for node_id, data in self._head_ontology.get_nx_graph().nodes(data=True):\n            if \"label\" in data and data[\"label\"] == head_join_node_label_in_bc_format:\n                head_join_node = node_id\n                break\n\n    if head_join_node not in self._head_ontology.get_nx_graph().nodes:\n        head_ontology = self._head_ontology._rdf_to_nx(\n            self._head_ontology.get_rdf_graph(),\n            self._head_ontology._root_label,\n            self._head_ontology._switch_label_and_id,\n            rename_nodes=False,\n        )\n        raise ValueError(\n            f\"Head join node '{head_join_node}' not found in head ontology. \"\n            f\"The head ontology contains the following nodes: {head_ontology.nodes}.\",\n        )\n    return head_join_node\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.Ontology._join_ontologies","title":"<code>_join_ontologies(adapter, head_join_node)</code>","text":"<p>Joins the ontologies by adding the tail ontology as a subgraph to the head ontology at the specified join nodes.</p> <pre><code>adapter (OntologyAdapter): The ontology adapter of the tail ontology\n    to be added to the head ontology.\n</code></pre> Source code in <code>biocypher/_ontology.py</code> <pre><code>def _join_ontologies(self, adapter: OntologyAdapter, head_join_node) -&gt; None:\n    \"\"\"Joins the ontologies by adding the tail ontology as a subgraph to the\n    head ontology at the specified join nodes.\n\n    Args:\n    ----\n        adapter (OntologyAdapter): The ontology adapter of the tail ontology\n            to be added to the head ontology.\n\n    \"\"\"\n    if not self._nx_graph:\n        self._nx_graph = self._head_ontology.get_nx_graph().copy()\n\n    tail_join_node = adapter.get_root_node()\n    tail_ontology = adapter.get_nx_graph()\n\n    # subtree of tail ontology at join node\n    tail_ontology_subtree = nx.dfs_tree(tail_ontology.reverse(), tail_join_node).reverse()\n\n    # transfer node attributes from tail ontology to subtree\n    for node in tail_ontology_subtree.nodes:\n        tail_ontology_subtree.nodes[node].update(tail_ontology.nodes[node])\n\n    # if merge_nodes is False, create parent of tail join node from head\n    # join node\n    if not adapter._merge_nodes:\n        # add head join node from head ontology to tail ontology subtree\n        # as parent of tail join node\n        tail_ontology_subtree.add_node(\n            head_join_node,\n            **self._head_ontology.get_nx_graph().nodes[head_join_node],\n        )\n        tail_ontology_subtree.add_edge(tail_join_node, head_join_node)\n\n    # else rename tail join node to match head join node if necessary\n    elif tail_join_node != head_join_node:\n        tail_ontology_subtree = nx.relabel_nodes(tail_ontology_subtree, {tail_join_node: head_join_node})\n\n    # combine head ontology and tail subtree\n    self._nx_graph = nx.compose(self._nx_graph, tail_ontology_subtree)\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.Ontology._load_ontologies","title":"<code>_load_ontologies()</code>","text":"<p>For each ontology, load the OntologyAdapter object and store it as an instance variable (head) or a dictionary (tail).</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>def _load_ontologies(self) -&gt; None:\n    \"\"\"For each ontology, load the OntologyAdapter object and store it as an\n    instance variable (head) or a dictionary (tail).\n    \"\"\"\n    logger.info(\"Loading ontologies...\")\n\n    self._head_ontology = OntologyAdapter(\n        ontology_file=self._head_ontology_meta[\"url\"],\n        root_label=self._head_ontology_meta[\"root_node\"],\n        ontology_file_format=self._head_ontology_meta.get(\"format\", None),\n        switch_label_and_id=self._head_ontology_meta.get(\"switch_label_and_id\", True),\n    )\n\n    if self._tail_ontology_meta:\n        self._tail_ontologies = {}\n        for key, value in self._tail_ontology_meta.items():\n            self._tail_ontologies[key] = OntologyAdapter(\n                ontology_file=value[\"url\"],\n                root_label=value[\"tail_join_node\"],\n                head_join_node_label=value[\"head_join_node\"],\n                ontology_file_format=value.get(\"format\", None),\n                merge_nodes=value.get(\"merge_nodes\", True),\n                switch_label_and_id=value.get(\"switch_label_and_id\", True),\n            )\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.Ontology._main","title":"<code>_main()</code>","text":"<p>Main method to be run on instantiation. Loads the ontologies, joins them, and returns the hybrid ontology. Loads only the head ontology if nothing else is given. Adds user extensions and properties from the mapping.</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>def _main(self) -&gt; None:\n    \"\"\"Main method to be run on instantiation. Loads the ontologies, joins\n    them, and returns the hybrid ontology. Loads only the head ontology\n    if nothing else is given. Adds user extensions and properties from\n    the mapping.\n    \"\"\"\n    self._load_ontologies()\n\n    if self._tail_ontologies:\n        for adapter in self._tail_ontologies.values():\n            head_join_node = self._get_head_join_node(adapter)\n            self._join_ontologies(adapter, head_join_node)\n    else:\n        self._nx_graph = self._head_ontology.get_nx_graph()\n\n    if self.mapping:\n        self._extend_ontology()\n\n        # experimental: add connections of disjoint classes to entity\n        # self._connect_biolink_classes()\n\n        self._add_properties()\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.Ontology.get_ancestors","title":"<code>get_ancestors(node_label)</code>","text":"<p>Get the ancestors of a node in the ontology.</p> <pre><code>node_label (str): The label of the node in the ontology.\n</code></pre> <pre><code>list: A list of the ancestors of the node.\n</code></pre> Source code in <code>biocypher/_ontology.py</code> <pre><code>def get_ancestors(self, node_label: str) -&gt; list:\n    \"\"\"Get the ancestors of a node in the ontology.\n\n    Args:\n    ----\n        node_label (str): The label of the node in the ontology.\n\n    Returns:\n    -------\n        list: A list of the ancestors of the node.\n\n    \"\"\"\n    return nx.dfs_tree(self._nx_graph, node_label)\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.Ontology.get_dict","title":"<code>get_dict()</code>","text":"<p>Returns a dictionary compatible with a BioCypher node for compatibility with the Neo4j driver.</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>def get_dict(self) -&gt; dict:\n    \"\"\"Returns a dictionary compatible with a BioCypher node for compatibility\n    with the Neo4j driver.\n    \"\"\"\n    d = {\n        \"node_id\": self._get_current_id(),\n        \"node_label\": \"BioCypher\",\n        \"properties\": {\n            \"schema\": \"self.ontology_mapping.extended_schema\",\n        },\n    }\n\n    return d\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.Ontology.show_ontology_structure","title":"<code>show_ontology_structure(to_disk=None, full=False)</code>","text":"<p>Show the ontology structure using treelib or write to GRAPHML file.</p> <pre><code>to_disk (str): If specified, the ontology structure will be saved\n    to disk as a GRAPHML file at the location (directory) specified\n    by the `to_disk` string, to be opened in your favourite graph\n    visualisation tool.\n\nfull (bool): If True, the full ontology structure will be shown,\n    including all nodes and edges. If False, only the nodes and\n    edges that are relevant to the extended schema will be shown.\n</code></pre> Source code in <code>biocypher/_ontology.py</code> <pre><code>def show_ontology_structure(self, to_disk: str = None, full: bool = False):\n    \"\"\"Show the ontology structure using treelib or write to GRAPHML file.\n\n    Args:\n    ----\n        to_disk (str): If specified, the ontology structure will be saved\n            to disk as a GRAPHML file at the location (directory) specified\n            by the `to_disk` string, to be opened in your favourite graph\n            visualisation tool.\n\n        full (bool): If True, the full ontology structure will be shown,\n            including all nodes and edges. If False, only the nodes and\n            edges that are relevant to the extended schema will be shown.\n\n    \"\"\"\n    if not full and not self.mapping.extended_schema:\n        raise ValueError(\n            \"You are attempting to visualise a subset of the loaded\"\n            \"ontology, but have not provided a schema configuration. \"\n            \"To display a partial ontology graph, please provide a schema \"\n            \"configuration file; to visualise the full graph, please use \"\n            \"the parameter `full = True`.\",\n        )\n\n    if not self._nx_graph:\n        raise ValueError(\"Ontology not loaded.\")\n\n    if not self._tail_ontologies:\n        msg = f\"Showing ontology structure based on {self._head_ontology._ontology_file}\"\n\n    else:\n        msg = f\"Showing ontology structure based on {len(self._tail_ontology_meta) + 1} ontologies: \"\n\n    logger.info(msg)\n\n    if not full:\n        # set of leaves and their intermediate parents up to the root\n        filter_nodes = set(self.mapping.extended_schema.keys())\n\n        for node in self.mapping.extended_schema.keys():\n            filter_nodes.update(self.get_ancestors(node).nodes)\n\n        # filter graph\n        G = self._nx_graph.subgraph(filter_nodes)\n\n    else:\n        G = self._nx_graph\n\n    if not to_disk:\n        # create tree\n        tree = create_tree_visualisation(G)\n\n        # add synonym information\n        for node in self.mapping.extended_schema:\n            if not isinstance(self.mapping.extended_schema[node], dict):\n                continue\n            if self.mapping.extended_schema[node].get(\"synonym_for\"):\n                tree.nodes[node].tag = f\"{node} = {self.mapping.extended_schema[node].get('synonym_for')}\"\n\n        logger.info(f\"\\n{tree}\")\n\n        return tree\n\n    else:\n        # convert lists/dicts to strings for vis only\n        for node in G.nodes:\n            # rename node and use former id as label\n            label = G.nodes[node].get(\"label\")\n\n            if not label:\n                label = node\n\n            G = nx.relabel_nodes(G, {node: label})\n            G.nodes[label][\"label\"] = node\n\n            for attrib in G.nodes[label]:\n                if type(G.nodes[label][attrib]) in [list, dict]:\n                    G.nodes[label][attrib] = str(G.nodes[label][attrib])\n\n        path = os.path.join(to_disk, \"ontology_structure.graphml\")\n\n        logger.info(f\"Writing ontology structure to {path}.\")\n\n        nx.write_graphml(G, path)\n\n        return True\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.OntologyAdapter","title":"<code>OntologyAdapter</code>","text":"<p>Class that represents an ontology to be used in the Biocypher framework. Can read from a variety of formats, including OWL, OBO, and RDF/XML. The ontology is represented by a networkx.DiGraph object; an RDFlib graph is also kept. By default, the DiGraph reverses the label and identifier of the nodes, such that the node name in the graph is the human-readable label. The edges are oriented from child to parent. Labels are formatted in lower sentence case and underscores are replaced by spaces. Identifiers are taken as defined and the prefixes are removed by default.</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>class OntologyAdapter:\n    \"\"\"Class that represents an ontology to be used in the Biocypher framework. Can\n    read from a variety of formats, including OWL, OBO, and RDF/XML. The\n    ontology is represented by a networkx.DiGraph object; an RDFlib graph is\n    also kept. By default, the DiGraph reverses the label and identifier of the\n    nodes, such that the node name in the graph is the human-readable label. The\n    edges are oriented from child to parent.\n    Labels are formatted in lower sentence case and underscores are replaced by spaces.\n    Identifiers are taken as defined and the prefixes are removed by default.\n    \"\"\"\n\n    def __init__(\n        self,\n        ontology_file: str,\n        root_label: str,\n        ontology_file_format: str | None = None,\n        head_join_node_label: str | None = None,\n        merge_nodes: bool | None = True,\n        switch_label_and_id: bool = True,\n        remove_prefixes: bool = True,\n    ):\n        \"\"\"Initialize the OntologyAdapter class.\n\n        Args:\n        ----\n            ontology_file (str): Path to the ontology file. Can be local or\n                remote.\n\n            root_label (str): The label of the root node in the ontology. In\n                case of a tail ontology, this is the tail join node.\n\n            ontology_file_format (str): The format of the ontology file (e.g. \"application/rdf+xml\")\n                If format is not passed, it is determined automatically.\n\n            head_join_node_label (str): Optional variable to store the label of the\n                node in the head ontology that should be used to join to the\n                root node of the tail ontology. Defaults to None.\n\n            merge_nodes (bool): If True, head and tail join nodes will be\n                merged, using the label of the head join node. If False, the\n                tail join node will be attached as a child of the head join\n                node.\n\n            switch_label_and_id (bool): If True, the node names in the graph will be\n                the human-readable labels. If False, the node names will be the\n                identifiers. Defaults to True.\n\n            remove_prefixes (bool): If True, the prefixes of the identifiers will\n                be removed. Defaults to True.\n\n        \"\"\"\n        logger.info(f\"Instantiating OntologyAdapter class for {ontology_file}.\")\n\n        self._ontology_file = ontology_file\n        self._root_label = root_label\n        self._format = ontology_file_format\n        self._merge_nodes = merge_nodes\n        self._head_join_node = head_join_node_label\n        self._switch_label_and_id = switch_label_and_id\n        self._remove_prefixes = remove_prefixes\n\n        self._rdf_graph = self._load_rdf_graph(ontology_file)\n\n        self._nx_graph = self._rdf_to_nx(self._rdf_graph, root_label, switch_label_and_id)\n\n    def _rdf_to_nx(\n        self,\n        _rdf_graph: rdflib.Graph,\n        root_label: str,\n        switch_label_and_id: bool,\n        rename_nodes: bool = True,\n    ) -&gt; nx.DiGraph:\n        one_to_one_triples, one_to_many_dict = self._get_relevant_rdf_triples(_rdf_graph)\n        nx_graph = self._convert_to_nx(one_to_one_triples, one_to_many_dict)\n        nx_graph = self._add_labels_to_nodes(nx_graph, switch_label_and_id)\n        nx_graph = self._change_nodes_to_biocypher_format(nx_graph, switch_label_and_id, rename_nodes)\n        nx_graph = self._get_all_ancestors(nx_graph, root_label, switch_label_and_id, rename_nodes)\n        return nx.DiGraph(nx_graph)\n\n    def _get_relevant_rdf_triples(self, g: rdflib.Graph) -&gt; tuple:\n        one_to_one_inheritance_graph = self._get_one_to_one_inheritance_triples(g)\n        intersection = self._get_multiple_inheritance_dict(g)\n        return one_to_one_inheritance_graph, intersection\n\n    def _get_one_to_one_inheritance_triples(self, g: rdflib.Graph) -&gt; rdflib.Graph:\n        \"\"\"Get the one to one inheritance triples from the RDF graph.\n\n        Args:\n        ----\n            g (rdflib.Graph): The RDF graph\n\n        Returns:\n        -------\n            rdflib.Graph: The one to one inheritance graph\n\n        \"\"\"\n        one_to_one_inheritance_graph = Graph()\n        for s, p, o in g.triples((None, rdflib.RDFS.subClassOf, None)):\n            if self.has_label(s, g):\n                one_to_one_inheritance_graph.add((s, p, o))\n        return one_to_one_inheritance_graph\n\n    def _get_multiple_inheritance_dict(self, g: rdflib.Graph) -&gt; dict:\n        \"\"\"Get the multiple inheritance dictionary from the RDF graph.\n\n        Args:\n        ----\n            g (rdflib.Graph): The RDF graph\n\n        Returns:\n        -------\n            dict: The multiple inheritance dictionary\n\n        \"\"\"\n        multiple_inheritance = g.triples((None, rdflib.OWL.intersectionOf, None))\n        intersection = {}\n        for (\n            node,\n            has_multiple_parents,\n            first_node_of_intersection_list,\n        ) in multiple_inheritance:\n            parents = self._retrieve_rdf_linked_list(first_node_of_intersection_list)\n            child_name = None\n            for s_, _, _ in g.triples((None, rdflib.RDFS.subClassOf, node)):\n                child_name = s_\n\n            # Handle Snomed CT post coordinated expressions\n            if not child_name:\n                for s_, _, _ in g.triples((None, rdflib.OWL.equivalentClass, node)):\n                    child_name = s_\n\n            if child_name:\n                intersection[node] = {\n                    \"child_name\": child_name,\n                    \"parent_node_names\": parents,\n                }\n        return intersection\n\n    def has_label(self, node: rdflib.URIRef, g: rdflib.Graph) -&gt; bool:\n        \"\"\"Does the node have a label in g?\n\n        Args:\n        ----\n            node (rdflib.URIRef): The node to check\n            g (rdflib.Graph): The graph to check in\n        Returns:\n            bool: True if the node has a label, False otherwise\n\n        \"\"\"\n        return (node, rdflib.RDFS.label, None) in g\n\n    def _retrieve_rdf_linked_list(self, subject: rdflib.URIRef) -&gt; list:\n        \"\"\"Recursively retrieves a linked list from RDF.\n        Example RDF list with the items [item1, item2]:\n        list_node - first -&gt; item1\n        list_node - rest -&gt; list_node2\n        list_node2 - first -&gt; item2\n        list_node2 - rest -&gt; nil\n        Args:\n            subject (rdflib.URIRef): One list_node of the RDF list\n        Returns:\n            list: The items of the RDF list\n        \"\"\"\n        g = self._rdf_graph\n        rdf_list = []\n        for s, p, o in g.triples((subject, rdflib.RDF.first, None)):\n            rdf_list.append(o)\n        for s, p, o in g.triples((subject, rdflib.RDF.rest, None)):\n            if o != rdflib.RDF.nil:\n                rdf_list.extend(self._retrieve_rdf_linked_list(o))\n        return rdf_list\n\n    def _convert_to_nx(self, one_to_one: rdflib.Graph, one_to_many: dict) -&gt; nx.DiGraph:\n        \"\"\"Convert the one to one and one to many inheritance graphs to networkx.\n\n        Args:\n        ----\n            one_to_one (rdflib.Graph): The one to one inheritance graph\n            one_to_many (dict): The one to many inheritance dictionary\n\n        Returns:\n        -------\n            nx.DiGraph: The networkx graph\n\n        \"\"\"\n        nx_graph = rdflib_to_networkx_digraph(one_to_one, edge_attrs=lambda s, p, o: {}, calc_weights=False)\n        for key, value in one_to_many.items():\n            nx_graph.add_edges_from([(value[\"child_name\"], parent) for parent in value[\"parent_node_names\"]])\n            if key in nx_graph.nodes:\n                nx_graph.remove_node(key)\n        return nx_graph\n\n    def _add_labels_to_nodes(self, nx_graph: nx.DiGraph, switch_label_and_id: bool) -&gt; nx.DiGraph:\n        \"\"\"Add labels to the nodes in the networkx graph.\n\n        Args:\n        ----\n            nx_graph (nx.DiGraph): The networkx graph\n            switch_label_and_id (bool): If True, id and label are switched\n\n        Returns:\n        -------\n            nx.DiGraph: The networkx graph with labels\n\n        \"\"\"\n        for node in list(nx_graph.nodes):\n            nx_id, nx_label = self._get_nx_id_and_label(node, switch_label_and_id)\n            if nx_id == \"none\":\n                # remove node if it has no id\n                nx_graph.remove_node(node)\n                continue\n\n            nx_graph.nodes[node][\"label\"] = nx_label\n        return nx_graph\n\n    def _change_nodes_to_biocypher_format(\n        self,\n        nx_graph: nx.DiGraph,\n        switch_label_and_id: bool,\n        rename_nodes: bool = True,\n    ) -&gt; nx.DiGraph:\n        \"\"\"Change the nodes in the networkx graph to BioCypher format:\n            - remove the prefix of the identifier\n            - switch id and label\n            - adapt the labels (replace _ with space and convert to lower sentence case)\n\n        Args:\n        ----\n            nx_graph (nx.DiGraph): The networkx graph\n            switch_label_and_id (bool): If True, id and label are switched\n            rename_nodes (bool): If True, the nodes are renamed\n\n        Returns:\n        -------\n            nx.DiGraph: The networkx ontology graph in BioCypher format\n\n        \"\"\"\n        mapping = {\n            node: self._get_nx_id_and_label(node, switch_label_and_id, rename_nodes)[0] for node in nx_graph.nodes\n        }\n        renamed = nx.relabel_nodes(nx_graph, mapping, copy=False)\n        return renamed\n\n    def _get_all_ancestors(\n        self,\n        renamed: nx.DiGraph,\n        root_label: str,\n        switch_label_and_id: bool,\n        rename_nodes: bool = True,\n    ) -&gt; nx.DiGraph:\n        \"\"\"Get all ancestors of the root node in the networkx graph.\n\n        Args:\n        ----\n            renamed (nx.DiGraph): The renamed networkx graph\n            root_label (str): The label of the root node in the ontology\n            switch_label_and_id (bool): If True, id and label are switched\n            rename_nodes (bool): If True, the nodes are renamed\n\n        Returns:\n        -------\n            nx.DiGraph: The filtered networkx graph\n\n        \"\"\"\n        root = self._get_nx_id_and_label(\n            self._find_root_label(self._rdf_graph, root_label),\n            switch_label_and_id,\n            rename_nodes,\n        )[0]\n        ancestors = nx.ancestors(renamed, root)\n        ancestors.add(root)\n        filtered_graph = renamed.subgraph(ancestors)\n        return filtered_graph\n\n    def _get_nx_id_and_label(self, node, switch_id_and_label: bool, rename_nodes: bool = True) -&gt; tuple[str, str]:\n        \"\"\"Rename node id and label for nx graph.\n\n        Args:\n        ----\n            node (str): The node to rename\n            switch_id_and_label (bool): If True, switch id and label\n\n        Returns:\n        -------\n            tuple[str, str]: The renamed node id and label\n\n        \"\"\"\n        node_id_str = self._remove_prefix(str(node))\n        node_label_str = str(self._rdf_graph.value(node, rdflib.RDFS.label))\n        if rename_nodes:\n            node_label_str = node_label_str.replace(\"_\", \" \")\n            node_label_str = to_lower_sentence_case(node_label_str)\n        nx_id = node_label_str if switch_id_and_label else node_id_str\n        nx_label = node_id_str if switch_id_and_label else node_label_str\n        return nx_id, nx_label\n\n    def _find_root_label(self, g, root_label):\n        # Loop through all labels in the ontology\n        for label_subject, _, label_in_ontology in g.triples((None, rdflib.RDFS.label, None)):\n            # If the label is the root label, set the root node to the label's subject\n            if str(label_in_ontology) == root_label:\n                root = label_subject\n                break\n        else:\n            labels_in_ontology = []\n            for label_subject, _, label_in_ontology in g.triples((None, rdflib.RDFS.label, None)):\n                labels_in_ontology.append(str(label_in_ontology))\n            raise ValueError(\n                f\"Could not find root node with label '{root_label}'. \"\n                f\"The ontology contains the following labels: {labels_in_ontology}\",\n            )\n        return root\n\n    def _remove_prefix(self, uri: str) -&gt; str:\n        \"\"\"Remove the prefix of a URI. URIs can contain either \"#\" or \"/\" as a\n        separator between the prefix and the local name. The prefix is\n        everything before the last separator.\n        \"\"\"\n        if self._remove_prefixes:\n            return uri.rsplit(\"#\", 1)[-1].rsplit(\"/\", 1)[-1]\n        else:\n            return uri\n\n    def _load_rdf_graph(self, ontology_file):\n        \"\"\"Load the ontology into an RDFlib graph. The ontology file can be in\n        OWL, OBO, or RDF/XML format.\n        \"\"\"\n        g = rdflib.Graph()\n        g.parse(ontology_file, format=self._get_format(ontology_file))\n        return g\n\n    def _get_format(self, ontology_file):\n        \"\"\"Get the format of the ontology file.\"\"\"\n        if self._format:\n            if self._format == \"owl\":\n                return \"application/rdf+xml\"\n            elif self._format == \"obo\":\n                raise NotImplementedError(\"OBO format not yet supported\")\n            elif self._format == \"rdf\":\n                return \"application/rdf+xml\"\n            elif self._format == \"ttl\":\n                return self._format\n            else:\n                raise ValueError(f\"Could not determine format of ontology file {ontology_file}\")\n\n        if ontology_file.endswith(\".owl\"):\n            return \"application/rdf+xml\"\n        elif ontology_file.endswith(\".obo\"):\n            raise NotImplementedError(\"OBO format not yet supported\")\n        elif ontology_file.endswith(\".rdf\"):\n            return \"application/rdf+xml\"\n        elif ontology_file.endswith(\".ttl\"):\n            return \"ttl\"\n        else:\n            raise ValueError(f\"Could not determine format of ontology file {ontology_file}\")\n\n    def get_nx_graph(self):\n        \"\"\"Get the networkx graph representing the ontology.\"\"\"\n        return self._nx_graph\n\n    def get_rdf_graph(self):\n        \"\"\"Get the RDFlib graph representing the ontology.\"\"\"\n        return self._rdf_graph\n\n    def get_root_node(self):\n        \"\"\"Get root node in the ontology.\n\n        Returns\n        -------\n            root_node: If _switch_label_and_id is True, the root node label is returned,\n                otherwise the root node id is returned.\n\n        \"\"\"\n        root_node = None\n        root_label = self._root_label.replace(\"_\", \" \")\n\n        if self._switch_label_and_id:\n            root_node = to_lower_sentence_case(root_label)\n        elif not self._switch_label_and_id:\n            for node, data in self.get_nx_graph().nodes(data=True):\n                if \"label\" in data and data[\"label\"] == to_lower_sentence_case(root_label):\n                    root_node = node\n                    break\n\n        return root_node\n\n    def get_ancestors(self, node_label):\n        \"\"\"Get the ancestors of a node in the ontology.\"\"\"\n        return nx.dfs_preorder_nodes(self._nx_graph, node_label)\n\n    def get_head_join_node(self):\n        \"\"\"Get the head join node of the ontology.\"\"\"\n        return self._head_join_node\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.OntologyAdapter.__init__","title":"<code>__init__(ontology_file, root_label, ontology_file_format=None, head_join_node_label=None, merge_nodes=True, switch_label_and_id=True, remove_prefixes=True)</code>","text":"<p>Initialize the OntologyAdapter class.</p> <pre><code>ontology_file (str): Path to the ontology file. Can be local or\n    remote.\n\nroot_label (str): The label of the root node in the ontology. In\n    case of a tail ontology, this is the tail join node.\n\nontology_file_format (str): The format of the ontology file (e.g. \"application/rdf+xml\")\n    If format is not passed, it is determined automatically.\n\nhead_join_node_label (str): Optional variable to store the label of the\n    node in the head ontology that should be used to join to the\n    root node of the tail ontology. Defaults to None.\n\nmerge_nodes (bool): If True, head and tail join nodes will be\n    merged, using the label of the head join node. If False, the\n    tail join node will be attached as a child of the head join\n    node.\n\nswitch_label_and_id (bool): If True, the node names in the graph will be\n    the human-readable labels. If False, the node names will be the\n    identifiers. Defaults to True.\n\nremove_prefixes (bool): If True, the prefixes of the identifiers will\n    be removed. Defaults to True.\n</code></pre> Source code in <code>biocypher/_ontology.py</code> <pre><code>def __init__(\n    self,\n    ontology_file: str,\n    root_label: str,\n    ontology_file_format: str | None = None,\n    head_join_node_label: str | None = None,\n    merge_nodes: bool | None = True,\n    switch_label_and_id: bool = True,\n    remove_prefixes: bool = True,\n):\n    \"\"\"Initialize the OntologyAdapter class.\n\n    Args:\n    ----\n        ontology_file (str): Path to the ontology file. Can be local or\n            remote.\n\n        root_label (str): The label of the root node in the ontology. In\n            case of a tail ontology, this is the tail join node.\n\n        ontology_file_format (str): The format of the ontology file (e.g. \"application/rdf+xml\")\n            If format is not passed, it is determined automatically.\n\n        head_join_node_label (str): Optional variable to store the label of the\n            node in the head ontology that should be used to join to the\n            root node of the tail ontology. Defaults to None.\n\n        merge_nodes (bool): If True, head and tail join nodes will be\n            merged, using the label of the head join node. If False, the\n            tail join node will be attached as a child of the head join\n            node.\n\n        switch_label_and_id (bool): If True, the node names in the graph will be\n            the human-readable labels. If False, the node names will be the\n            identifiers. Defaults to True.\n\n        remove_prefixes (bool): If True, the prefixes of the identifiers will\n            be removed. Defaults to True.\n\n    \"\"\"\n    logger.info(f\"Instantiating OntologyAdapter class for {ontology_file}.\")\n\n    self._ontology_file = ontology_file\n    self._root_label = root_label\n    self._format = ontology_file_format\n    self._merge_nodes = merge_nodes\n    self._head_join_node = head_join_node_label\n    self._switch_label_and_id = switch_label_and_id\n    self._remove_prefixes = remove_prefixes\n\n    self._rdf_graph = self._load_rdf_graph(ontology_file)\n\n    self._nx_graph = self._rdf_to_nx(self._rdf_graph, root_label, switch_label_and_id)\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.OntologyAdapter._add_labels_to_nodes","title":"<code>_add_labels_to_nodes(nx_graph, switch_label_and_id)</code>","text":"<p>Add labels to the nodes in the networkx graph.</p> <pre><code>nx_graph (nx.DiGraph): The networkx graph\nswitch_label_and_id (bool): If True, id and label are switched\n</code></pre> <pre><code>nx.DiGraph: The networkx graph with labels\n</code></pre> Source code in <code>biocypher/_ontology.py</code> <pre><code>def _add_labels_to_nodes(self, nx_graph: nx.DiGraph, switch_label_and_id: bool) -&gt; nx.DiGraph:\n    \"\"\"Add labels to the nodes in the networkx graph.\n\n    Args:\n    ----\n        nx_graph (nx.DiGraph): The networkx graph\n        switch_label_and_id (bool): If True, id and label are switched\n\n    Returns:\n    -------\n        nx.DiGraph: The networkx graph with labels\n\n    \"\"\"\n    for node in list(nx_graph.nodes):\n        nx_id, nx_label = self._get_nx_id_and_label(node, switch_label_and_id)\n        if nx_id == \"none\":\n            # remove node if it has no id\n            nx_graph.remove_node(node)\n            continue\n\n        nx_graph.nodes[node][\"label\"] = nx_label\n    return nx_graph\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.OntologyAdapter._change_nodes_to_biocypher_format","title":"<code>_change_nodes_to_biocypher_format(nx_graph, switch_label_and_id, rename_nodes=True)</code>","text":"Change the nodes in the networkx graph to BioCypher format <ul> <li>remove the prefix of the identifier</li> <li>switch id and label</li> <li>adapt the labels (replace _ with space and convert to lower sentence case)</li> </ul> <pre><code>nx_graph (nx.DiGraph): The networkx graph\nswitch_label_and_id (bool): If True, id and label are switched\nrename_nodes (bool): If True, the nodes are renamed\n</code></pre> <pre><code>nx.DiGraph: The networkx ontology graph in BioCypher format\n</code></pre> Source code in <code>biocypher/_ontology.py</code> <pre><code>def _change_nodes_to_biocypher_format(\n    self,\n    nx_graph: nx.DiGraph,\n    switch_label_and_id: bool,\n    rename_nodes: bool = True,\n) -&gt; nx.DiGraph:\n    \"\"\"Change the nodes in the networkx graph to BioCypher format:\n        - remove the prefix of the identifier\n        - switch id and label\n        - adapt the labels (replace _ with space and convert to lower sentence case)\n\n    Args:\n    ----\n        nx_graph (nx.DiGraph): The networkx graph\n        switch_label_and_id (bool): If True, id and label are switched\n        rename_nodes (bool): If True, the nodes are renamed\n\n    Returns:\n    -------\n        nx.DiGraph: The networkx ontology graph in BioCypher format\n\n    \"\"\"\n    mapping = {\n        node: self._get_nx_id_and_label(node, switch_label_and_id, rename_nodes)[0] for node in nx_graph.nodes\n    }\n    renamed = nx.relabel_nodes(nx_graph, mapping, copy=False)\n    return renamed\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.OntologyAdapter._convert_to_nx","title":"<code>_convert_to_nx(one_to_one, one_to_many)</code>","text":"<p>Convert the one to one and one to many inheritance graphs to networkx.</p> <pre><code>one_to_one (rdflib.Graph): The one to one inheritance graph\none_to_many (dict): The one to many inheritance dictionary\n</code></pre> <pre><code>nx.DiGraph: The networkx graph\n</code></pre> Source code in <code>biocypher/_ontology.py</code> <pre><code>def _convert_to_nx(self, one_to_one: rdflib.Graph, one_to_many: dict) -&gt; nx.DiGraph:\n    \"\"\"Convert the one to one and one to many inheritance graphs to networkx.\n\n    Args:\n    ----\n        one_to_one (rdflib.Graph): The one to one inheritance graph\n        one_to_many (dict): The one to many inheritance dictionary\n\n    Returns:\n    -------\n        nx.DiGraph: The networkx graph\n\n    \"\"\"\n    nx_graph = rdflib_to_networkx_digraph(one_to_one, edge_attrs=lambda s, p, o: {}, calc_weights=False)\n    for key, value in one_to_many.items():\n        nx_graph.add_edges_from([(value[\"child_name\"], parent) for parent in value[\"parent_node_names\"]])\n        if key in nx_graph.nodes:\n            nx_graph.remove_node(key)\n    return nx_graph\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.OntologyAdapter._get_all_ancestors","title":"<code>_get_all_ancestors(renamed, root_label, switch_label_and_id, rename_nodes=True)</code>","text":"<p>Get all ancestors of the root node in the networkx graph.</p> <pre><code>renamed (nx.DiGraph): The renamed networkx graph\nroot_label (str): The label of the root node in the ontology\nswitch_label_and_id (bool): If True, id and label are switched\nrename_nodes (bool): If True, the nodes are renamed\n</code></pre> <pre><code>nx.DiGraph: The filtered networkx graph\n</code></pre> Source code in <code>biocypher/_ontology.py</code> <pre><code>def _get_all_ancestors(\n    self,\n    renamed: nx.DiGraph,\n    root_label: str,\n    switch_label_and_id: bool,\n    rename_nodes: bool = True,\n) -&gt; nx.DiGraph:\n    \"\"\"Get all ancestors of the root node in the networkx graph.\n\n    Args:\n    ----\n        renamed (nx.DiGraph): The renamed networkx graph\n        root_label (str): The label of the root node in the ontology\n        switch_label_and_id (bool): If True, id and label are switched\n        rename_nodes (bool): If True, the nodes are renamed\n\n    Returns:\n    -------\n        nx.DiGraph: The filtered networkx graph\n\n    \"\"\"\n    root = self._get_nx_id_and_label(\n        self._find_root_label(self._rdf_graph, root_label),\n        switch_label_and_id,\n        rename_nodes,\n    )[0]\n    ancestors = nx.ancestors(renamed, root)\n    ancestors.add(root)\n    filtered_graph = renamed.subgraph(ancestors)\n    return filtered_graph\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.OntologyAdapter._get_format","title":"<code>_get_format(ontology_file)</code>","text":"<p>Get the format of the ontology file.</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>def _get_format(self, ontology_file):\n    \"\"\"Get the format of the ontology file.\"\"\"\n    if self._format:\n        if self._format == \"owl\":\n            return \"application/rdf+xml\"\n        elif self._format == \"obo\":\n            raise NotImplementedError(\"OBO format not yet supported\")\n        elif self._format == \"rdf\":\n            return \"application/rdf+xml\"\n        elif self._format == \"ttl\":\n            return self._format\n        else:\n            raise ValueError(f\"Could not determine format of ontology file {ontology_file}\")\n\n    if ontology_file.endswith(\".owl\"):\n        return \"application/rdf+xml\"\n    elif ontology_file.endswith(\".obo\"):\n        raise NotImplementedError(\"OBO format not yet supported\")\n    elif ontology_file.endswith(\".rdf\"):\n        return \"application/rdf+xml\"\n    elif ontology_file.endswith(\".ttl\"):\n        return \"ttl\"\n    else:\n        raise ValueError(f\"Could not determine format of ontology file {ontology_file}\")\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.OntologyAdapter._get_multiple_inheritance_dict","title":"<code>_get_multiple_inheritance_dict(g)</code>","text":"<p>Get the multiple inheritance dictionary from the RDF graph.</p> <pre><code>g (rdflib.Graph): The RDF graph\n</code></pre> <pre><code>dict: The multiple inheritance dictionary\n</code></pre> Source code in <code>biocypher/_ontology.py</code> <pre><code>def _get_multiple_inheritance_dict(self, g: rdflib.Graph) -&gt; dict:\n    \"\"\"Get the multiple inheritance dictionary from the RDF graph.\n\n    Args:\n    ----\n        g (rdflib.Graph): The RDF graph\n\n    Returns:\n    -------\n        dict: The multiple inheritance dictionary\n\n    \"\"\"\n    multiple_inheritance = g.triples((None, rdflib.OWL.intersectionOf, None))\n    intersection = {}\n    for (\n        node,\n        has_multiple_parents,\n        first_node_of_intersection_list,\n    ) in multiple_inheritance:\n        parents = self._retrieve_rdf_linked_list(first_node_of_intersection_list)\n        child_name = None\n        for s_, _, _ in g.triples((None, rdflib.RDFS.subClassOf, node)):\n            child_name = s_\n\n        # Handle Snomed CT post coordinated expressions\n        if not child_name:\n            for s_, _, _ in g.triples((None, rdflib.OWL.equivalentClass, node)):\n                child_name = s_\n\n        if child_name:\n            intersection[node] = {\n                \"child_name\": child_name,\n                \"parent_node_names\": parents,\n            }\n    return intersection\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.OntologyAdapter._get_nx_id_and_label","title":"<code>_get_nx_id_and_label(node, switch_id_and_label, rename_nodes=True)</code>","text":"<p>Rename node id and label for nx graph.</p> <pre><code>node (str): The node to rename\nswitch_id_and_label (bool): If True, switch id and label\n</code></pre> <pre><code>tuple[str, str]: The renamed node id and label\n</code></pre> Source code in <code>biocypher/_ontology.py</code> <pre><code>def _get_nx_id_and_label(self, node, switch_id_and_label: bool, rename_nodes: bool = True) -&gt; tuple[str, str]:\n    \"\"\"Rename node id and label for nx graph.\n\n    Args:\n    ----\n        node (str): The node to rename\n        switch_id_and_label (bool): If True, switch id and label\n\n    Returns:\n    -------\n        tuple[str, str]: The renamed node id and label\n\n    \"\"\"\n    node_id_str = self._remove_prefix(str(node))\n    node_label_str = str(self._rdf_graph.value(node, rdflib.RDFS.label))\n    if rename_nodes:\n        node_label_str = node_label_str.replace(\"_\", \" \")\n        node_label_str = to_lower_sentence_case(node_label_str)\n    nx_id = node_label_str if switch_id_and_label else node_id_str\n    nx_label = node_id_str if switch_id_and_label else node_label_str\n    return nx_id, nx_label\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.OntologyAdapter._get_one_to_one_inheritance_triples","title":"<code>_get_one_to_one_inheritance_triples(g)</code>","text":"<p>Get the one to one inheritance triples from the RDF graph.</p> <pre><code>g (rdflib.Graph): The RDF graph\n</code></pre> <pre><code>rdflib.Graph: The one to one inheritance graph\n</code></pre> Source code in <code>biocypher/_ontology.py</code> <pre><code>def _get_one_to_one_inheritance_triples(self, g: rdflib.Graph) -&gt; rdflib.Graph:\n    \"\"\"Get the one to one inheritance triples from the RDF graph.\n\n    Args:\n    ----\n        g (rdflib.Graph): The RDF graph\n\n    Returns:\n    -------\n        rdflib.Graph: The one to one inheritance graph\n\n    \"\"\"\n    one_to_one_inheritance_graph = Graph()\n    for s, p, o in g.triples((None, rdflib.RDFS.subClassOf, None)):\n        if self.has_label(s, g):\n            one_to_one_inheritance_graph.add((s, p, o))\n    return one_to_one_inheritance_graph\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.OntologyAdapter._load_rdf_graph","title":"<code>_load_rdf_graph(ontology_file)</code>","text":"<p>Load the ontology into an RDFlib graph. The ontology file can be in OWL, OBO, or RDF/XML format.</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>def _load_rdf_graph(self, ontology_file):\n    \"\"\"Load the ontology into an RDFlib graph. The ontology file can be in\n    OWL, OBO, or RDF/XML format.\n    \"\"\"\n    g = rdflib.Graph()\n    g.parse(ontology_file, format=self._get_format(ontology_file))\n    return g\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.OntologyAdapter._remove_prefix","title":"<code>_remove_prefix(uri)</code>","text":"<p>Remove the prefix of a URI. URIs can contain either \"#\" or \"/\" as a separator between the prefix and the local name. The prefix is everything before the last separator.</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>def _remove_prefix(self, uri: str) -&gt; str:\n    \"\"\"Remove the prefix of a URI. URIs can contain either \"#\" or \"/\" as a\n    separator between the prefix and the local name. The prefix is\n    everything before the last separator.\n    \"\"\"\n    if self._remove_prefixes:\n        return uri.rsplit(\"#\", 1)[-1].rsplit(\"/\", 1)[-1]\n    else:\n        return uri\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.OntologyAdapter._retrieve_rdf_linked_list","title":"<code>_retrieve_rdf_linked_list(subject)</code>","text":"<p>Recursively retrieves a linked list from RDF. Example RDF list with the items [item1, item2]: list_node - first -&gt; item1 list_node - rest -&gt; list_node2 list_node2 - first -&gt; item2 list_node2 - rest -&gt; nil Args:     subject (rdflib.URIRef): One list_node of the RDF list Returns:     list: The items of the RDF list</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>def _retrieve_rdf_linked_list(self, subject: rdflib.URIRef) -&gt; list:\n    \"\"\"Recursively retrieves a linked list from RDF.\n    Example RDF list with the items [item1, item2]:\n    list_node - first -&gt; item1\n    list_node - rest -&gt; list_node2\n    list_node2 - first -&gt; item2\n    list_node2 - rest -&gt; nil\n    Args:\n        subject (rdflib.URIRef): One list_node of the RDF list\n    Returns:\n        list: The items of the RDF list\n    \"\"\"\n    g = self._rdf_graph\n    rdf_list = []\n    for s, p, o in g.triples((subject, rdflib.RDF.first, None)):\n        rdf_list.append(o)\n    for s, p, o in g.triples((subject, rdflib.RDF.rest, None)):\n        if o != rdflib.RDF.nil:\n            rdf_list.extend(self._retrieve_rdf_linked_list(o))\n    return rdf_list\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.OntologyAdapter.get_ancestors","title":"<code>get_ancestors(node_label)</code>","text":"<p>Get the ancestors of a node in the ontology.</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>def get_ancestors(self, node_label):\n    \"\"\"Get the ancestors of a node in the ontology.\"\"\"\n    return nx.dfs_preorder_nodes(self._nx_graph, node_label)\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.OntologyAdapter.get_head_join_node","title":"<code>get_head_join_node()</code>","text":"<p>Get the head join node of the ontology.</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>def get_head_join_node(self):\n    \"\"\"Get the head join node of the ontology.\"\"\"\n    return self._head_join_node\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.OntologyAdapter.get_nx_graph","title":"<code>get_nx_graph()</code>","text":"<p>Get the networkx graph representing the ontology.</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>def get_nx_graph(self):\n    \"\"\"Get the networkx graph representing the ontology.\"\"\"\n    return self._nx_graph\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.OntologyAdapter.get_rdf_graph","title":"<code>get_rdf_graph()</code>","text":"<p>Get the RDFlib graph representing the ontology.</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>def get_rdf_graph(self):\n    \"\"\"Get the RDFlib graph representing the ontology.\"\"\"\n    return self._rdf_graph\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.OntologyAdapter.get_root_node","title":"<code>get_root_node()</code>","text":"<p>Get root node in the ontology.</p>"},{"location":"reference/source/#biocypher._ontology.OntologyAdapter.get_root_node--returns","title":"Returns","text":"<pre><code>root_node: If _switch_label_and_id is True, the root node label is returned,\n    otherwise the root node id is returned.\n</code></pre> Source code in <code>biocypher/_ontology.py</code> <pre><code>def get_root_node(self):\n    \"\"\"Get root node in the ontology.\n\n    Returns\n    -------\n        root_node: If _switch_label_and_id is True, the root node label is returned,\n            otherwise the root node id is returned.\n\n    \"\"\"\n    root_node = None\n    root_label = self._root_label.replace(\"_\", \" \")\n\n    if self._switch_label_and_id:\n        root_node = to_lower_sentence_case(root_label)\n    elif not self._switch_label_and_id:\n        for node, data in self.get_nx_graph().nodes(data=True):\n            if \"label\" in data and data[\"label\"] == to_lower_sentence_case(root_label):\n                root_node = node\n                break\n\n    return root_node\n</code></pre>"},{"location":"reference/source/#biocypher._ontology.OntologyAdapter.has_label","title":"<code>has_label(node, g)</code>","text":"<p>Does the node have a label in g?</p> <pre><code>node (rdflib.URIRef): The node to check\ng (rdflib.Graph): The graph to check in\n</code></pre> <p>Returns:     bool: True if the node has a label, False otherwise</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>def has_label(self, node: rdflib.URIRef, g: rdflib.Graph) -&gt; bool:\n    \"\"\"Does the node have a label in g?\n\n    Args:\n    ----\n        node (rdflib.URIRef): The node to check\n        g (rdflib.Graph): The graph to check in\n    Returns:\n        bool: True if the node has a label, False otherwise\n\n    \"\"\"\n    return (node, rdflib.RDFS.label, None) in g\n</code></pre>"},{"location":"reference/source/#_translatepy","title":"_translate.py","text":"<p>BioCypher 'translation' module. Responsible for translating between the raw input data and the BioCypherNode and BioCypherEdge objects.</p>"},{"location":"reference/source/#biocypher._translate.Translator","title":"<code>Translator</code>","text":"<p>Class responsible for exacting the translation process that is configured in the schema_config.yaml file. Creates a mapping dictionary from that file, and, given nodes and edges, translates them into BioCypherNodes and BioCypherEdges. During this process, can also filter the properties of the entities if the schema_config.yaml file specifies a property whitelist or blacklist.</p> <p>Provides utility functions for translating between input and output labels and cypher queries.</p> Source code in <code>biocypher/_translate.py</code> <pre><code>class Translator:\n    \"\"\"\n    Class responsible for exacting the translation process that is configured in\n    the schema_config.yaml file. Creates a mapping dictionary from that file,\n    and, given nodes and edges, translates them into BioCypherNodes and\n    BioCypherEdges. During this process, can also filter the properties of the\n    entities if the schema_config.yaml file specifies a property whitelist or\n    blacklist.\n\n    Provides utility functions for translating between input and output labels\n    and cypher queries.\n    \"\"\"\n\n    def __init__(self, ontology: \"Ontology\", strict_mode: bool = False):\n        \"\"\"\n        Args:\n            leaves:\n                Dictionary detailing the leaves of the hierarchy\n                tree representing the structure of the graph; the leaves are\n                the entities that will be direct components of the graph,\n                while the intermediary nodes are additional labels for\n                filtering purposes.\n            strict_mode:\n                If True, the translator will raise an error if input data do not\n                carry source, licence, and version information.\n        \"\"\"\n\n        self.ontology = ontology\n        self.strict_mode = strict_mode\n\n        # record nodes without biolink type configured in schema_config.yaml\n        self.notype = {}\n\n        # mapping functionality for translating terms and queries\n        self.mappings = {}\n        self.reverse_mappings = {}\n\n        self._update_ontology_types()\n\n    def translate_entities(self, entities):\n        entities = peekable(entities)\n        if (\n            isinstance(entities.peek(), BioCypherNode)\n            or isinstance(entities.peek(), BioCypherEdge)\n            or isinstance(entities.peek(), BioCypherRelAsNode)\n        ):\n            translated_entities = entities\n        elif len(entities.peek()) &lt; 4:\n            translated_entities = self.translate_nodes(entities)\n        else:\n            translated_entities = self.translate_edges(entities)\n        return translated_entities\n\n    def translate_nodes(\n        self,\n        node_tuples: Iterable,\n    ) -&gt; Generator[BioCypherNode, None, None]:\n        \"\"\"\n        Translates input node representation to a representation that\n        conforms to the schema of the given BioCypher graph. For now\n        requires explicit statement of node type on pass.\n\n        Args:\n            node_tuples (list of tuples): collection of tuples\n                representing individual nodes by their unique id and a type\n                that is translated from the original database notation to\n                the corresponding BioCypher notation.\n\n        \"\"\"\n\n        self._log_begin_translate(node_tuples, \"nodes\")\n\n        for _id, _type, _props in node_tuples:\n            # check for strict mode requirements\n            required_props = [\"source\", \"licence\", \"version\"]\n\n            if self.strict_mode:\n                # rename 'license' to 'licence' in _props\n                if _props.get(\"license\"):\n                    _props[\"licence\"] = _props.pop(\"license\")\n\n                for prop in required_props:\n                    if prop not in _props:\n                        raise ValueError(\n                            f\"Property `{prop}` missing from node {_id}. \"\n                            \"Strict mode is enabled, so this is not allowed.\"\n                        )\n\n            # find the node in leaves that represents ontology node type\n            _ontology_class = self._get_ontology_mapping(_type)\n\n            if _ontology_class:\n                # filter properties for those specified in schema_config if any\n                _filtered_props = self._filter_props(_ontology_class, _props)\n\n                # preferred id\n                _preferred_id = self._get_preferred_id(_ontology_class)\n\n                yield BioCypherNode(\n                    node_id=_id,\n                    node_label=_ontology_class,\n                    preferred_id=_preferred_id,\n                    properties=_filtered_props,\n                )\n\n            else:\n                self._record_no_type(_type, _id)\n\n        self._log_finish_translate(\"nodes\")\n\n    def _get_preferred_id(self, _bl_type: str) -&gt; str:\n        \"\"\"\n        Returns the preferred id for the given Biolink type.\n        \"\"\"\n\n        return (\n            self.ontology.mapping.extended_schema[_bl_type][\"preferred_id\"]\n            if \"preferred_id\" in self.ontology.mapping.extended_schema.get(_bl_type, {})\n            else \"id\"\n        )\n\n    def _filter_props(self, bl_type: str, props: dict) -&gt; dict:\n        \"\"\"\n        Filters properties for those specified in schema_config if any.\n        \"\"\"\n\n        filter_props = self.ontology.mapping.extended_schema[bl_type].get(\"properties\", {})\n\n        # strict mode: add required properties (only if there is a whitelist)\n        if self.strict_mode and filter_props:\n            filter_props.update(\n                {\"source\": \"str\", \"licence\": \"str\", \"version\": \"str\"},\n            )\n\n        exclude_props = self.ontology.mapping.extended_schema[bl_type].get(\"exclude_properties\", [])\n\n        if isinstance(exclude_props, str):\n            exclude_props = [exclude_props]\n\n        if filter_props and exclude_props:\n            filtered_props = {k: v for k, v in props.items() if (k in filter_props.keys() and k not in exclude_props)}\n\n        elif filter_props:\n            filtered_props = {k: v for k, v in props.items() if k in filter_props.keys()}\n\n        elif exclude_props:\n            filtered_props = {k: v for k, v in props.items() if k not in exclude_props}\n\n        else:\n            return props\n\n        missing_props = [k for k in filter_props.keys() if k not in filtered_props.keys()]\n        # add missing properties with default values\n        for k in missing_props:\n            filtered_props[k] = None\n\n        return filtered_props\n\n    def translate_edges(\n        self,\n        edge_tuples: Iterable,\n    ) -&gt; Generator[Union[BioCypherEdge, BioCypherRelAsNode], None, None]:\n        \"\"\"\n        Translates input edge representation to a representation that\n        conforms to the schema of the given BioCypher graph. For now\n        requires explicit statement of edge type on pass.\n\n        Args:\n\n            edge_tuples (list of tuples):\n\n                collection of tuples representing source and target of\n                an interaction via their unique ids as well as the type\n                of interaction in the original database notation, which\n                is translated to BioCypher notation using the `leaves`.\n                Can optionally possess its own ID.\n        \"\"\"\n\n        self._log_begin_translate(edge_tuples, \"edges\")\n\n        # legacy: deal with 4-tuples (no edge id)\n        # TODO remove for performance reasons once safe\n        edge_tuples = peekable(edge_tuples)\n        if len(edge_tuples.peek()) == 4:\n            edge_tuples = [(None, src, tar, typ, props) for src, tar, typ, props in edge_tuples]\n\n        for _id, _src, _tar, _type, _props in edge_tuples:\n            # check for strict mode requirements\n            if self.strict_mode:\n                if \"source\" not in _props:\n                    raise ValueError(\n                        f\"Edge {_id if _id else (_src, _tar)} does not have a `source` property.\",\n                        \" This is required in strict mode.\",\n                    )\n                if \"licence\" not in _props:\n                    raise ValueError(\n                        f\"Edge {_id if _id else (_src, _tar)} does not have a `licence` property.\",\n                        \" This is required in strict mode.\",\n                    )\n\n            # match the input label (_type) to\n            # a Biolink label from schema_config\n            bl_type = self._get_ontology_mapping(_type)\n\n            if bl_type:\n                # filter properties for those specified in schema_config if any\n                _filtered_props = self._filter_props(bl_type, _props)\n\n                rep = self.ontology.mapping.extended_schema[bl_type][\"represented_as\"]\n\n                if rep == \"node\":\n                    if _id:\n                        # if it brings its own ID, use it\n                        node_id = _id\n\n                    else:\n                        # source target concat\n                        node_id = str(_src) + \"_\" + str(_tar) + \"_\" + \"_\".join(str(v) for v in _filtered_props.values())\n\n                    n = BioCypherNode(\n                        node_id=node_id,\n                        node_label=bl_type,\n                        properties=_filtered_props,\n                    )\n\n                    # directionality check TODO generalise to account for\n                    # different descriptions of directionality or find a\n                    # more consistent solution for indicating directionality\n                    if _filtered_props.get(\"directed\") == True:  # noqa: E712 (seems to not work without '== True')\n                        l1 = \"IS_SOURCE_OF\"\n                        l2 = \"IS_TARGET_OF\"\n\n                    elif _filtered_props.get(\n                        \"src_role\",\n                    ) and _filtered_props.get(\"tar_role\"):\n                        l1 = _filtered_props.get(\"src_role\")\n                        l2 = _filtered_props.get(\"tar_role\")\n\n                    else:\n                        l1 = l2 = \"IS_PART_OF\"\n\n                    e_s = BioCypherEdge(\n                        source_id=_src,\n                        target_id=node_id,\n                        relationship_label=l1,\n                        # additional here\n                    )\n\n                    e_t = BioCypherEdge(\n                        source_id=_tar,\n                        target_id=node_id,\n                        relationship_label=l2,\n                        # additional here\n                    )\n\n                    yield BioCypherRelAsNode(n, e_s, e_t)\n\n                else:\n                    edge_label = self.ontology.mapping.extended_schema[bl_type].get(\"label_as_edge\")\n\n                    if edge_label is None:\n                        edge_label = bl_type\n\n                    yield BioCypherEdge(\n                        relationship_id=_id,\n                        source_id=_src,\n                        target_id=_tar,\n                        relationship_label=edge_label,\n                        properties=_filtered_props,\n                    )\n\n            else:\n                self._record_no_type(_type, (_src, _tar))\n\n        self._log_finish_translate(\"edges\")\n\n    def _record_no_type(self, _type: Any, what: Any) -&gt; None:\n        \"\"\"\n        Records the type of a node or edge that is not represented in the\n        schema_config.\n        \"\"\"\n\n        logger.debug(f\"No ontology type defined for `{_type}`: {what}\")\n\n        if self.notype.get(_type, None):\n            self.notype[_type] += 1\n\n        else:\n            self.notype[_type] = 1\n\n    def get_missing_biolink_types(self) -&gt; dict:\n        \"\"\"\n        Returns a dictionary of types that were not represented in the\n        schema_config.\n        \"\"\"\n\n        return self.notype\n\n    @staticmethod\n    def _log_begin_translate(_input: Iterable, what: str):\n        n = f\"{len(_input)} \" if hasattr(_input, \"__len__\") else \"\"\n\n        logger.debug(f\"Translating {n}{what} to BioCypher\")\n\n    @staticmethod\n    def _log_finish_translate(what: str):\n        logger.debug(f\"Finished translating {what} to BioCypher.\")\n\n    def _update_ontology_types(self):\n        \"\"\"\n        Creates a dictionary to translate from input labels to ontology labels.\n\n        If multiple input labels, creates mapping for each.\n        \"\"\"\n\n        self._ontology_mapping = {}\n\n        for key, value in self.ontology.mapping.extended_schema.items():\n            labels = value.get(\"input_label\") or value.get(\"label_in_input\")\n\n            if isinstance(labels, str):\n                self._ontology_mapping[labels] = key\n\n            elif isinstance(labels, list):\n                for label in labels:\n                    self._ontology_mapping[label] = key\n\n            if value.get(\"label_as_edge\"):\n                self._add_translation_mappings(labels, value[\"label_as_edge\"])\n\n            else:\n                self._add_translation_mappings(labels, key)\n\n    def _get_ontology_mapping(self, label: str) -&gt; Optional[str]:\n        \"\"\"\n        For each given input type (\"input_label\" or \"label_in_input\"), find the\n        corresponding ontology class in the leaves dictionary (from the\n        `schema_config.yam`).\n\n        Args:\n            label:\n                The input type to find (`input_label` or `label_in_input` in\n                `schema_config.yaml`).\n        \"\"\"\n\n        # commented out until behaviour of _update_bl_types is fixed\n        return self._ontology_mapping.get(label, None)\n\n    def translate_term(self, term):\n        \"\"\"\n        Translate a single term.\n        \"\"\"\n\n        return self.mappings.get(term, None)\n\n    def reverse_translate_term(self, term):\n        \"\"\"\n        Reverse translate a single term.\n        \"\"\"\n\n        return self.reverse_mappings.get(term, None)\n\n    def translate(self, query):\n        \"\"\"\n        Translate a cypher query. Only translates labels as of now.\n        \"\"\"\n        for key in self.mappings:\n            query = query.replace(\":\" + key, \":\" + self.mappings[key])\n        return query\n\n    def reverse_translate(self, query):\n        \"\"\"\n        Reverse translate a cypher query. Only translates labels as of\n        now.\n        \"\"\"\n        for key in self.reverse_mappings:\n            a = \":\" + key + \")\"\n            b = \":\" + key + \"]\"\n            # TODO this conditional probably does not cover all cases\n            if a in query or b in query:\n                if isinstance(self.reverse_mappings[key], list):\n                    raise NotImplementedError(\n                        \"Reverse translation of multiple inputs not \"\n                        \"implemented yet. Many-to-one mappings are \"\n                        \"not reversible. \"\n                        f\"({key} -&gt; {self.reverse_mappings[key]})\",\n                    )\n                else:\n                    query = query.replace(\n                        a,\n                        \":\" + self.reverse_mappings[key] + \")\",\n                    ).replace(b, \":\" + self.reverse_mappings[key] + \"]\")\n        return query\n\n    def _add_translation_mappings(self, original_name, biocypher_name):\n        \"\"\"\n        Add translation mappings for a label and name. We use here the\n        PascalCase version of the BioCypher name, since sentence case is\n        not useful for Cypher queries.\n        \"\"\"\n        if isinstance(original_name, list):\n            for on in original_name:\n                self.mappings[on] = self.name_sentence_to_pascal(\n                    biocypher_name,\n                )\n        else:\n            self.mappings[original_name] = self.name_sentence_to_pascal(\n                biocypher_name,\n            )\n\n        if isinstance(biocypher_name, list):\n            for bn in biocypher_name:\n                self.reverse_mappings[\n                    self.name_sentence_to_pascal(\n                        bn,\n                    )\n                ] = original_name\n        else:\n            self.reverse_mappings[\n                self.name_sentence_to_pascal(\n                    biocypher_name,\n                )\n            ] = original_name\n\n    @staticmethod\n    def name_sentence_to_pascal(name: str) -&gt; str:\n        \"\"\"\n        Converts a name in sentence case to pascal case.\n        \"\"\"\n        # split on dots if dot is present\n        if \".\" in name:\n            return \".\".join(\n                [_misc.sentencecase_to_pascalcase(n) for n in name.split(\".\")],\n            )\n        else:\n            return _misc.sentencecase_to_pascalcase(name)\n</code></pre>"},{"location":"reference/source/#biocypher._translate.Translator.__init__","title":"<code>__init__(ontology, strict_mode=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>leaves</code> <p>Dictionary detailing the leaves of the hierarchy tree representing the structure of the graph; the leaves are the entities that will be direct components of the graph, while the intermediary nodes are additional labels for filtering purposes.</p> required <code>strict_mode</code> <code>bool</code> <p>If True, the translator will raise an error if input data do not carry source, licence, and version information.</p> <code>False</code> Source code in <code>biocypher/_translate.py</code> <pre><code>def __init__(self, ontology: \"Ontology\", strict_mode: bool = False):\n    \"\"\"\n    Args:\n        leaves:\n            Dictionary detailing the leaves of the hierarchy\n            tree representing the structure of the graph; the leaves are\n            the entities that will be direct components of the graph,\n            while the intermediary nodes are additional labels for\n            filtering purposes.\n        strict_mode:\n            If True, the translator will raise an error if input data do not\n            carry source, licence, and version information.\n    \"\"\"\n\n    self.ontology = ontology\n    self.strict_mode = strict_mode\n\n    # record nodes without biolink type configured in schema_config.yaml\n    self.notype = {}\n\n    # mapping functionality for translating terms and queries\n    self.mappings = {}\n    self.reverse_mappings = {}\n\n    self._update_ontology_types()\n</code></pre>"},{"location":"reference/source/#biocypher._translate.Translator._add_translation_mappings","title":"<code>_add_translation_mappings(original_name, biocypher_name)</code>","text":"<p>Add translation mappings for a label and name. We use here the PascalCase version of the BioCypher name, since sentence case is not useful for Cypher queries.</p> Source code in <code>biocypher/_translate.py</code> <pre><code>def _add_translation_mappings(self, original_name, biocypher_name):\n    \"\"\"\n    Add translation mappings for a label and name. We use here the\n    PascalCase version of the BioCypher name, since sentence case is\n    not useful for Cypher queries.\n    \"\"\"\n    if isinstance(original_name, list):\n        for on in original_name:\n            self.mappings[on] = self.name_sentence_to_pascal(\n                biocypher_name,\n            )\n    else:\n        self.mappings[original_name] = self.name_sentence_to_pascal(\n            biocypher_name,\n        )\n\n    if isinstance(biocypher_name, list):\n        for bn in biocypher_name:\n            self.reverse_mappings[\n                self.name_sentence_to_pascal(\n                    bn,\n                )\n            ] = original_name\n    else:\n        self.reverse_mappings[\n            self.name_sentence_to_pascal(\n                biocypher_name,\n            )\n        ] = original_name\n</code></pre>"},{"location":"reference/source/#biocypher._translate.Translator._filter_props","title":"<code>_filter_props(bl_type, props)</code>","text":"<p>Filters properties for those specified in schema_config if any.</p> Source code in <code>biocypher/_translate.py</code> <pre><code>def _filter_props(self, bl_type: str, props: dict) -&gt; dict:\n    \"\"\"\n    Filters properties for those specified in schema_config if any.\n    \"\"\"\n\n    filter_props = self.ontology.mapping.extended_schema[bl_type].get(\"properties\", {})\n\n    # strict mode: add required properties (only if there is a whitelist)\n    if self.strict_mode and filter_props:\n        filter_props.update(\n            {\"source\": \"str\", \"licence\": \"str\", \"version\": \"str\"},\n        )\n\n    exclude_props = self.ontology.mapping.extended_schema[bl_type].get(\"exclude_properties\", [])\n\n    if isinstance(exclude_props, str):\n        exclude_props = [exclude_props]\n\n    if filter_props and exclude_props:\n        filtered_props = {k: v for k, v in props.items() if (k in filter_props.keys() and k not in exclude_props)}\n\n    elif filter_props:\n        filtered_props = {k: v for k, v in props.items() if k in filter_props.keys()}\n\n    elif exclude_props:\n        filtered_props = {k: v for k, v in props.items() if k not in exclude_props}\n\n    else:\n        return props\n\n    missing_props = [k for k in filter_props.keys() if k not in filtered_props.keys()]\n    # add missing properties with default values\n    for k in missing_props:\n        filtered_props[k] = None\n\n    return filtered_props\n</code></pre>"},{"location":"reference/source/#biocypher._translate.Translator._get_ontology_mapping","title":"<code>_get_ontology_mapping(label)</code>","text":"<p>For each given input type (\"input_label\" or \"label_in_input\"), find the corresponding ontology class in the leaves dictionary (from the <code>schema_config.yam</code>).</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>The input type to find (<code>input_label</code> or <code>label_in_input</code> in <code>schema_config.yaml</code>).</p> required Source code in <code>biocypher/_translate.py</code> <pre><code>def _get_ontology_mapping(self, label: str) -&gt; Optional[str]:\n    \"\"\"\n    For each given input type (\"input_label\" or \"label_in_input\"), find the\n    corresponding ontology class in the leaves dictionary (from the\n    `schema_config.yam`).\n\n    Args:\n        label:\n            The input type to find (`input_label` or `label_in_input` in\n            `schema_config.yaml`).\n    \"\"\"\n\n    # commented out until behaviour of _update_bl_types is fixed\n    return self._ontology_mapping.get(label, None)\n</code></pre>"},{"location":"reference/source/#biocypher._translate.Translator._get_preferred_id","title":"<code>_get_preferred_id(_bl_type)</code>","text":"<p>Returns the preferred id for the given Biolink type.</p> Source code in <code>biocypher/_translate.py</code> <pre><code>def _get_preferred_id(self, _bl_type: str) -&gt; str:\n    \"\"\"\n    Returns the preferred id for the given Biolink type.\n    \"\"\"\n\n    return (\n        self.ontology.mapping.extended_schema[_bl_type][\"preferred_id\"]\n        if \"preferred_id\" in self.ontology.mapping.extended_schema.get(_bl_type, {})\n        else \"id\"\n    )\n</code></pre>"},{"location":"reference/source/#biocypher._translate.Translator._record_no_type","title":"<code>_record_no_type(_type, what)</code>","text":"<p>Records the type of a node or edge that is not represented in the schema_config.</p> Source code in <code>biocypher/_translate.py</code> <pre><code>def _record_no_type(self, _type: Any, what: Any) -&gt; None:\n    \"\"\"\n    Records the type of a node or edge that is not represented in the\n    schema_config.\n    \"\"\"\n\n    logger.debug(f\"No ontology type defined for `{_type}`: {what}\")\n\n    if self.notype.get(_type, None):\n        self.notype[_type] += 1\n\n    else:\n        self.notype[_type] = 1\n</code></pre>"},{"location":"reference/source/#biocypher._translate.Translator._update_ontology_types","title":"<code>_update_ontology_types()</code>","text":"<p>Creates a dictionary to translate from input labels to ontology labels.</p> <p>If multiple input labels, creates mapping for each.</p> Source code in <code>biocypher/_translate.py</code> <pre><code>def _update_ontology_types(self):\n    \"\"\"\n    Creates a dictionary to translate from input labels to ontology labels.\n\n    If multiple input labels, creates mapping for each.\n    \"\"\"\n\n    self._ontology_mapping = {}\n\n    for key, value in self.ontology.mapping.extended_schema.items():\n        labels = value.get(\"input_label\") or value.get(\"label_in_input\")\n\n        if isinstance(labels, str):\n            self._ontology_mapping[labels] = key\n\n        elif isinstance(labels, list):\n            for label in labels:\n                self._ontology_mapping[label] = key\n\n        if value.get(\"label_as_edge\"):\n            self._add_translation_mappings(labels, value[\"label_as_edge\"])\n\n        else:\n            self._add_translation_mappings(labels, key)\n</code></pre>"},{"location":"reference/source/#biocypher._translate.Translator.get_missing_biolink_types","title":"<code>get_missing_biolink_types()</code>","text":"<p>Returns a dictionary of types that were not represented in the schema_config.</p> Source code in <code>biocypher/_translate.py</code> <pre><code>def get_missing_biolink_types(self) -&gt; dict:\n    \"\"\"\n    Returns a dictionary of types that were not represented in the\n    schema_config.\n    \"\"\"\n\n    return self.notype\n</code></pre>"},{"location":"reference/source/#biocypher._translate.Translator.name_sentence_to_pascal","title":"<code>name_sentence_to_pascal(name)</code>  <code>staticmethod</code>","text":"<p>Converts a name in sentence case to pascal case.</p> Source code in <code>biocypher/_translate.py</code> <pre><code>@staticmethod\ndef name_sentence_to_pascal(name: str) -&gt; str:\n    \"\"\"\n    Converts a name in sentence case to pascal case.\n    \"\"\"\n    # split on dots if dot is present\n    if \".\" in name:\n        return \".\".join(\n            [_misc.sentencecase_to_pascalcase(n) for n in name.split(\".\")],\n        )\n    else:\n        return _misc.sentencecase_to_pascalcase(name)\n</code></pre>"},{"location":"reference/source/#biocypher._translate.Translator.reverse_translate","title":"<code>reverse_translate(query)</code>","text":"<p>Reverse translate a cypher query. Only translates labels as of now.</p> Source code in <code>biocypher/_translate.py</code> <pre><code>def reverse_translate(self, query):\n    \"\"\"\n    Reverse translate a cypher query. Only translates labels as of\n    now.\n    \"\"\"\n    for key in self.reverse_mappings:\n        a = \":\" + key + \")\"\n        b = \":\" + key + \"]\"\n        # TODO this conditional probably does not cover all cases\n        if a in query or b in query:\n            if isinstance(self.reverse_mappings[key], list):\n                raise NotImplementedError(\n                    \"Reverse translation of multiple inputs not \"\n                    \"implemented yet. Many-to-one mappings are \"\n                    \"not reversible. \"\n                    f\"({key} -&gt; {self.reverse_mappings[key]})\",\n                )\n            else:\n                query = query.replace(\n                    a,\n                    \":\" + self.reverse_mappings[key] + \")\",\n                ).replace(b, \":\" + self.reverse_mappings[key] + \"]\")\n    return query\n</code></pre>"},{"location":"reference/source/#biocypher._translate.Translator.reverse_translate_term","title":"<code>reverse_translate_term(term)</code>","text":"<p>Reverse translate a single term.</p> Source code in <code>biocypher/_translate.py</code> <pre><code>def reverse_translate_term(self, term):\n    \"\"\"\n    Reverse translate a single term.\n    \"\"\"\n\n    return self.reverse_mappings.get(term, None)\n</code></pre>"},{"location":"reference/source/#biocypher._translate.Translator.translate","title":"<code>translate(query)</code>","text":"<p>Translate a cypher query. Only translates labels as of now.</p> Source code in <code>biocypher/_translate.py</code> <pre><code>def translate(self, query):\n    \"\"\"\n    Translate a cypher query. Only translates labels as of now.\n    \"\"\"\n    for key in self.mappings:\n        query = query.replace(\":\" + key, \":\" + self.mappings[key])\n    return query\n</code></pre>"},{"location":"reference/source/#biocypher._translate.Translator.translate_edges","title":"<code>translate_edges(edge_tuples)</code>","text":"<p>Translates input edge representation to a representation that conforms to the schema of the given BioCypher graph. For now requires explicit statement of edge type on pass.</p> <p>Args:</p> <pre><code>edge_tuples (list of tuples):\n\n    collection of tuples representing source and target of\n    an interaction via their unique ids as well as the type\n    of interaction in the original database notation, which\n    is translated to BioCypher notation using the `leaves`.\n    Can optionally possess its own ID.\n</code></pre> Source code in <code>biocypher/_translate.py</code> <pre><code>def translate_edges(\n    self,\n    edge_tuples: Iterable,\n) -&gt; Generator[Union[BioCypherEdge, BioCypherRelAsNode], None, None]:\n    \"\"\"\n    Translates input edge representation to a representation that\n    conforms to the schema of the given BioCypher graph. For now\n    requires explicit statement of edge type on pass.\n\n    Args:\n\n        edge_tuples (list of tuples):\n\n            collection of tuples representing source and target of\n            an interaction via their unique ids as well as the type\n            of interaction in the original database notation, which\n            is translated to BioCypher notation using the `leaves`.\n            Can optionally possess its own ID.\n    \"\"\"\n\n    self._log_begin_translate(edge_tuples, \"edges\")\n\n    # legacy: deal with 4-tuples (no edge id)\n    # TODO remove for performance reasons once safe\n    edge_tuples = peekable(edge_tuples)\n    if len(edge_tuples.peek()) == 4:\n        edge_tuples = [(None, src, tar, typ, props) for src, tar, typ, props in edge_tuples]\n\n    for _id, _src, _tar, _type, _props in edge_tuples:\n        # check for strict mode requirements\n        if self.strict_mode:\n            if \"source\" not in _props:\n                raise ValueError(\n                    f\"Edge {_id if _id else (_src, _tar)} does not have a `source` property.\",\n                    \" This is required in strict mode.\",\n                )\n            if \"licence\" not in _props:\n                raise ValueError(\n                    f\"Edge {_id if _id else (_src, _tar)} does not have a `licence` property.\",\n                    \" This is required in strict mode.\",\n                )\n\n        # match the input label (_type) to\n        # a Biolink label from schema_config\n        bl_type = self._get_ontology_mapping(_type)\n\n        if bl_type:\n            # filter properties for those specified in schema_config if any\n            _filtered_props = self._filter_props(bl_type, _props)\n\n            rep = self.ontology.mapping.extended_schema[bl_type][\"represented_as\"]\n\n            if rep == \"node\":\n                if _id:\n                    # if it brings its own ID, use it\n                    node_id = _id\n\n                else:\n                    # source target concat\n                    node_id = str(_src) + \"_\" + str(_tar) + \"_\" + \"_\".join(str(v) for v in _filtered_props.values())\n\n                n = BioCypherNode(\n                    node_id=node_id,\n                    node_label=bl_type,\n                    properties=_filtered_props,\n                )\n\n                # directionality check TODO generalise to account for\n                # different descriptions of directionality or find a\n                # more consistent solution for indicating directionality\n                if _filtered_props.get(\"directed\") == True:  # noqa: E712 (seems to not work without '== True')\n                    l1 = \"IS_SOURCE_OF\"\n                    l2 = \"IS_TARGET_OF\"\n\n                elif _filtered_props.get(\n                    \"src_role\",\n                ) and _filtered_props.get(\"tar_role\"):\n                    l1 = _filtered_props.get(\"src_role\")\n                    l2 = _filtered_props.get(\"tar_role\")\n\n                else:\n                    l1 = l2 = \"IS_PART_OF\"\n\n                e_s = BioCypherEdge(\n                    source_id=_src,\n                    target_id=node_id,\n                    relationship_label=l1,\n                    # additional here\n                )\n\n                e_t = BioCypherEdge(\n                    source_id=_tar,\n                    target_id=node_id,\n                    relationship_label=l2,\n                    # additional here\n                )\n\n                yield BioCypherRelAsNode(n, e_s, e_t)\n\n            else:\n                edge_label = self.ontology.mapping.extended_schema[bl_type].get(\"label_as_edge\")\n\n                if edge_label is None:\n                    edge_label = bl_type\n\n                yield BioCypherEdge(\n                    relationship_id=_id,\n                    source_id=_src,\n                    target_id=_tar,\n                    relationship_label=edge_label,\n                    properties=_filtered_props,\n                )\n\n        else:\n            self._record_no_type(_type, (_src, _tar))\n\n    self._log_finish_translate(\"edges\")\n</code></pre>"},{"location":"reference/source/#biocypher._translate.Translator.translate_nodes","title":"<code>translate_nodes(node_tuples)</code>","text":"<p>Translates input node representation to a representation that conforms to the schema of the given BioCypher graph. For now requires explicit statement of node type on pass.</p> <p>Parameters:</p> Name Type Description Default <code>node_tuples</code> <code>list of tuples</code> <p>collection of tuples representing individual nodes by their unique id and a type that is translated from the original database notation to the corresponding BioCypher notation.</p> required Source code in <code>biocypher/_translate.py</code> <pre><code>def translate_nodes(\n    self,\n    node_tuples: Iterable,\n) -&gt; Generator[BioCypherNode, None, None]:\n    \"\"\"\n    Translates input node representation to a representation that\n    conforms to the schema of the given BioCypher graph. For now\n    requires explicit statement of node type on pass.\n\n    Args:\n        node_tuples (list of tuples): collection of tuples\n            representing individual nodes by their unique id and a type\n            that is translated from the original database notation to\n            the corresponding BioCypher notation.\n\n    \"\"\"\n\n    self._log_begin_translate(node_tuples, \"nodes\")\n\n    for _id, _type, _props in node_tuples:\n        # check for strict mode requirements\n        required_props = [\"source\", \"licence\", \"version\"]\n\n        if self.strict_mode:\n            # rename 'license' to 'licence' in _props\n            if _props.get(\"license\"):\n                _props[\"licence\"] = _props.pop(\"license\")\n\n            for prop in required_props:\n                if prop not in _props:\n                    raise ValueError(\n                        f\"Property `{prop}` missing from node {_id}. \"\n                        \"Strict mode is enabled, so this is not allowed.\"\n                    )\n\n        # find the node in leaves that represents ontology node type\n        _ontology_class = self._get_ontology_mapping(_type)\n\n        if _ontology_class:\n            # filter properties for those specified in schema_config if any\n            _filtered_props = self._filter_props(_ontology_class, _props)\n\n            # preferred id\n            _preferred_id = self._get_preferred_id(_ontology_class)\n\n            yield BioCypherNode(\n                node_id=_id,\n                node_label=_ontology_class,\n                preferred_id=_preferred_id,\n                properties=_filtered_props,\n            )\n\n        else:\n            self._record_no_type(_type, _id)\n\n    self._log_finish_translate(\"nodes\")\n</code></pre>"},{"location":"reference/source/#biocypher._translate.Translator.translate_term","title":"<code>translate_term(term)</code>","text":"<p>Translate a single term.</p> Source code in <code>biocypher/_translate.py</code> <pre><code>def translate_term(self, term):\n    \"\"\"\n    Translate a single term.\n    \"\"\"\n\n    return self.mappings.get(term, None)\n</code></pre>"},{"location":"reference/source/#classes","title":"Classes","text":""},{"location":"reference/source/#functions","title":"Functions","text":""},{"location":"reference/source/#other-members","title":"Other Members","text":""}]}