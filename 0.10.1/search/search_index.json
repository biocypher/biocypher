{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Democratising Knowledge Graphs","text":"Building a biomedical knowledge graph often takes months or even years.         What if you could do it in just weeks or days?      <p>We created BioCypher to revolutionise the process\u2014making it easier than ever while maintaining flexibility and transparency.</p> <p>At its core, BioCypher is designed around the principle of threefold modularity:</p> <ol> <li>Modular data sources \u2013 Seamlessly integrate diverse biomedical datasets.</li> <li>Modular ontology structures \u2013 Define flexible, structured knowledge representations.</li> <li>Modular output formats \u2013 Adapt results to various applications and tools.</li> </ol> <p>This modular approach maximises flexibility and reusability, empowering the biomedical community to accelerate research while streamlining efforts.</p> <p></p> <p>Hot Topics</p> <p>Model Context Protocol has opened new avenues for accessibility. We have created a registry for biomedical MCP servers (see https://biocontext.ai) as well as a dedicated MCP server to help with implementing BioCypher solutions (see here).</p> <p>BioCypher is the simplest way to create an AI-enabled knowledge graph for biomedical (or other) tasks. Check our BioChatter documentation for more information.</p> <p>We have also recently published a perspective on connecting knowledge and machine learning to enable causal reasoning in biomedicine, with a particular focus on the currently emerging \"foundation models.\" You can read it here.</p>"},{"location":"#new-to-knowledge-graphs","title":"New to Knowledge Graphs?","text":"<p>If you\u2019re new to knowledge graphs and want to familiarise with the concepts that drive BioCypher, we recommend to read our paper (self-archived version here on Zenodo, online version at this https link)!</p>"},{"location":"#additional-resources","title":"Additional Resources","text":"<ul> <li> Questions? Join our Zulip channel</li> <li> Find us on GitHub</li> </ul>"},{"location":"installation/","title":"Installation guide","text":"<p>Before diving into developing wonderful use cases with BioCypher, we strongly recommend installing a few prerequisites to ensure a smooth experience. These prerequisites are:</p> <ol> <li>Python 3 (version &gt;= 3.10)<ul> <li>Install Python 3</li> </ul> </li> <li>Poetry (Python packaging and dependency manager)<ul> <li>Install Poetry</li> </ul> </li> <li>git (version control manager)<ul> <li>Install git</li> </ul> </li> <li>Docker (containerization technology) [optional]<ul> <li>Install Docker</li> </ul> </li> </ol> <p>Tip</p> <p>If you are missing any of those pre-requisites, please follow the installation guide in each resource before you continue.</p>"},{"location":"installation/#checking-prerequisites","title":"Checking prerequisites","text":"<p>You can verify access to these components in your terminal:</p> <ol> <li><code>python</code> version 3.10 or higher.    <pre><code>python --version\n</code></pre></li> <li><code>poetry</code> <pre><code>poetry --version\n</code></pre></li> <li><code>git</code> <pre><code>git --version\n</code></pre></li> <li><code>docker</code> <pre><code>docker --version\n</code></pre></li> </ol>"},{"location":"installation/#option-1-use-a-project-pre-configured-with-biocypher","title":"Option 1. Use a project pre-configured with BioCypher","text":"<p>The easiest way to start using BioCypher is with a pre-configured project that includes all the essential code, dependencies, environment settings, and the recommended directory structure. This setup allows you to focus solely on implementing your use case, with minimal modifications to a few existing files, depending on your needs. If this approach suits you, follow the instructions below to get started.</p> GitHub repo from template (recommended)Cloning the project template directly <p>Step  1: Go to Biocypher project template repository, click on \"Use this template\", then click on \"Create a new repository\".</p> <p></p> <p>Step 2: Complete the information on owner (GitHub username or organisation), name, description, and visibility of your repository.</p> <p></p> <p>Step 3:  Now, you can clone the repository and navigate into it. Replace user name (alternatively, GitHub organisation) and repository name with your own.</p> <pre><code>git clone https://github.com/&lt;your-github-username&gt;/&lt;your-project-name&gt;.git\ncd &lt;your-project-name&gt;\n</code></pre> <p>Step 4: Open the <code>pyproject.toml</code> file, change the following sections, and do not forget to save your changes.</p> <ul> <li><code>name</code>: replace the default project's name (<code>biocypher-project-template</code>) with the name you want to use for your project. This name does not need to be equal to the repository name (but often it is).</li> <li><code>description</code>: change the default description to a meaningful one based on your use case.</li> </ul> <p>Step 5: Install the dependencies using Poetry.</p> <pre><code>poetry install --no-root\n</code></pre> <p>Step 6: Run the script <code>create_knowledge_graph.py</code></p> <pre><code>poetry run python create_knowledge_graph.py\n</code></pre> <p>Step  1: Clone the project template repository, rename it, and navigate to the project folder.</p> <pre><code>git clone https://github.com/biocypher/project-template.git\nmv project-template my-knowledge-graph-project\ncd my-project\n</code></pre> <p>Step  2: Make the repository your own.</p> <pre><code>rm -rf .git\ngit init\ngit add .\ngit commit -m \"Initial commit\"\n# (you can add your remote repository here)\n</code></pre> <p>Step 3: Open the <code>pyproject.toml</code> file, change the following sections, and do not forget to save your changes.</p> <ul> <li><code>name</code>: replace the default project's name (<code>biocypher-project-template</code>) with the name you want to use for your project. This name does not need to be equal to the repository name (but often it is).</li> <li><code>description</code>: change the default description to a meaningful one based on your use case.</li> </ul> <p>Step 4: Install the dependencies using Poetry.</p> <pre><code>poetry install --no-root\n</code></pre> <p>Step 5: Run the script <code>create_knowledge_graph.py</code></p> <pre><code>poetry run python create_knowledge_graph.py\n</code></pre>"},{"location":"installation/#option-2-install-from-a-package-manager","title":"Option 2. Install from a Package Manager","text":"poetry (recommended)pip <p>Note: about Poetry</p> <p>Poetry is a tool for dependency management and packaging in Python. It allows you to declare the libraries your project depends on and will manage (install/update) them for you. Poetry offers a lock file to ensure reproducible environments and allows you to easily build your project for distribution. For information about the installation process, see here.</p> <pre><code># Create a new Poetry project, i.e. my-awesome-kg-project.\npoetry new &lt;name-of-the-project&gt;\n\n# Navigate into the recently created folder's project\ncd &lt;name-of-the-project&gt;\n\n# Install the BioCypher package with all the dependencies automatically\npoetry add biocypher\n</code></pre> <p>Note: Virtual environment and best practices</p> <p>To follow best practices in software engineering and prevent issues with your Python installation, we highly recommend installing packages in a separate virtual environment instead of directly in the base Python installation.</p> <ol> <li> <p>Create and activate a virtual environment. Replace <code>&lt;name-of-environment&gt;</code> with the name of the environment you desire, i.e. <code>biocypher_env</code></p> condavenv <pre><code># Create a conda environment with Python 3.10\nconda create --name &lt;name-of-environment&gt; python=3.10\n\n# Activate the new created environment\nconda activate &lt;name-of-environment&gt;\n</code></pre> <pre><code># Create a virtualenv environment\npython3 -m venv &lt;name-of-environment&gt;\n\n# Activate the new created environment\nsource ./&lt;name-of-environment&gt;/bin/activate\n</code></pre> </li> <li> <p>Install BioCypher package from <code>pip</code>. Type the following command to install BioCypher package. Note: do not forget to activate a virtual environment before do it.</p> <pre><code>pip install biocypher\n</code></pre> </li> </ol>"},{"location":"installation/#option-3-docker","title":"Option 3. Docker","text":"<p>Play with your data in Neo4j with this Docker container</p> <p>The project template includes a Docker compose workflow that allows to:</p> <ol> <li>Create an example database using BioCypher.</li> <li>Load the data into a dockerized Neo4j instance automatically.</li> </ol> <p>Once you have created your project using one of the template options (option 1), please follow the steps below:</p> <p>Step 1: Start a single detached Docker container running a Neo4j instance, which contains the knowledge graph built by BioCypher as the default Neo4j database.</p> <pre><code>docker compose up -d\n</code></pre> <p>Step 2: Open the Neo4j instance in a web browser by typing the address and port: localhost:7474.</p> <p>Authentication is deactivated by default and can be modified in the <code>docker_variables.env</code> file (in which case you need to provide the .env file to the deploy stage of the <code>docker-compose.yml</code>).</p>"},{"location":"installation/#docker-workflow","title":"Docker Workflow","text":"<p>The Docker Compose file creates three containers: build, import, and deploy. These containers share files using a Docker Volume. In the BioCypher build procedure, the <code>biocypher_docker_config.yaml</code> file is used instead of <code>biocypher_config.yaml</code>, as specified in <code>scripts/build.sh</code>.</p> <ul> <li>Containers and their functions<ul> <li>build: Installs and runs the BioCypher pipeline.</li> <li>import: installs Neo4j and executes the data import.</li> <li>deploy: deploys the Neo4j instance on localhost.</li> </ul> </li> </ul> <p>This three-stage setup strictly is not necessary for the mounting of a read-write instance of Neo4j, but is required if the purpose is to provide a read-only instance (e.g. for a web app) that is updated regularly;for an example, see the meta graph repository. The read-only setting is configured in the docker-compose.yml file (<code>NEO4J_dbms_databases_default__to__read__only: \"false\"</code>) and is deactivated by default.</p>"},{"location":"installation/#for-developers","title":"For Developers","text":"<p>If you want to directly install BioCypher, here are the steps (requires Poetry):</p> <p>Execute in bash<pre><code>git clone https://github.com/biocypher/biocypher\ncd BioCypher\npoetry install\n</code></pre> Poetry creates a virtual environment for you (starting with <code>biocypher-</code>; alternatively you can name it yourself) and installs all dependencies.</p> <p>If you want to run the tests that use a local Neo4j or PostgreSQL DBMS (database management system) instance:</p> <ul> <li> <p>Make sure that you have a Neo4j instance with the APOC plugin installed and a database named <code>test</code> running on standard bolt port <code>7687</code></p> </li> <li> <p>A PostgreSQL instance with the psql command line tool should be installed locally and running on standard port <code>5432</code></p> </li> <li> <p>Activate the virtual environment by running <code>poetry shell</code> and then run the tests by running <code>pytest</code> in the root directory of the repository with the command line argument <code>--password=&lt;your DBMS password&gt;</code>.</p> </li> </ul> <p>Once this is set up, you can go through the tutorial or use it in your project as a local dependency.</p>"},{"location":"llms/","title":"LLM Integration Guide","text":"AI coding assistants can accelerate BioCypher development.         These guides help LLMs understand BioCypher's patterns and conventions.      <p>The easiest way to integrate specific BioCypher instructions into your AI-assisted workflow is to connect your software to our dedicated MCP server at https://mcp.biocypher.org.</p> <p>Note</p> <p>This functionality is currently experimental and does not cover all BioCypher functionality yet. Use with caution.</p> <p>This section provides specialized documentation for AI coding assistants (Copilot, Cursor, Claude, etc.) to help them understand BioCypher's architecture and create high-quality code that follows our conventions.</p>"},{"location":"llms/#why-llm-specific-documentation","title":"Why LLM-Specific Documentation?","text":"<p>BioCypher follows specific patterns and conventions that may not be immediately obvious to AI assistants. These guides provide:</p> <ul> <li>Clear architectural patterns for common tasks</li> <li>Concrete examples with expected input/output formats</li> <li>Validation rules to ensure code quality</li> <li>Best practices specific to BioCypher's domain</li> </ul>"},{"location":"llms/#available-guides","title":"Available Guides","text":"<p>For AI assistants, the following <code>.txt</code> files are available in the root of this documentation:</p> <ul> <li>llms.txt - Comprehensive functionality index and quick reference</li> <li>llms-adapters.txt - Adapter creation guide with patterns and examples</li> <li>llms-example-adapter.txt - Complete working GEO adapter example</li> </ul>"},{"location":"llms/#how-to-use-these-guides","title":"How to Use These Guides","text":"<p>When working with an AI assistant on BioCypher code:</p> <ol> <li>Reference the specific guide for your task (e.g., adapter creation)</li> <li>Provide the guide content in your prompt for context</li> <li>Specify the schema configuration you're working with</li> <li>Request validation against the patterns described</li> </ol>"},{"location":"llms/#example-prompt","title":"Example Prompt","text":"<pre><code>I'm creating a BioCypher adapter for NCBI GEO data. Please follow the adapter creation guide at llms-adapters.txt.\n\nMy schema configuration defines these node types:\n- geo_sample (input_label: \"geo_sample\")\n- geo_series (input_label: \"geo_series\")\n\nAnd these edge types:\n- HAS_SAMPLE (input_label: \"HAS_SAMPLE\")\n\nPlease create an adapter that follows the 3-tuple (node_id, node_label, attributes_dict) and 5-tuple (edge_id, source_id, target_id, edge_label, attributes_dict) patterns described in the guide.\n</code></pre>"},{"location":"llms/#contributing","title":"Contributing","text":"<p>If you find patterns or conventions that would benefit from LLM documentation, please contribute by:</p> <ol> <li>Creating a new guide file following the naming convention <code>llms-*.txt</code></li> <li>Updating the main <code>llms.txt</code> index</li> <li>Adding the guide to this overview page</li> </ol>"},{"location":"llms/#related-resources","title":"Related Resources","text":"<ul> <li>BioCypher + LLMs - Integration with BioChatter</li> <li>Adapter Tutorial - Human-focused adapter tutorial</li> <li>Schema Configuration - Schema configuration reference</li> </ul>"},{"location":"biocypher-project/biochatter-integration/","title":"Connect your Knowledge Graph to Large Language Models","text":"<p>BioChatter is a Python package implementing a generic backend library for the connection of biomedical applications to conversational AI. We describe the framework in this paper. BioChatter is part of the BioCypher ecosystem, connecting natively to BioCypher knowledge graphs. Find the docs here.</p>"},{"location":"biocypher-project/biochatter-integration/#abstract","title":"Abstract","text":"<ul> <li> <p> BioChatter Preview Web Apps</p> <p> To BioChatter Web</p> </li> <li> <p> BioChatter Repository</p> <p> To BioChatter Repository</p> </li> </ul>"},{"location":"biocypher-project/design-philosophy/","title":"BioCypher design philosophy","text":"<p>At its core, BioCypher is designed around the principle of threefold modularity:</p> <ol> <li>Modular data sources \u2013 Seamlessly integrate diverse biomedical datasets.</li> <li>Modular ontology structures \u2013 Define flexible, structured knowledge representations.</li> <li>Modular output formats \u2013 Adapt results to various applications and tools.</li> </ol>"},{"location":"biocypher-project/design-philosophy/#design-principles","title":"Design Principles","text":""},{"location":"biocypher-project/design-philosophy/#1-modular-data-sources","title":"1. Modular data sources","text":""},{"location":"biocypher-project/design-philosophy/#resources","title":"Resources","text":"<p>Resources are diverse data inputs and sources that feed into the knowledge graph through \"adapters.\" A Resource could be a file, a list of files, an API request, or a list of API requests. BioCypher can download resources from a given URL, cache them, and manage their lifecycle.</p>"},{"location":"biocypher-project/design-philosophy/#adapters","title":"Adapters","text":"<p>BioCypher is a modular framework, with the main purpose of avoiding redundant maintenance work for maintainers of secondary resources and end users alike. To achieve this, we use a collection of reusable \"adapters\" for the different sources of biomedical knowledge as well as for different ontologies.</p>"},{"location":"biocypher-project/design-philosophy/#2-modular-ontology-structures","title":"2. Modular ontology structures","text":""},{"location":"biocypher-project/design-philosophy/#ontologies","title":"Ontologies","text":"<p>An ontology is a formal, hierarchical representation of knowledge within a specific domain, organizing concepts and their relationships. It structures concepts into subclasses of more general categories, such as a wardrobe being a subclass of furniture. BioCypher requires a certain amount of knowledge about ontologies and how to use them. We try to make dealing with ontologies as easy as possible, but some basic understanding is required.</p> <p>Philosophically, a lot has changed since the introduction of current-generation large language models (LLMs). For instance, LLMs bring a sophisticated world model without explicitly modelling concepts, which is in stark contrast to the modelling decisions of traditional ontologies. We need to critically re-evaluate the future role of ontologies in the modern scientific knowledge management ecosystem. They provide valuable context via the thousands of hours of human curation, but they also come with many intricacies and inconsistencies.</p> <p>Our Philosophy</p> <p>BioCypher aims to disrupt the traditional workflow to boost knowledge management into the AI era. While we hope to preserve the benefits of human curation, we also want to critically re-evaluate the role of all parts of the knowledge representation pipeline.</p>"},{"location":"biocypher-project/design-philosophy/#3-modular-output-formats","title":"3. Modular output formats","text":""},{"location":"biocypher-project/design-philosophy/#outputs","title":"Outputs","text":"<p>Initially focused on Neo4j due to OmniPath's migration, BioCypher now supports multiple output formats, including RDF, SQL, ArangoDB, CSV, PostgreSQL, SQLite, and NetworkX, specified via the dbms parameter in the <code>biocypher_config.yaml</code> file. Users can choose between online mode (manipulation of a running database) or offline mode.</p>"},{"location":"biocypher-project/design-philosophy/#configuration","title":"Configuration","text":"<p>Configuration in BioCypher involves setting up and customizing the system to meet specific needs. BioCypher provides default configuration parameters, which can be overridden by creating a <code>biocypher_config.yaml</code> file in your project's root or config directory, specifying the parameters you wish to change.</p>"},{"location":"biocypher-project/project/","title":"BioCypher Project","text":""},{"location":"biocypher-project/project/#our-mission","title":"Our Mission","text":"<p>We aim to enable access to versatile and powerful knowledge graphs for as many researchers as possible. Making biomedical knowledge \u201ctheir own\u201d is often a privilege of the companies and groups that can afford individuals or teams working on knowledge representation in a dedicated manner. With BioCypher, we aim to change that. Creating a knowledge graph should be \u201cas simple as possible, but not any simpler.\u201d To achieve this, we have developed a framework that facilitates the creation of knowledge graphs that are informed by the latest developments in the field of biomedical knowledge representation. However, to make this framework truly accessible and comprehensive, we need the input of the biomedical community. We are therefore inviting you to join us in this endeavour!</p>"},{"location":"biocypher-project/project/#our-vision","title":"Our Vision","text":"<p>The machine learning models we train are only as good as the data they are trained on. However, most developments today still rely on manually engineered and non-reproducible data processing. We envision a future where the creation of knowledge graphs is as easy as running a script, enabling researchers to build reliable knowledge representations with up-to-date information. We believe that making the knowledge representation process more agile and lifting it to the same level of attention as the process of algorithm development will lead to more robust and reliable machine learning models. We are convinced that this will be a crucial step towards the democratisation of AI in biomedicine and beyond.</p>"},{"location":"biocypher-project/r-bioc/","title":"R and Bioconductor","text":"<p>We are intermittently working on a Bioconductor package to make BioCypher functionality available to the R community. Some work in progess is available in this repository, and we are additionally working on making Bioconductor more accessible via LLM-integration in BioChatter.</p> <p>However, we currently don't have engineers with R focus in the core team. So, if you are an R developer and interested in contributing or using the package, please get in touch!</p>"},{"location":"biocypher-project/use-cases/","title":"Use Cases","text":""},{"location":"biocypher-project/use-cases/#how-researchers-are-leveraging-biocypher","title":"How Researchers Are Leveraging BioCypher","text":"<p>BioCypher has been instrumental in advancing research around Knowledge Graphs. Teams have primarily used BioCypher for:</p> <ol> <li> <p>Creation and maintenance of knowledge repositories (\"storage\"): Ensuring structured, scalable, and easily accessible data storage.</p> </li> <li> <p>Project-specific knowledge graph creation (\"analysis\"): Facilitating streamlined data integration and insightful analysis tailored to research needs.</p> </li> </ol> <p>We are excited to showcase some real-world use cases where BioCypher has made a significant impact. If BioCypher has helped your research, we\u2019d love to hear about it\u2014your use case could be the next one featured here!</p>"},{"location":"biocypher-project/use-cases/#a-knowledge-graph-for-impact-of-genomic-variation-on-function-igvf","title":"A Knowledge Graph for Impact of Genomic Variation on Function (IGVF)","text":"<ul> <li> <p>Impact of Genomic Variation on Function (IGVF)</p> <p>Description: The impact of Genomic Variation on Function (IGVF) project aims to provide a comprenhensive and integrated view of the impact of genomic variation on human health and disease.</p> <p>Resources: The BioCypher pipeline used to build the knowledge graph uses several adapters for genetics data sources; an overview is available in our meta-graph and on the GitHub Components Board (pipelines column). The pipeline boasts a Docker Compose workflow that builds the graph and the API (using tRPC), and is available on GitHub.</p> <p> To the project</p> </li> </ul> <p>Testimonial</p> <p>\"Our project, Impact of Genomic Variation on Function (IGVF, https://igvf.org), is building a massive biological knowledge graph to attempt to link human variation and disease with genomic datasets at the single-cell level. We are creating a user-facing API (and eventually UI) that will access this graph. BioCypher, which acts as an intermediary between Biolink and graph databases (we are using ArangoDB) has been instrumental in helping us design the schema and move our project forward. Specifically, it provides a framework we can use to parse the dozens of data files and formats into a Biolink-inspired schema\".</p> <ul> <li>Ben Hitz, Director of Genomics Data Resources, Project Manager ENCODE, Stanford University</li> </ul>"},{"location":"biocypher-project/use-cases/#drug-repurposing-with-crossbar","title":"Drug Repurposing with CROssBAR","text":"<ul> <li> <p>CROssBAR</p> <p>Description: CROssBAR is a biomedical data integration and representation project. CROssBAR knowledge graphs incorporate relevant genes-proteins, molecular interactions, pathways, phenotypes, diseases, as well as known/predicted drugs and bioactive compounds, and they are constructred on-the-fly based on simple non-programmatic user queries.</p> <p>Resources: Using BioCypher, CROssBAR v2 will be a flexible property graph database comprised of single input adapters for each data source. As above, you can see its current state in the meta-graph and on the GitHub Components Board (pipelines column).</p> <p> To the project</p> </li> </ul> <p>Testimonial</p> <p>\"We built CROssBAR v1 on NoSQL since property graph databases were quite new at the time and there was no framework to help us establish the system. We used an available NoSQL solution to house different layers of biological/biomedical data as independent collections. CROssBAR\u2019s \u201csmall-scale knowledge graph (KG) construction module\u201d queries each collection separately, collects the data, and merges the data points according to their mappings (which are held in the database as well, as cross-references), eliminates redundancy, queries each and every collection again with the entries retrieved in the previous step, and repeats all subsequent steps. Given that user queries can start with a single or multiple genes/proteins, compounds/drugs, diseases, phenotypes, pathways, or any combination of those, this procedure gets extremely complicated, requiring an average of 64 NoSQL queries to construct one single user-specific KG. The total number of lines of code required for this procedure alone is around 8000. This task could have been achieved significantly faster and more efficiently if we had had BioCypher five years ago\".</p> <ul> <li>Tunca Do\u011fan, Department of Computer Engineering and Artificial Intelligence Engineering, Hacettepe University and Protein Function Development Team (UniProt database), European Molecular Biology Laboratory, European Bioinformatics Institute (EMBL-EBI)</li> </ul>"},{"location":"biocypher-project/use-cases/#building-a-knowledge-graph-for-contextualised-metabolic-enzymatic-interactions","title":"Building a Knowledge Graph for Contextualised Metabolic-Enzymatic Interactions","text":"<ul> <li> <p>Metalinks</p> <p>Description: The metalinks project aims to build a knowledge graph for contextualised metabolic-enzymatic interactions.</p> <p>Resources: The BioCypher pipeline used to build the knowledge graph uses several adapters, some of which overlap with the CROssBAR project, which helps synergising maintenance efforts. An overview is available in our meta-graph and on the GitHub Components Board (pipelines column).</p> <p> To the project</p> </li> </ul> <p>Testimonial</p> <p>\"In the metalinks project, we build a knowledge graph (KG) that incorporates attributes of metabolites, proteins and their interactions to ultimately study cell-cell communication. We use two types of interaction between metabolites and proteins, I) production and degradation of metabolites by enzymes and II) interaction of metabolites with protein receptors. During the KG assembly we access multiple databases that provide information in diverse formats. BioCypher takes all of these inputs, gives them a reasonable, reproducible structure, and facilitates proper versioning. The KG produced by BioCypher can be easily contextualized to biological questions aiming for specific tissues, diseases or metabolite properties, which facilitates downstream analysis and interpretability. While spending 2.5 months to create a loose collection of scripts and directories for the initial project, I was able to obtain a structured result with BioCypher within 2 weeks.\".</p> <ul> <li>Elias Farr, Institute for Computational Biomedicine, University Hospital Heidelberg</li> </ul>"},{"location":"community/","title":"Join Us","text":"<p>Warning</p> <p>This page is redundant with the unified docs in <code>biocypher/documentation</code>. Community docs will be maintained there.</p> <p>Welcome to the BioCypher community! We follow open-source principles and encourage any sort of contribution. We communicate on GitHub, where we also organise our projects. For more informal discussions and updates, we have a Zulip channel. Everybody is welcome!</p> <ul> <li> <p> Where to Start</p> <p>If you'd like to learn how to contribute to our projects, please follow the steps outlined in the contribution guide.</p> <p> To the contribution guide</p> </li> </ul>"},{"location":"community/#our-community-zulip-channel","title":"Our Community Zulip Channel","text":"<p>Some channels are public, others can be seen after joining the Zulip community (free of charge).</p> <ul> <li><code>#announcements</code>: Stay informed about seminars, workshops, and other events.</li> <li><code>#help</code>: Ask questions and get help related to BioCypher ecosystem projects.</li> <li><code>#general</code>: You can introduce yourselves, discuss ideas and share your work.</li> <li><code>#development</code>: Exchange ideas about development of BioCypher ecosystem projects.</li> </ul> <p>Why Zulip? We chose Zulip because it is a modern, open-source alternative to Slack. It is organised by topics, which helps in keeping the discussions focused. It also is free of charge for open-source and academic projects, which means that they sponsor the cloud hosting for our channel (as for many other open-source projects) - thanks!</p> <p>Sign up here.</p>"},{"location":"community/#contributing-guidelines-github-links","title":"Contributing Guidelines GitHub Links","text":"<ul> <li> <p>Contribution guidelines</p> </li> <li> <p>Code of Conduct</p> </li> <li> <p>Developer Guide</p> </li> </ul>"},{"location":"community/biocypher-docstring-guide/","title":"BioCypher docstring guide","text":""},{"location":"community/biocypher-docstring-guide/#about-docstrings-and-standards","title":"About docstrings and standards","text":"<p>A Python docstring is a string used to document a Python module, class, function or method, so programmers can understand what it does without having to read the details of the implementation.</p> <p>Also, it is a common practice to generate online (html) documentation automatically from docstrings. <code>Sphinx &lt;https://www.sphinx-doc.org&gt;</code>_ serves this purpose.</p> <p>The next example gives an idea of what a docstring looks like:</p> <p>.. code-block:: python</p> <pre><code>def add(num1, num2):\n    \"\"\"\n    Add up two integer numbers.\n\n    This function simply wraps the ``+`` operator, and does not\n    do anything interesting, except for illustrating what\n    the docstring of a very simple function looks like.\n\n    Parameters\n    ----------\n    num1 : int\n        First number to add.\n    num2 : int\n        Second number to add.\n\n    Returns\n    -------\n    int\n        The sum of ``num1`` and ``num2``.\n\n    See Also\n    --------\n    subtract : Subtract one integer from another.\n\n    Examples\n    --------\n    &gt;&gt;&gt; add(2, 2)\n    4\n    &gt;&gt;&gt; add(25, 0)\n    25\n    &gt;&gt;&gt; add(10, -10)\n    0\n    \"\"\"\n    return num1 + num2\n</code></pre> <p>Some standards regarding docstrings exist, which make them easier to read, and allow them be easily exported to other formats such as html or pdf.</p> <p>The first conventions every Python docstring should follow are defined in <code>PEP-257 &lt;https://www.python.org/dev/peps/pep-0257/&gt;</code>_.</p> <p>As PEP-257 is quite broad, other more specific standards also exist. In the case of biocypher, the NumPy docstring convention is followed. These conventions are explained in this document:</p> <ul> <li><code>numpydoc docstring guide &lt;https://numpydoc.readthedocs.io/en/latest/format.html&gt;</code>_</li> </ul> <p>numpydoc is a Sphinx extension to support the NumPy docstring convention.</p> <p>The standard uses reStructuredText (reST). reStructuredText is a markup language that allows encoding styles in plain text files. Documentation about reStructuredText can be found in:</p> <ul> <li><code>Sphinx reStructuredText primer &lt;https://www.sphinx-doc.org/en/stable/rest.html&gt;</code>_</li> <li><code>Quick reStructuredText reference &lt;https://docutils.sourceforge.io/docs/user/rst/quickref.html&gt;</code>_</li> <li><code>Full reStructuredText specification &lt;https://docutils.sourceforge.io/docs/ref/rst/restructuredtext.html&gt;</code>_</li> </ul> <p>biocypher has some helpers for sharing docstrings between related classes, see :ref:<code>docstring.sharing</code>.</p> <p>The rest of this document will summarize all the above guidelines, and will provide additional conventions specific to the biocypher project.</p> <p>.. _docstring.tutorial:</p>"},{"location":"community/biocypher-docstring-guide/#writing-a-docstring","title":"Writing a docstring","text":"<p>.. _docstring.general:</p> <p>General rules ~~~~~~~~~~~~~</p> <p>Docstrings must be defined with three double-quotes. No blank lines should be left before or after the docstring. The text starts in the next line after the opening quotes. The closing quotes have their own line (meaning that they are not at the end of the last sentence).</p> <p>On rare occasions reST styles like bold text or italics will be used in docstrings, but is it common to have inline code, which is presented between backticks. The following are considered inline code:</p> <ul> <li>The name of a parameter</li> <li>Python code, a module, function, built-in, type, literal... (e.g. <code>os</code>,   <code>list</code>, <code>numpy.abs</code>, <code>datetime.date</code>, <code>True</code>)</li> <li>A biocypher class (in the form <code>`:class:</code>biocypher.Series```)</li> <li>A biocypher method (in the form <code>`:meth:</code>biocypher.Series.sum```)</li> <li>A biocypher function (in the form <code>`:func:</code>biocypher.to_datetime```)</li> </ul> <p>.. note::     To display only the last component of the linked class, method or     function, prefix it with <code>~</code>. For example, <code>:class:`~biocypher.Series```     will link to</code>biocypher.Series<code>but only display the last part,</code>Series<code>`     as the link text. See</code>Sphinx cross-referencing syntax     https://www.sphinx-doc.org/en/stable/domains.html#cross-referencing-syntax`_     for details.</p> <p>Good:</p> <p>.. code-block:: python</p> <pre><code>def add_values(arr):\n    \"\"\"\n    Add the values in ``arr``.\n\n    This is equivalent to Python ``sum`` of :meth:`biocypher.Series.sum`.\n\n    Some sections are omitted here for simplicity.\n    \"\"\"\n    return sum(arr)\n</code></pre> <p>Bad:</p> <p>.. code-block:: python</p> <pre><code>def func():\n\n    \"\"\"Some function.\n\n    With several mistakes in the docstring.\n\n    It has a blank like after the signature ``def func():``.\n\n    The text 'Some function' should go in the line after the\n    opening quotes of the docstring, not in the same line.\n\n    There is a blank line between the docstring and the first line\n    of code ``foo = 1``.\n\n    The closing quotes should be in the next line, not in this one.\"\"\"\n\n    foo = 1\n    bar = 2\n    return foo + bar\n</code></pre> <p>.. _docstring.short_summary:</p> <p>Section 1: short summary ~~~~~~~~~~~~~~~~~~~~~~~~</p> <p>The short summary is a single sentence that expresses what the function does in a concise way.</p> <p>The short summary must start with a capital letter, end with a dot, and fit in a single line. It needs to express what the object does without providing details. For functions and methods, the short summary must start with an infinitive verb.</p> <p>Good:</p> <p>.. code-block:: python</p> <pre><code>def astype(dtype):\n    \"\"\"\n    Cast Series type.\n\n    This section will provide further details.\n    \"\"\"\n    pass\n</code></pre> <p>Bad:</p> <p>.. code-block:: python</p> <pre><code>def astype(dtype):\n    \"\"\"\n    Casts Series type.\n\n    Verb in third-person of the present simple, should be infinitive.\n    \"\"\"\n    pass\n</code></pre> <p>.. code-block:: python</p> <pre><code>def astype(dtype):\n    \"\"\"\n    Method to cast Series type.\n\n    Does not start with verb.\n    \"\"\"\n    pass\n</code></pre> <p>.. code-block:: python</p> <pre><code>def astype(dtype):\n    \"\"\"\n    Cast Series type\n\n    Missing dot at the end.\n    \"\"\"\n    pass\n</code></pre> <p>.. code-block:: python</p> <pre><code>def astype(dtype):\n    \"\"\"\n    Cast Series type from its current type to the new type defined in\n    the parameter dtype.\n\n    Summary is too verbose and doesn't fit in a single line.\n    \"\"\"\n    pass\n</code></pre> <p>.. _docstring.extended_summary:</p> <p>Section 2: extended summary ~~~~~~~~~~~~~~~~~~~~~~~~~~~</p> <p>The extended summary provides details on what the function does. It should not go into the details of the parameters, or discuss implementation notes, which go in other sections.</p> <p>A blank line is left between the short summary and the extended summary. Every paragraph in the extended summary ends with a dot.</p> <p>The extended summary should provide details on why the function is useful and their use cases, if it is not too generic.</p> <p>.. code-block:: python</p> <pre><code>def unstack():\n    \"\"\"\n    Pivot a row index to columns.\n\n    When using a MultiIndex, a level can be pivoted so each value in\n    the index becomes a column. This is especially useful when a subindex\n    is repeated for the main index, and data is easier to visualize as a\n    pivot table.\n\n    The index level will be automatically removed from the index when added\n    as columns.\n    \"\"\"\n    pass\n</code></pre> <p>.. _docstring.parameters:</p> <p>Section 3: parameters ~~~~~~~~~~~~~~~~~~~~~</p> <p>The details of the parameters will be added in this section. This section has the title \"Parameters\", followed by a line with a hyphen under each letter of the word \"Parameters\". A blank line is left before the section title, but not after, and not between the line with the word \"Parameters\" and the one with the hyphens.</p> <p>After the title, each parameter in the signature must be documented, including <code>*args</code> and <code>**kwargs</code>, but not <code>self</code>.</p> <p>The parameters are defined by their name, followed by a space, a colon, another space, and the type (or types). Note that the space between the name and the colon is important. Types are not defined for <code>*args</code> and <code>**kwargs</code>, but must be defined for all other parameters. After the parameter definition, it is required to have a line with the parameter description, which is indented, and can have multiple lines. The description must start with a capital letter, and finish with a dot.</p> <p>For keyword arguments with a default value, the default will be listed after a comma at the end of the type. The exact form of the type in this case will be \"int, default 0\". In some cases it may be useful to explain what the default argument means, which can be added after a comma \"int, default -1, meaning all cpus\".</p> <p>In cases where the default value is <code>None</code>, meaning that the value will not be used. Instead of <code>\"str, default None\"</code>, it is preferred to write <code>\"str, optional\"</code>. When <code>None</code> is a value being used, we will keep the form \"str, default None\". For example, in <code>df.to_csv(compression=None)</code>, <code>None</code> is not a value being used, but means that compression is optional, and no compression is being used if not provided. In this case we will use <code>\"str, optional\"</code>. Only in cases like <code>func(value=None)</code> and <code>None</code> is being used in the same way as <code>0</code> or <code>foo</code> would be used, then we will specify \"str, int or None, default None\".</p> <p>Good:</p> <p>.. code-block:: python</p> <pre><code>class Series:\n    def plot(self, kind, color='blue', **kwargs):\n        \"\"\"\n        Generate a plot.\n\n        Render the data in the Series as a matplotlib plot of the\n        specified kind.\n\n        Parameters\n        ----------\n        kind : str\n            Kind of matplotlib plot.\n        color : str, default 'blue'\n            Color name or rgb code.\n        **kwargs\n            These parameters will be passed to the matplotlib plotting\n            function.\n        \"\"\"\n        pass\n</code></pre> <p>Bad:</p> <p>.. code-block:: python</p> <pre><code>class Series:\n    def plot(self, kind, **kwargs):\n        \"\"\"\n        Generate a plot.\n\n        Render the data in the Series as a matplotlib plot of the\n        specified kind.\n\n        Note the blank line between the parameters title and the first\n        parameter. Also, note that after the name of the parameter ``kind``\n        and before the colon, a space is missing.\n\n        Also, note that the parameter descriptions do not start with a\n        capital letter, and do not finish with a dot.\n\n        Finally, the ``**kwargs`` parameter is missing.\n\n        Parameters\n        ----------\n\n        kind: str\n            kind of matplotlib plot\n        \"\"\"\n        pass\n</code></pre> <p>.. _docstring.parameter_types:</p> <p>Parameter types ^^^^^^^^^^^^^^^</p> <p>When specifying the parameter types, Python built-in data types can be used directly (the Python type is preferred to the more verbose string, integer, boolean, etc):</p> <ul> <li>int</li> <li>float</li> <li>str</li> <li>bool</li> </ul> <p>For complex types, define the subtypes. For <code>dict</code> and <code>tuple</code>, as more than one type is present, we use the brackets to help read the type (curly brackets for <code>dict</code> and normal brackets for <code>tuple</code>):</p> <ul> <li>list of int</li> <li>dict of {str : int}</li> <li>tuple of (str, int, int)</li> <li>tuple of (str,)</li> <li>set of str</li> </ul> <p>In case where there are just a set of values allowed, list them in curly brackets and separated by commas (followed by a space). If the values are ordinal and they have an order, list them in this order. Otherwise, list the default value first, if there is one:</p> <ul> <li>{0, 10, 25}</li> <li>{'simple', 'advanced'}</li> <li>{'low', 'medium', 'high'}</li> <li>{'cat', 'dog', 'bird'}</li> </ul> <p>If the type is defined in a Python module, the module must be specified:</p> <ul> <li>datetime.date</li> <li>datetime.datetime</li> <li>decimal.Decimal</li> </ul> <p>If the type is in a package, the module must be also specified:</p> <ul> <li>numpy.ndarray</li> <li>scipy.sparse.coo_matrix</li> </ul> <p>If the type is a biocypher type, also specify biocypher except for Series and DataFrame:</p> <ul> <li>Series</li> <li>DataFrame</li> <li>biocypher.Index</li> <li>biocypher.Categorical</li> <li>biocypher.arrays.SparseArray</li> </ul> <p>If the exact type is not relevant, but must be compatible with a NumPy array, array-like can be specified. If Any type that can be iterated is accepted, iterable can be used:</p> <ul> <li>array-like</li> <li>iterable</li> </ul> <p>If more than one type is accepted, separate them by commas, except the last two types, that need to be separated by the word 'or':</p> <ul> <li>int or float</li> <li>float, decimal.Decimal or None</li> <li>str or list of str</li> </ul> <p>If <code>None</code> is one of the accepted values, it always needs to be the last in the list.</p> <p>For axis, the convention is to use something like:</p> <ul> <li>axis : {0 or 'index', 1 or 'columns', None}, default None</li> </ul> <p>.. _docstring.returns:</p> <p>Section 4: returns or yields ~~~~~~~~~~~~~~~~~~~~~~~~~~~~</p> <p>If the method returns a value, it will be documented in this section. Also if the method yields its output.</p> <p>The title of the section will be defined in the same way as the \"Parameters\". With the names \"Returns\" or \"Yields\" followed by a line with as many hyphens as the letters in the preceding word.</p> <p>The documentation of the return is also similar to the parameters. But in this case, no name will be provided, unless the method returns or yields more than one value (a tuple of values).</p> <p>The types for \"Returns\" and \"Yields\" are the same as the ones for the \"Parameters\". Also, the description must finish with a dot.</p> <p>For example, with a single value:</p> <p>.. code-block:: python</p> <pre><code>def sample():\n    \"\"\"\n    Generate and return a random number.\n\n    The value is sampled from a continuous uniform distribution between\n    0 and 1.\n\n    Returns\n    -------\n    float\n        Random number generated.\n    \"\"\"\n    return np.random.random()\n</code></pre> <p>With more than one value:</p> <p>.. code-block:: python</p> <pre><code>import string\n\ndef random_letters():\n    \"\"\"\n    Generate and return a sequence of random letters.\n\n    The length of the returned string is also random, and is also\n    returned.\n\n    Returns\n    -------\n    length : int\n        Length of the returned string.\n    letters : str\n        String of random letters.\n    \"\"\"\n    length = np.random.randint(1, 10)\n    letters = ''.join(np.random.choice(string.ascii_lowercase)\n                      for i in range(length))\n    return length, letters\n</code></pre> <p>If the method yields its value:</p> <p>.. code-block:: python</p> <pre><code>def sample_values():\n    \"\"\"\n    Generate an infinite sequence of random numbers.\n\n    The values are sampled from a continuous uniform distribution between\n    0 and 1.\n\n    Yields\n    ------\n    float\n        Random number generated.\n    \"\"\"\n    while True:\n        yield np.random.random()\n</code></pre> <p>.. _docstring.see_also:</p> <p>Section 5: see also ~~~~~~~~~~~~~~~~~~~</p> <p>This section is used to let users know about biocypher functionality related to the one being documented. In rare cases, if no related methods or functions can be found at all, this section can be skipped.</p> <p>An obvious example would be the <code>head()</code> and <code>tail()</code> methods. As <code>tail()</code> does the equivalent as <code>head()</code> but at the end of the <code>Series</code> or <code>DataFrame</code> instead of at the beginning, it is good to let the users know about it.</p> <p>To give an intuition on what can be considered related, here there are some examples:</p> <ul> <li><code>loc</code> and <code>iloc</code>, as they do the same, but in one case providing indices   and in the other positions</li> <li><code>max</code> and <code>min</code>, as they do the opposite</li> <li><code>iterrows</code>, <code>itertuples</code> and <code>items</code>, as it is easy that a user   looking for the method to iterate over columns ends up in the method to   iterate over rows, and vice-versa</li> <li><code>fillna</code> and <code>dropna</code>, as both methods are used to handle missing values</li> <li><code>read_csv</code> and <code>to_csv</code>, as they are complementary</li> <li><code>merge</code> and <code>join</code>, as one is a generalization of the other</li> <li><code>astype</code> and <code>biocypher.to_datetime</code>, as users may be reading the   documentation of <code>astype</code> to know how to cast as a date, and the way to do   it is with <code>biocypher.to_datetime</code></li> <li><code>where</code> is related to <code>numpy.where</code>, as its functionality is based on it</li> </ul> <p>When deciding what is related, you should mainly use your common sense and think about what can be useful for the users reading the documentation, especially the less experienced ones.</p> <p>When relating to other libraries (mainly <code>numpy</code>), use the name of the module first (not an alias like <code>np</code>). If the function is in a module which is not the main one, like <code>scipy.sparse</code>, list the full module (e.g. <code>scipy.sparse.coo_matrix</code>).</p> <p>This section has a header, \"See Also\" (note the capital S and A), followed by the line with hyphens and preceded by a blank line.</p> <p>After the header, we will add a line for each related method or function, followed by a space, a colon, another space, and a short description that illustrates what this method or function does, why is it relevant in this context, and what the key differences are between the documented function and the one being referenced. The description must also end with a dot.</p> <p>Note that in \"Returns\" and \"Yields\", the description is located on the line after the type. In this section, however, it is located on the same line, with a colon in between. If the description does not fit on the same line, it can continue onto other lines which must be further indented.</p> <p>For example:</p> <p>.. code-block:: python</p> <pre><code>class Series:\n    def head(self):\n        \"\"\"\n        Return the first 5 elements of the Series.\n\n        This function is mainly useful to preview the values of the\n        Series without displaying the whole of it.\n\n        Returns\n        -------\n        Series\n            Subset of the original series with the 5 first values.\n\n        See Also\n        --------\n        Series.tail : Return the last 5 elements of the Series.\n        Series.iloc : Return a slice of the elements in the Series,\n            which can also be used to return the first or last n.\n        \"\"\"\n        return self.iloc[:5]\n</code></pre> <p>.. _docstring.notes:</p> <p>Section 6: notes ~~~~~~~~~~~~~~~~</p> <p>This is an optional section used for notes about the implementation of the algorithm, or to document technical aspects of the function behavior.</p> <p>Feel free to skip it, unless you are familiar with the implementation of the algorithm, or you discover some counter-intuitive behavior while writing the examples for the function.</p> <p>This section follows the same format as the extended summary section.</p> <p>.. _docstring.examples:</p> <p>Section 7: examples ~~~~~~~~~~~~~~~~~~~</p> <p>This is one of the most important sections of a docstring, despite being placed in the last position, as often people understand concepts better by example than through accurate explanations.</p> <p>Examples in docstrings, besides illustrating the usage of the function or method, must be valid Python code, that returns the given output in a deterministic way, and that can be copied and run by users.</p> <p>Examples are presented as a session in the Python terminal. <code>&gt;&gt;&gt;</code> is used to present code. <code>...</code> is used for code continuing from the previous line. Output is presented immediately after the last line of code generating the output (no blank lines in between). Comments describing the examples can be added with blank lines before and after them.</p> <p>The way to present examples is as follows:</p> <ol> <li> <p>Import required libraries (except <code>numpy</code> and <code>biocypher</code>)</p> </li> <li> <p>Create the data required for the example</p> </li> <li> <p>Show a very basic example that gives an idea of the most common use case</p> </li> <li> <p>Add examples with explanations that illustrate how the parameters can be    used for extended functionality</p> </li> </ol> <p>A simple example could be:</p> <p>.. code-block:: python</p> <pre><code>class Series:\n\n    def head(self, n=5):\n        \"\"\"\n        Return the first elements of the Series.\n\n        This function is mainly useful to preview the values of the\n        Series without displaying all of it.\n\n        Parameters\n        ----------\n        n : int\n            Number of values to return.\n\n        Return\n        ------\n        biocypher.Series\n            Subset of the original series with the n first values.\n\n        See Also\n        --------\n        tail : Return the last n elements of the Series.\n\n        Examples\n        --------\n        &gt;&gt;&gt; ser = pd.Series(['Ant', 'Bear', 'Cow', 'Dog', 'Falcon',\n        ...                'Lion', 'Monkey', 'Rabbit', 'Zebra'])\n        &gt;&gt;&gt; ser.head()\n        0   Ant\n        1   Bear\n        2   Cow\n        3   Dog\n        4   Falcon\n        dtype: object\n\n        With the ``n`` parameter, we can change the number of returned rows:\n\n        &gt;&gt;&gt; ser.head(n=3)\n        0   Ant\n        1   Bear\n        2   Cow\n        dtype: object\n        \"\"\"\n        return self.iloc[:n]\n</code></pre> <p>The examples should be as concise as possible. In cases where the complexity of the function requires long examples, is recommended to use blocks with headers in bold. Use double star <code>**</code> to make a text bold, like in <code>**this example**</code>.</p> <p>.. _docstring.example_conventions:</p> <p>Conventions for the examples ^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p> <p>Code in examples is assumed to always start with these two lines which are not shown:</p> <p>.. code-block:: python</p> <pre><code>import numpy as np\nimport biocypher as pd\n</code></pre> <p>Any other module used in the examples must be explicitly imported, one per line (as recommended in :pep:<code>8#imports</code>) and avoiding aliases. Avoid excessive imports, but if needed, imports from the standard library go first, followed by third-party libraries (like matplotlib).</p> <p>When illustrating examples with a single <code>Series</code> use the name <code>ser</code>, and if illustrating with a single <code>DataFrame</code> use the name <code>df</code>. For indices, <code>idx</code> is the preferred name. If a set of homogeneous <code>Series</code> or <code>DataFrame</code> is used, name them <code>ser1</code>, <code>ser2</code>, <code>ser3</code>...  or <code>df1</code>, <code>df2</code>, <code>df3</code>... If the data is not homogeneous, and more than one structure is needed, name them with something meaningful, for example <code>df_main</code> and <code>df_to_join</code>.</p> <p>Data used in the example should be as compact as possible. The number of rows is recommended to be around 4, but make it a number that makes sense for the specific example. For example in the <code>head</code> method, it requires to be higher than 5, to show the example with the default values. If doing the <code>mean</code>, we could use something like <code>[1, 2, 3]</code>, so it is easy to see that the value returned is the mean.</p> <p>For more complex examples (grouping for example), avoid using data without interpretation, like a matrix of random numbers with columns A, B, C, D... And instead use a meaningful example, which makes it easier to understand the concept. Unless required by the example, use names of animals, to keep examples consistent. And numerical properties of them.</p> <p>When calling the method, keywords arguments <code>head(n=3)</code> are preferred to positional arguments <code>head(3)</code>.</p> <p>Good:</p> <p>.. code-block:: python</p> <pre><code>class Series:\n\n    def mean(self):\n        \"\"\"\n        Compute the mean of the input.\n\n        Examples\n        --------\n        &gt;&gt;&gt; ser = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; ser.mean()\n        2\n        \"\"\"\n        pass\n\n\n    def fillna(self, value):\n        \"\"\"\n        Replace missing values by ``value``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; ser = pd.Series([1, np.nan, 3])\n        &gt;&gt;&gt; ser.fillna(0)\n        [1, 0, 3]\n        \"\"\"\n        pass\n\n    def groupby_mean(self):\n        \"\"\"\n        Group by index and return mean.\n\n        Examples\n        --------\n        &gt;&gt;&gt; ser = pd.Series([380., 370., 24., 26],\n        ...               name='max_speed',\n        ...               index=['falcon', 'falcon', 'parrot', 'parrot'])\n        &gt;&gt;&gt; ser.groupby_mean()\n        index\n        falcon    375.0\n        parrot     25.0\n        Name: max_speed, dtype: float64\n        \"\"\"\n        pass\n\n    def contains(self, pattern, case_sensitive=True, na=numpy.nan):\n        \"\"\"\n        Return whether each value contains ``pattern``.\n\n        In this case, we are illustrating how to use sections, even\n        if the example is simple enough and does not require them.\n\n        Examples\n        --------\n        &gt;&gt;&gt; ser = pd.Series('Antelope', 'Lion', 'Zebra', np.nan)\n        &gt;&gt;&gt; ser.contains(pattern='a')\n        0    False\n        1    False\n        2     True\n        3      NaN\n        dtype: bool\n\n        **Case sensitivity**\n\n        With ``case_sensitive`` set to ``False`` we can match ``a`` with both\n        ``a`` and ``A``:\n\n        &gt;&gt;&gt; s.contains(pattern='a', case_sensitive=False)\n        0     True\n        1    False\n        2     True\n        3      NaN\n        dtype: bool\n\n        **Missing values**\n\n        We can fill missing values in the output using the ``na`` parameter:\n\n        &gt;&gt;&gt; ser.contains(pattern='a', na=False)\n        0    False\n        1    False\n        2     True\n        3    False\n        dtype: bool\n        \"\"\"\n        pass\n</code></pre> <p>Bad:</p> <p>.. code-block:: python</p> <pre><code>def method(foo=None, bar=None):\n    \"\"\"\n    A sample DataFrame method.\n\n    Do not import NumPy and biocypher.\n\n    Try to use meaningful data, when it makes the example easier\n    to understand.\n\n    Try to avoid positional arguments like in ``df.method(1)``. They\n    can be all right if previously defined with a meaningful name,\n    like in ``present_value(interest_rate)``, but avoid them otherwise.\n\n    When presenting the behavior with different parameters, do not place\n    all the calls one next to the other. Instead, add a short sentence\n    explaining what the example shows.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import biocypher as pd\n    &gt;&gt;&gt; df = pd.DataFrame(np.random.randn(3, 3),\n    ...                   columns=('a', 'b', 'c'))\n    &gt;&gt;&gt; df.method(1)\n    21\n    &gt;&gt;&gt; df.method(bar=14)\n    123\n    \"\"\"\n    pass\n</code></pre> <p>.. _docstring.doctest_tips:</p> <p>Tips for getting your examples pass the doctests ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p> <p>Getting the examples pass the doctests in the validation script can sometimes be tricky. Here are some attention points:</p> <ul> <li> <p>Import all needed libraries (except for biocypher and NumPy, those are already   imported as <code>import biocypher as pd</code> and <code>import numpy as np</code>) and define   all variables you use in the example.</p> </li> <li> <p>Try to avoid using random data. However random data might be OK in some   cases, like if the function you are documenting deals with probability   distributions, or if the amount of data needed to make the function result   meaningful is too much, such that creating it manually is very cumbersome.   In those cases, always use a fixed random seed to make the generated examples   predictable. Example::</p> <p>np.random.seed(42) df = pd.DataFrame({'normal': np.random.normal(100, 5, 20)})</p> </li> <li> <p>If you have a code snippet that wraps multiple lines, you need to use '...'   on the continued lines: ::</p> <p>df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], index=['a', 'b', 'c'], ...                   columns=['A', 'B'])</p> </li> <li> <p>If you want to show a case where an exception is raised, you can do::</p> <p>pd.to_datetime([\"712-01-01\"]) Traceback (most recent call last): OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 712-01-01 00:00:00</p> </li> </ul> <p>It is essential to include the \"Traceback (most recent call last):\", but for   the actual error only the error name is sufficient.</p> <ul> <li>If there is a small part of the result that can vary (e.g. a hash in an object   representation), you can use <code>...</code> to represent this part.</li> </ul> <p>If you want to show that <code>s.plot()</code> returns a matplotlib AxesSubplot object,   this will fail the doctest ::</p> <pre><code>&gt;&gt;&gt; s.plot()\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7efd0c0b0690&gt;\n</code></pre> <p>However, you can do (notice the comment that needs to be added) ::</p> <pre><code>&gt;&gt;&gt; s.plot()  # doctest: +ELLIPSIS\n&lt;matplotlib.axes._subplots.AxesSubplot at ...&gt;\n</code></pre> <p>.. _docstring.example_plots:</p> <p>Plots in examples ^^^^^^^^^^^^^^^^^</p> <p>There are some methods in biocypher returning plots. To render the plots generated by the examples in the documentation, the <code>.. plot::</code> directive exists.</p> <p>To use it, place the next code after the \"Examples\" header as shown below. The plot will be generated automatically when building the documentation.</p> <p>.. code-block:: python</p> <pre><code>class Series:\n    def plot(self):\n        \"\"\"\n        Generate a plot with the ``Series`` data.\n\n        Examples\n        --------\n\n        .. plot::\n            :context: close-figs\n\n            &gt;&gt;&gt; ser = pd.Series([1, 2, 3])\n            &gt;&gt;&gt; ser.plot()\n        \"\"\"\n        pass\n</code></pre> <p>.. _docstring.sharing:</p>"},{"location":"community/biocypher-docstring-guide/#sharing-docstrings","title":"Sharing docstrings","text":"<p>biocypher has a system for sharing docstrings, with slight variations, between classes. This helps us keep docstrings consistent, while keeping things clear for the user reading. It comes at the cost of some complexity when writing.</p> <p>Each shared docstring will have a base template with variables, like <code>{klass}</code>. The variables filled in later on using the <code>doc</code> decorator. Finally, docstrings can also be appended to with the <code>doc</code> decorator.</p> <p>In this example, we'll create a parent docstring normally (this is like <code>biocypher.core.generic.NDFrame</code>. Then we'll have two children (like <code>biocypher.core.series.Series</code> and <code>biocypher.core.frame.DataFrame</code>). We'll substitute the class names in this docstring.</p> <p>.. code-block:: python</p> <p>class Parent:        @doc(klass=\"Parent\")        def my_function(self):            \"\"\"Apply my function to {klass}.\"\"\"            ...</p> <p>class ChildA(Parent):        @doc(Parent.my_function, klass=\"ChildA\")        def my_function(self):            ...</p> <p>class ChildB(Parent):        @doc(Parent.my_function, klass=\"ChildB\")        def my_function(self):            ...</p> <p>The resulting docstrings are</p> <p>.. code-block:: python</p> <p>print(Parent.my_function.doc)    Apply my function to Parent. print(ChildA.my_function.doc)    Apply my function to ChildA. print(ChildB.my_function.doc)    Apply my function to ChildB.</p> <p>Notice:</p> <ol> <li>We \"append\" the parent docstring to the children docstrings, which are    initially empty.</li> </ol> <p>Our files will often contain a module-level <code>_shared_doc_kwargs</code> with some common substitution values (things like <code>klass</code>, <code>axes</code>, etc).</p> <p>You can substitute and append in one shot with something like</p> <p>.. code-block:: python</p> <p>@doc(template, **_shared_doc_kwargs)    def my_function(self):        ...</p> <p>where <code>template</code> may come from a module-level <code>_shared_docs</code> dictionary mapping function names to docstrings. Wherever possible, we prefer using <code>doc</code>, since the docstring-writing processes is slightly closer to normal.</p> <p>See <code>biocypher.core.generic.NDFrame.fillna</code> for an example template, and <code>biocypher.core.series.Series.fillna</code> and <code>biocypher.core.generic.frame.fillna</code> for the filled versions.</p>"},{"location":"community/contribute-codebase/","title":"Contribute to the Code Base","text":"<p>Warning</p> <p>This page is redundant with the unified docs in <code>biocypher/documentation</code>. Community docs will be maintained there.</p>"},{"location":"community/contribute-codebase/#developer-guide","title":"Developer Guide","text":"<p>Thank you for considering to contribute to the project! This guide will help you to get started with the development of the project. If you have any questions, please feel free to ask them in the issue tracker or on Zulip.</p>"},{"location":"community/contribute-codebase/#small-contributions","title":"Small Contributions","text":"<p>If you want to contribute a small change (e.g., a bugfix), you can probably immediately go ahead and create a pull request. For more substantial changes or additions, please read on.</p>"},{"location":"community/contribute-codebase/#larger-contributions","title":"Larger Contributions","text":"<p>If you want to contribute a larger change, please create an issue first. This will allow us to discuss the change and make sure that it fits into the project.  It can happen that development for a feature is already in progress, so it is important to check first to avoid duplicate work. If you have any questions, feel free to approach us in any way you like.</p>"},{"location":"community/contribute-codebase/#dependency-management","title":"Dependency management","text":"<p>We use Poetry for dependency management. Please make sure that you have installed Poetry and set up the environment correctly before starting development.</p>"},{"location":"community/contribute-codebase/#setting-up-the-environment","title":"Setting up the environment","text":"<ul> <li> <p>Install dependencies from the lock file: <code>poetry install</code></p> </li> <li> <p>Use the environment: You can either run commands directly with <code>poetry run &lt;command&gt;</code> or open a shell with <code>poetry shell</code> and then run commands directly.</p> </li> </ul>"},{"location":"community/contribute-codebase/#updating-the-environment","title":"Updating the environment","text":"<p>If you want to fix dependency issues, please do so in the Poetry framework. If Poetry does not work for you for some reason, please let us know.</p> <p>The Poetry dependencies are organized in groups. There are groups with dependencies needed for running BioCypher (<code>[tool.poetry.dependencies]</code> with the group name <code>main</code>) and a group with dependencies needed for development (<code>[tool.poetry.group.dev.dependencies]</code> with the group name <code>dev</code>).</p> <p>For adding new dependencies:</p> <ul> <li> <p>Add new dependencies via <code>poetry add</code>: <code>poetry add &lt;dependency&gt; --group &lt;group&gt;</code>. This will update the <code>pyproject.toml</code> and lock file automatically.</p> </li> <li> <p>Add new dependencies via <code>pyproject.toml</code>: Add the dependency to the <code>pyproject.toml</code> file in the correct group, including version. Then update the lock file: <code>poetry lock</code> and install the dependencies: <code>poetry install</code>.</p> </li> </ul>"},{"location":"community/contribute-codebase/#code-quality-and-formal-requirements","title":"Code quality and formal requirements","text":"<p>For ensuring code quality, the following tools are used:</p> <ul> <li> <p>isort for sorting imports</p> </li> <li> <p>black for automated code formatting</p> </li> <li> <p>pre-commit-hooks for ensuring some general rules</p> </li> <li> <p>pep585-upgrade for automatically upgrading type hints to the new native types defined in PEP 585</p> </li> <li> <p>pygrep-hooks for ensuring some general naming rules</p> </li> <li> <p>NEW Ruff An extremely fast Python linter and code formatter, written in Rust</p> </li> </ul> <p>We recommend configuring your IDE to execute Ruff on save/type, which will automatically keep your code clean and fix some linting errors as you type. This is made possible by the fast execution of Ruff and removes the need to run a dedicated pre-commit step. For instance, in VSCode or Cursor, you can add this to your <code>.vscode/settings.json</code>:</p> <pre><code>{\n    \"editor.formatOnType\": true,\n    \"editor.formatOnSave\": true,\n    \"editor.codeActionsOnSave\": {\n        \"source.fixAll.ruff\": \"explicit\",\n        \"source.organizeImports.ruff\": \"explicit\"\n    },\n    \"editor.defaultFormatter\": \"charliermarsh.ruff\"\n}\n</code></pre> <p>Alternatively, pre-commit hooks can be used to automatically or manually run these tools before each commit. They are defined in <code>.pre-commit-config.yaml</code>. To install the hooks run <code>poetry run pre-commit install</code>. The hooks are then executed before each commit. For running the hook for all project files (not only the changed ones) run <code>poetry run pre-commit run --all-files</code>. Our CI runs the pre-commit hooks, so running them locally is a good way to check if your code conforms to the formatting rules.</p>"},{"location":"community/contribute-codebase/#testing","title":"Testing","text":"<p>The project uses pytest for testing. To run the tests, please run <code>pytest</code> in the root directory of the project. We are developing BioCypher using test-driven development. Please make sure that you add tests for your code before submitting a pull request.</p> <p>The existing tests can also help you to understand how the code works. If you have any questions, please feel free to ask them in the issue tracker or on Zulip.</p> <p>Before submitting a pull request, please make sure that all tests pass and that the documentation builds correctly.</p>"},{"location":"community/contribute-codebase/#versioning","title":"Versioning","text":"<p>We use semantic versioning for the project. This means that the version number is incremented according to the following scheme:</p> <ul> <li> <p>Increment the major version number if you make incompatible API changes.</p> </li> <li> <p>Increment the minor version number if you add functionality in a backwards-   compatible manner. Since we are still in the 0.x.y version range, most of the   significant changes will increase the minor version number.</p> </li> <li> <p>Increment the patch version number if you make backwards-compatible bug fixes.</p> </li> </ul> <p>We use the <code>bumpversion</code> tool to update the version number in the <code>pyproject.toml</code> file. This will create a new git tag automatically. Usually, versioning is done by the maintainers, so please do not increment versions in pull requests by default.</p>"},{"location":"community/contribute-codebase/#finding-an-issue-to-contribute-to","title":"Finding an issue to contribute to","text":"<p>If you are brand new to BioCypher or open-source development, we recommend searching the GitHub \"Issues\" tab to find issues that interest you. Unassigned issues labeled <code>Docs</code> and good first issue are typically good for newer contributors.</p> <p>Once you've found an interesting issue, it's a good idea to assign the issue to yourself, so nobody else duplicates the work on it.</p> <p>If for whatever reason you are not able to continue working with the issue, please unassign it, so other people know it's available again. If you want to work on an issue that is currently assigned but you're unsure whether work is actually being done, feel free to kindly ask the current assignee if you can take over (please allow at least a week of inactivity before getting in touch).</p>"},{"location":"community/contribute-codebase/#submitting-a-pull-request","title":"Submitting a Pull Request","text":""},{"location":"community/contribute-codebase/#tips-for-a-successful-pull-request","title":"Tips for a successful pull request","text":"<p>To improve the chances of your pull request being reviewed, you should:</p> <ul> <li>Reference an open issue for non-trivial changes to clarify the PR's purpose.</li> <li>Ensure you have appropriate tests. Tests should be the focus of any PR (apart from documentation changes).</li> <li>Keep your pull requests as simple as possible. Larger PRs take longer to review.</li> <li>Ensure that CI is in a green state. Reviewers may tell you to fix the CI before looking at anything else.</li> </ul>"},{"location":"community/contribute-codebase/#version-control-git-and-github","title":"Version control, Git, and GitHub","text":"<p>BioCypher is hosted on GitHub, and to contribute, you will need to sign up for a free GitHub account. We use Git for version control to allow many people to work together on the project.</p> <p>If you are new to Git, you can reference some of these resources for learning Git. Feel free to reach out to the contributor community for help if needed:</p> <ul> <li>Git documentation.</li> </ul> <p>The project follows a forking workflow further described on this page whereby contributors fork the repository, make changes and then create a Pull Request. So please be sure to read and follow all the instructions in this guide.</p> <p>If you are new to contributing to projects through forking on GitHub, take a look at the GitHub documentation for contributing to projects. GitHub provides a quick tutorial using a test repository that may help you become more familiar with forking a repository, cloning a fork, creating a feature branch, pushing changes and making Pull Requests.</p> <p>Below are some useful resources for learning more about forking and Pull Requests on GitHub:</p> <ul> <li> <p>the GitHub documentation for forking a repo.</p> </li> <li> <p>the GitHub documentation for collaborating with Pull Requests.</p> </li> <li> <p>the GitHub documentation for working with forks.</p> </li> </ul> <p>There are also many unwritten rules and conventions that are helpful in interacting with other open-source contributors. These lessons from PyOpenSci are a good resource for learning more about how to interact with other open-source contributors in scientific computing.</p>"},{"location":"community/contribute-codebase/#getting-started-with-git","title":"Getting started with Git","text":"<p>GitHub has instructions for installing git, setting up your SSH key, and configuring git. All these steps need to be completed before you can work seamlessly between your local repository and GitHub.</p>"},{"location":"community/contribute-codebase/#create-a-fork-of-biocypher","title":"Create a fork of BioCypher","text":"<p>You will need your own fork of BioCypher in order to eventually open a Pull Request. Go to the BioCypher project page and hit the Fork button. Please uncheck the box to copy only the main branch before selecting Create Fork. You will then want to clone your fork to your machine.</p> <pre><code>git clone https://github.com/your-user-name/biocypher.git\ncd biocypher\ngit remote add upstream https://github.com/biocypher/biocypher.git\ngit fetch upstream\n</code></pre> <p>This creates the directory <code>biocypher</code> and connects your repository to the upstream (main project) biocypher repository. They have the same name, but your local repository and fork are separate from the upstream repository.</p>"},{"location":"community/contribute-codebase/#creating-a-feature-branch","title":"Creating a feature branch","text":"<p>Your local <code>main</code> branch should always reflect the current state of BioCypher repository. First ensure it's up-to-date with the main BioCypher repository.</p> <pre><code>git checkout main\ngit pull upstream main --ff-only\n</code></pre> <p>Then, create a feature branch for making your changes. For example, we are going to create a branch called <code>my-new-feature-for-biocypher</code></p> <pre><code>git checkout -b my-new-feature-for-biocypher\n</code></pre> <p>This changes your working branch from <code>main</code> to the <code>my-new-feature-for-biocypher</code> branch. Keep any changes in this branch specific to one bug or feature so it is clear what the branch brings to BioCypher. You can have many feature branches and switch between them using the <code>git checkout</code> command.</p>"},{"location":"community/contribute-codebase/#making-code-changes","title":"Making code changes","text":"<p>Before modifying any code, ensure you follow the contributing environment guidelines to set up an appropriate development environment.</p> <p>When making changes, follow these BioCypher-specific guidelines:</p> <ol> <li> <p>Keep changes of that branch/PR focused on a single feature or bug fix.</p> </li> <li> <p>Follow roughly the conventional commit message conventions.</p> </li> </ol>"},{"location":"community/contribute-codebase/#pushing-your-changes","title":"Pushing your changes","text":"<p>When you want your committed changes to appear publicly on your GitHub page, you can push your forked feature branch's commits to your forked repository on GitHub.</p> <p>Now your code is on GitHub, but it is not yet a part of the BioCypher project. For that to happen, a Pull Request (PR) needs to be submitted.</p>"},{"location":"community/contribute-codebase/#opening-a-pull-request-pr","title":"Opening a Pull Request (PR)","text":"<p>If everything looks good according to the general guidelines, you are ready to make a Pull Request. A Pull Request is how code from your fork becomes available to the project maintainers to review and merge into the project to appear in the next release. To submit a Pull Request:</p> <ol> <li> <p>Navigate to your repository on GitHub.</p> </li> <li> <p>Click on the Compare &amp; Pull Request button.</p> </li> <li> <p>You can then click on Commits and Files Changed to make sure everything looks okay one last time.</p> </li> <li> <p>Write a descriptive title that includes prefixes. BioCypher uses a convention for title prefixes, most commonly, <code>feat:</code> for features, <code>fix:</code> for bug fixes, and <code>refactor:</code> for refactoring.</p> </li> <li> <p>Write a description of your changes in the <code>Preview Discussion</code> tab. This description will inform the reviewers about the changes you made, so please include all relevant information, including the motivation, implementation details, and references to any issues that you are addressing.</p> </li> <li> <p>Make sure to <code>Allow edits from maintainers</code>; this allows the maintainers to make changes to your PR directly, which is useful if you are not sure how to fix the PR.</p> </li> <li> <p>Click <code>Send Pull Request</code>.</p> </li> <li> <p>Optionally, you can assign reviewers to your PR, if you know who should review it.</p> </li> </ol> <p>This request then goes to the repository maintainers, and they will review the code.</p>"},{"location":"community/contribute-codebase/#updating-your-pull-request","title":"Updating your Pull Request","text":"<p>Based on the review you get on your pull request, you will probably need to make some changes to the code. You can follow the steps above again to address any feedback and update your pull request.</p>"},{"location":"community/contribute-codebase/#parallel-changes-in-the-upstream-main-branch","title":"Parallel changes in the upstream <code>main</code> branch","text":"<p>In case of simultaneous changes to the upstream code, it is important that these changes are reflected in your pull request. To update your feature branch with changes in the biocypher <code>main</code> branch, run:</p> <pre><code>    git checkout my-new-feature-for-biocypher\n    git fetch upstream\n    git merge upstream/main\n</code></pre> <p>If there are no conflicts (or they could be fixed automatically), a file with a default commit message will open, and you can simply save and quit this file.</p> <p>If there are merge conflicts, you need to resolve those conflicts. See here for an explanation on how to do this.</p> <p>Once the conflicts are resolved, run:</p> <ol> <li><code>git add -u</code> to stage any files you've updated;</li> <li><code>git commit</code> to finish the merge.</li> </ol> <p>After the feature branch has been updated locally, you can now update your pull request by pushing to the branch on GitHub:</p> <pre><code>    git push origin my-new-feature-for-biocypher\n</code></pre> <p>Any <code>git push</code> will automatically update your pull request with your branch's changes and restart the <code>Continuous Integration</code> checks.</p>"},{"location":"community/contribute-docs/","title":"Contribute to the Documentation","text":"<p>Warning</p> <p>This page is redundant with the unified docs in <code>biocypher/documentation</code>. Community docs will be maintained there.</p>"},{"location":"community/contribute-docs/#contributing-to-the-documentation","title":"Contributing to the documentation","text":"<p>Contributing to the documentation benefits everyone who uses BioCypher. We encourage you to help us improve the documentation, and you don't have to be an expert on BioCypher to do so! In fact, there are sections of the docs that are worse off after being written by experts. If something in the docs doesn't make sense to you, updating the relevant section after you figure it out is a great way to ensure it will help the next person.</p>"},{"location":"community/contribute-docs/#how-to-contribute-to-the-documentation","title":"How to contribute to the documentation","text":"<p>The documentation is written in Markdown, which is almost like writing in plain English, and built using Material for MkDocs. The simplest way to contribute to the docs is to click on the <code>Edit</code> button (pen and paper) at the top right of any page. This will take you to the source file on GitHub, where you can make your changes and create a pull request using GitHub's web interface (the <code>Commit changes...</code> button).</p> <p>Some other important things to know about the docs:</p> <ul> <li> <p>The BioCypher documentation consists of two parts: the docstrings in the code   itself and the docs in the <code>docs/</code> folder. The docstrings provide a clear   explanation of the usage of the individual functions, while the documentation   website you are looking at is built from the <code>docs/</code> folder.</p> </li> <li> <p>The docstrings follow a BioCypher convention, based on the Google Docstring   Standard.</p> </li> <li> <p>Our API documentation files in <code>docs/reference/source</code> contain the   instructions for the auto-generated documentation from the docstrings. For   classes, there are a few subtleties around controlling which methods and   attributes have pages auto-generated.</p> </li> </ul>"},{"location":"community/contribute/","title":"Where to Start","text":"<p>Warning</p> <p>This page is redundant with the unified docs in <code>biocypher/documentation</code>. Community docs will be maintained there.</p>"},{"location":"community/contribute/#how-to-start-contributing","title":"How to Start Contributing","text":"<p>There are many valuable ways to contribute besides writing code. Thank you for dedicating your time to improve our projects!</p>"},{"location":"community/contribute/#bug-reports-and-enhancement-requests","title":"Bug reports and enhancement requests","text":"<p>Bug reports and enhancement requests are an important part of making any software more stable. We curate them though Github issues. When opening an issue or request, please select the appropriate category and fill out the issue form fully to ensure others and the core development team can fully understand the scope of the issue. If your category is not listed, you can create a blank issue.</p> <p>The issue will then show up to the BioCypher community and be open to comments/ideas from others.</p>"},{"location":"community/contribute/#categories","title":"Categories","text":"<ul> <li>Bug Report: Report incorrect behavior in the BioCypher library</li> <li>Register New Component: Register a new component in the BioCypher ecosystem, either one you have created, or one that you would like to see added</li> <li>Documentation Improvement: Report wrong or missing documentation</li> <li>Feature Request: Suggest an idea for BioCypher</li> </ul>"},{"location":"community/contribute/#detailed-guides","title":"Detailed Guides","text":"<ul> <li> <p> Contributing to the Documentation</p> <p>A simple way to get started is to contribute to the documentation. Please follow the guide here to learn how to do so.</p> <p> To the contribution guide</p> </li> </ul> <ul> <li> <p> Contributing to the Code Base</p> <p>The best way to contribute code is to open a pull request on Github. Please follow the guide here to learn how to do so.</p> <p> To the contribution guide</p> </li> </ul>"},{"location":"community/maintenance/","title":"Biocypher Maintenance","text":"<p>This guide is for BioCypher's maintainers. It may also be interesting to contributors looking to understand the BioCypher development process and what steps are necessary to become a maintainer.</p>"},{"location":"community/maintenance/#roles","title":"Roles","text":"<p>GitHub publishes the full list of permissions.</p>"},{"location":"community/maintenance/#tasks","title":"Tasks","text":"<p>biocypher is largely a volunteer project, so these tasks shouldn't be read as \"expectations\" of triage and maintainers. Rather, they're general descriptions of what it means to be a maintainer.</p> <ul> <li>Triage newly filed issues (see :ref:<code>maintaining.triage</code>)</li> <li>Review newly opened pull requests</li> <li>Respond to updates on existing issues and pull requests</li> <li>Drive discussion and decisions on stalled issues and pull requests</li> <li>Provide experience / wisdom on API design questions to ensure consistency and maintainability</li> <li>Project organization (run / attend developer meetings, represent biocypher)</li> </ul> <p>https://matthewrocklin.com/blog/2019/05/18/maintainer may be interesting background reading.</p>"},{"location":"community/maintenance/#issue-triage","title":"Issue triage","text":"<p>Triage is an important first step in addressing issues reported by the community, and even partial contributions are a great way to help maintain biocypher. Only remove the \"Needs Triage\" tag once all of the steps below have been completed.</p> <p>Here's a typical workflow for triaging a newly opened issue.</p> <ol> <li> <p>Thank the reporter for opening an issue    The issue tracker is many people's first interaction with the biocypher project itself,    beyond just using the library. As such, we want it to be a welcoming, pleasant    experience.</p> </li> <li> <p>Is the necessary information provided?    Ideally reporters would fill out the issue template, but many don't.    If crucial information (like the version of biocypher they used), is missing    feel free to ask for that and label the issue with \"Needs info\". The    report should follow the guidelines in :ref:<code>contributing.bug_reports</code>.    You may want to link to that if they didn't follow the template.    Make sure that the title accurately reflects the issue. Edit it yourself    if it's not clear.</p> </li> <li> <p>Is this a duplicate issue?    We have many open issues. If a new issue is clearly a duplicate, label the    new issue as \"Duplicate\" and close the issue with a link to the original issue.    Make sure to still thank the reporter, and encourage them to chime in on the    original issue, and perhaps try to fix it.    If the new issue provides relevant information, such as a better or slightly    different example, add it to the original issue as a comment or an edit to    the original post.</p> </li> <li> <p>Is the issue minimal and reproducible?    For bug reports, we ask that the reporter provide a minimal reproducible    example. See https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports    for a good explanation. If the example is not reproducible, or if it's    clearly not minimal, feel free to ask the reporter if they can provide    and example or simplify the provided one. Do acknowledge that writing    minimal reproducible examples is hard work. If the reporter is struggling,    you can try to write one yourself and we'll edit the original post to include it.    If a reproducible example can't be provided, add the \"Needs info\" label.    If a reproducible example is provided, but you see a simplification,    edit the original post with your simpler reproducible example.    Ensure the issue exists on the main branch and that it has the \"Needs Triage\" tag    until all steps have been completed. Add a comment to the issue once you have    verified it exists on the main branch, so others know it has been confirmed.</p> </li> <li> <p>Is this a clearly defined feature request?    Generally, biocypher prefers to discuss and design new features in issues,    before a pull request is made. Encourage the submitter to include a proposed    API for the new feature. Having them write a full docstring is a good way to    pin down specifics.    Tag new feature requests with \"Needs Discussion\", as we'll need a discussion    from several biocypher maintainers before deciding whether the proposal is in    scope for biocypher.</p> </li> <li> <p>Is this a usage question?    We prefer that usage questions are asked on StackOverflow with the biocypher    tag. https://stackoverflow.com/questions/tagged/biocypher    If it's easy to answer, feel free to link to the relevant documentation section,    let them know that in the future this kind of question should be on    StackOverflow, and close the issue.</p> </li> <li> <p>What labels and milestones should I add?    Apply the relevant labels. This is a bit of an art, and comes with experience.    Look at similar issues to get a feel for how things are labeled.    If the issue is clearly defined and the fix seems relatively straightforward,    label the issue as \"Good first issue\".    Once you have completed the above, make sure to remove the \"needs triage\" label.</p> </li> </ol>"},{"location":"community/maintenance/#closing-issues","title":"Closing issues","text":"<p>Be delicate here: many people interpret closing an issue as us saying that the conversation is over. It's typically best to give the reporter some time to respond or self-close their issue if it's determined that the behavior is not a bug, or the feature is out of scope. Sometimes reporters just go away though, and we'll close the issue after the conversation has died. If you think an issue should be closed but are not completely sure, please apply the \"closing candidate\" label and wait for other maintainers to take a look.</p>"},{"location":"community/maintenance/#reviewing-pull-requests","title":"Reviewing pull requests","text":"<p>Anybody can review a pull request: regular contributors, triagers, or core-team members. But only core-team members can merge pull requests when they're ready.</p> <p>Here are some things to check when reviewing a pull request.</p> <ul> <li>Tests should be in a sensible location: in the same file as closely related tests.</li> <li>New public APIs should be included somewhere in <code>doc/source/reference/</code>.</li> <li>New / changed API should use the <code>versionadded</code> or <code>versionchanged</code> directives in the docstring.</li> <li>User-facing changes should have a whatsnew in the appropriate file.</li> <li>Regression tests should reference the original GitHub issue number like <code># GH-1234</code>.</li> <li>The pull request should be labeled and assigned the appropriate milestone (the next patch release   for regression fixes and small bug fixes, the next minor milestone otherwise)</li> <li>Changes should comply with our :ref:<code>policies.version</code>.</li> </ul>"},{"location":"community/maintenance/#cleaning-up-old-issues","title":"Cleaning up old issues","text":"<p>Every open issue in biocypher has a cost. Open issues make finding duplicates harder, and can make it harder to know what needs to be done in biocypher. That said, closing issues isn't a goal on its own. Our goal is to make biocypher the best it can be, and that's best done by ensuring that the quality of our open issues is high.</p> <p>Occasionally, bugs are fixed but the issue isn't linked to in the Pull Request. In these cases, comment that \"This has been fixed, but could use a test.\" and label the issue as \"Good First Issue\" and \"Needs Test\".</p> <p>If an older issue doesn't follow our issue template, edit the original post to include a minimal example, the actual output, and the expected output. Uniformity in issue reports is valuable.</p> <p>If an older issue lacks a reproducible example, label it as \"Needs Info\" and ask them to provide one (or write one yourself if possible). If one isn't provide reasonably soon, close it according to the policies in :ref:<code>maintaining.closing</code>.</p>"},{"location":"community/maintenance/#cleaning-up-old-pull-requests","title":"Cleaning up old pull requests","text":"<p>Occasionally, contributors are unable to finish off a pull request. If some time has passed (two weeks, say) since the last review requesting changes, gently ask if they're still interested in working on this. If another two weeks or so passes with no response, thank them for their work and then either:</p> <ul> <li>close the pull request;</li> <li>push to the contributor's branch to push their work over the finish line (if   you're part of <code>biocypher-core</code>). This can be helpful for pushing an important PR   across the line, or for fixing a small merge conflict.</li> </ul> <p>If closing the pull request, then please comment on the original issue that \"There's a stalled PR at #1234 that may be helpful.\", and perhaps label the issue as \"Good first issue\" if the PR was relatively close to being accepted.</p>"},{"location":"community/maintenance/#becoming-a-biocypher-maintainer","title":"Becoming a biocypher maintainer","text":"<p>The full process is outlined in our <code>governance documents</code>_. In summary, we're happy to give triage permissions to anyone who shows interest by being helpful on the issue tracker.</p> <p>The required steps for adding a maintainer are:</p> <ol> <li>Contact the contributor and ask their interest to join.</li> <li> <p>Add the contributor to the appropriate <code>GitHub Team &lt;https://github.com/orgs/biocypher-dev/teams&gt;</code>_ if accepted the invitation.</p> </li> <li> <p><code>biocypher-core</code> is for core team members</p> </li> <li><code>biocypher-triage</code> is for biocypher triage members</li> </ol> <p>If adding to <code>biocypher-core</code>, there are two additional steps:</p> <ol> <li>Add the contributor to the biocypher Google group.</li> <li>Create a pull request to add the contributor's GitHub handle to <code>biocypher-dev/biocypher/web/biocypher/config.yml</code>.</li> </ol> <p>The current list of core-team members is at https://github.com/biocypher-dev/biocypher/blob/main/web/biocypher/config.yml</p>"},{"location":"community/maintenance/#merging-pull-requests","title":"Merging pull requests","text":"<p>Only core team members can merge pull requests. We have a few guidelines.</p> <ol> <li>You should typically not self-merge your own pull requests without approval.    Exceptions include things like small changes to fix CI    (e.g. pinning a package version). Self-merging with approval from other    core team members is fine if the change is something you're very confident    about.</li> <li>You should not merge pull requests that have an active discussion, or pull    requests that has any <code>-1</code> votes from a core maintainer. biocypher operates    by consensus.</li> <li>For larger changes, it's good to have a +1 from at least two core team members.</li> </ol> <p>In addition to the items listed in :ref:<code>maintaining.closing</code>, you should verify that the pull request is assigned the correct milestone.</p> <p>Pull requests merged with a patch-release milestone will typically be backported by our bot. Verify that the bot noticed the merge (it will leave a comment within a minute typically). If a manual backport is needed please do that, and remove the \"Needs backport\" label once you've done it manually. If you forget to assign a milestone before tagging, you can request the bot to backport it with:</p> <p>.. code-block:: console</p> <p>@Meeseeksdev backport  <p>.. _maintaining.asv-machine:</p>"},{"location":"community/maintenance/#benchmark-machine","title":"Benchmark machine","text":"<p>The team currently owns dedicated hardware for hosting a website for biocypher' ASV performance benchmark. The results are published to https://asv-runner.github.io/asv-collection/biocypher/</p>"},{"location":"community/maintenance/#configuration","title":"Configuration","text":"<p>The machine can be configured with the <code>Ansible &lt;http://docs.ansible.com/ansible/latest/index.html&gt;</code>_ playbook in https://github.com/tomaugspurger/asv-runner.</p>"},{"location":"community/maintenance/#publishing","title":"Publishing","text":"<p>The results are published to another GitHub repository, https://github.com/tomaugspurger/asv-collection. Finally, we have a cron job on our docs server to pull from https://github.com/tomaugspurger/asv-collection, to serve them from <code>/speed</code>. Ask Tom or Joris for access to the webserver.</p>"},{"location":"community/maintenance/#debugging","title":"Debugging","text":"<p>The benchmarks are scheduled by Airflow. It has a dashboard for viewing and debugging the results. You'll need to setup an SSH tunnel to view them <pre><code>ssh -L 8080:localhost:8080 biocypher@panda.likescandy.com\n</code></pre></p>"},{"location":"community/maintenance/#release-process","title":"Release process","text":"<p>The release process makes a snapshot of biocypher (a git commit) available to users with a particular version number. After the release the new biocypher version will be available in the next places:</p> <ul> <li>Git repo with a <code>new tag &lt;https://github.com/biocypher-dev/biocypher/tags&gt;</code>_</li> <li>Source distribution in a <code>GitHub release &lt;https://github.com/biocypher-dev/biocypher/releases&gt;</code>_</li> <li>Pip packages in the <code>PyPI &lt;https://pypi.org/project/biocypher/&gt;</code>_</li> <li>Conda/Mamba packages in <code>conda-forge &lt;https://anaconda.org/conda-forge/biocypher&gt;</code>_</li> </ul> <p>The process for releasing a new version of biocypher is detailed next section.</p> <p>The instructions contain <code>&lt;version&gt;</code> which needs to be replaced with the version to be released (e.g. <code>1.5.2</code>). Also the branch to be released <code>&lt;branch&gt;</code>, which depends on whether the version being released is the release candidate of a new version, or any other version. Release candidates are released from <code>main</code>, while other versions are released from their branch (e.g. <code>1.5.x</code>).</p>"},{"location":"community/maintenance/#prerequisites","title":"Prerequisites","text":"<p>In order to be able to release a new biocypher version, the next permissions are needed:</p> <ul> <li>Merge rights to the <code>biocypher &lt;https://github.com/biocypher-dev/biocypher/&gt;</code> and   <code>biocypher-feedstock &lt;https://github.com/conda-forge/biocypher-feedstock/&gt;</code> repositories.   For the latter, open a PR adding your GitHub username to the conda-forge recipe.</li> <li>Permissions to push to <code>main</code> in the biocypher repository, to push the new tags.</li> <li><code>Write permissions to PyPI &lt;https://github.com/conda-forge/biocypher-feedstock/pulls&gt;</code>_.</li> <li>Access to our website / documentation server. Share your public key with the   infrastructure committee to be added to the <code>authorized_keys</code> file of the main   server user.</li> <li>Access to the social media accounts, to publish the announcements.</li> </ul>"},{"location":"community/maintenance/#pre-release","title":"Pre-release","text":"<ol> <li> <p>Agree with the core team on the next topics:</p> </li> <li> <p>Release date (major/minor releases happen usually every 6 months, and patch releases      monthly until x.x.5, just before the next major/minor)</p> </li> <li>Blockers (issues and PRs that must be part of the release)</li> <li> <p>Next version after the one being released</p> </li> <li> <p>Update and clean release notes for the version to be released, including:</p> </li> <li> <p>Set the final date of the release</p> </li> <li>Remove any unused bullet point</li> <li> <p>Make sure there are no formatting issues, typos, etc.</p> </li> <li> <p>Make sure the CI is green for the last commit of the branch being released.</p> </li> <li> <p>If not a release candidate, make sure all backporting pull requests to the branch    being released are merged.</p> </li> <li> <p>Create a new issue and milestone for the version after the one being released.    If the release was a release candidate, we would usually want to create issues and    milestones for both the next major/minor, and the next patch release. In the    milestone of a patch release, we add the description <code>on-merge: backport to &lt;branch&gt;</code>,    so tagged PRs are automatically backported to the release branch by our bot.</p> </li> <li> <p>Change the milestone of all issues and PRs in the milestone being released to the    next milestone.</p> </li> </ol>"},{"location":"community/maintenance/#release","title":"Release","text":"<ol> <li>Create an empty commit and a tag in the last commit of the branch to be released:: <pre><code>git checkout &lt;branch&gt;\ngit pull --ff-only upstream &lt;branch&gt;\ngit clean -xdf\ngit commit --allow-empty --author=\"biocypher Development Team &lt;biocypher-dev@python.org&gt;\" -m \"RLS: &lt;version&gt;\"\ngit tag -a v&lt;version&gt; -m \"Version &lt;version&gt;\"  # NOTE that the tag is v1.5.2 with \"v\" not 1.5.2\ngit push upstream &lt;branch&gt; --follow-tags\n</code></pre></li> </ol> <p>The docs for the new version will be built and published automatically with the docs job in the CI, which will be triggered when the tag is pushed.</p> <ol> <li>Only if the release is a release candidate, we want to create a new branch for it, immediately    after creating the tag. For example, if we are releasing biocypher 1.4.0rc0, we would like to    create the branch 1.4.x to backport commits to the 1.4 versions. As well as create a tag to    mark the start of the development of 1.5.0 (assuming it is the next version)::</li> </ol> <p><pre><code>git checkout -b 1.4.x\ngit push upstream 1.4.x\ngit checkout main\ngit commit --allow-empty -m \"Start 1.5.0\"\ngit tag -a v1.5.0.dev0 -m \"DEV: Start 1.5.0\"\ngit push upstream main --follow-tags\n</code></pre> 3. Download the source distribution and wheels from the <code>wheel staging area &lt;https://anaconda.org/scientific-python-nightly-wheels/biocypher&gt;</code>_.    Be careful to make sure that no wheels are missing (e.g. due to failed builds).</p> <p>Running scripts/download_wheels.sh with the version that you want to download wheels/the sdist for should do the trick.    This script will make a <code>dist</code> folder inside your clone of biocypher and put the downloaded wheels and sdist there::</p> <p><pre><code>scripts/download_wheels.sh &lt;VERSION&gt;\n</code></pre> 4. Create a <code>new GitHub release &lt;https://github.com/biocypher-dev/biocypher/releases/new&gt;</code>_:</p> <ul> <li>Tag: <code>&lt;version&gt;</code></li> <li>Title: <code>biocypher &lt;version&gt;</code></li> <li>Description: Copy the description of the last release of the same kind (release candidate, major/minor or patch release)</li> <li>Files: <code>biocypher-&lt;version&gt;.tar.gz</code> source distribution just generated</li> <li>Set as a pre-release: Only check for a release candidate</li> <li> <p>Set as the latest release: Leave checked, unless releasing a patch release for an older version      (e.g. releasing 1.4.5 after 1.5 has been released)</p> </li> <li> <p>Upload wheels to PyPI:: <pre><code>twine upload biocypher/dist/biocypher-&lt;version&gt;*.{whl,tar.gz} --skip-existing\n</code></pre></p> </li> <li>The GitHub release will after some hours trigger an    <code>automated conda-forge PR &lt;https://github.com/conda-forge/biocypher-feedstock/pulls&gt;</code>_.    (If you don't want to wait, you can open an issue titled <code>@conda-forge-admin, please update version</code> to trigger the bot.)    Merge it once the CI is green, and it will generate the conda-forge packages.</li> </ul> <p>In case a manual PR needs to be done, the version, sha256 and build fields are the    ones that usually need to be changed. If anything else in the recipe has changed since    the last release, those changes should be available in <code>ci/meta.yaml</code>.</p>"},{"location":"community/maintenance/#post-release","title":"Post-Release","text":"<ol> <li> <p>Update symlinks to stable documentation by logging in to our web server, and    editing <code>/var/www/html/biocypher-docs/stable</code> to point to <code>version/&lt;latest-version&gt;</code>    for major and minor releases, or <code>version/&lt;minor&gt;</code> to <code>version/&lt;patch&gt;</code> for    patch releases. The exact instructions are (replace the example version numbers by    the appropriate ones for the version you are releasing):</p> <ul> <li>Log in to the server and use the correct user.</li> <li><code>cd /var/www/html/biocypher-docs/</code></li> <li><code>ln -sfn version/2.1 stable</code> (for a major or minor release)</li> <li><code>ln -sfn version/2.0.3 version/2.0</code> (for a patch release)</li> </ul> </li> <li> <p>If releasing a major or minor release, open a PR in our source code to update    <code>web/biocypher/versions.json</code>, to have the desired versions in the documentation    dropdown menu.</p> </li> <li> <p>Close the milestone and the issue for the released version.</p> </li> <li> <p>Create a new issue for the next release, with the estimated date of release.</p> </li> <li> <p>Open a PR with the placeholder for the release notes of the next version. See    for example <code>the PR for 1.5.3 &lt;https://github.com/biocypher-dev/biocypher/pull/49843/files&gt;</code>_.    Note that the template to use depends on whether it is a major, minor or patch release.</p> </li> <li> <p>Announce the new release in the official channels (use previous announcements    for reference):</p> <ul> <li>The biocypher-dev and pydata mailing lists</li> <li>Twitter, Mastodon, Telegram and LinkedIn</li> </ul> </li> <li> <p>Update this release instructions to fix anything incorrect and to update about any    change since the last release.</p> </li> </ol> <p>.. _governance documents: https://github.com/biocypher-dev/biocypher/blob/main/web/biocypher/about/governance.md .. _list of permissions: https://docs.github.com/en/organizations/managing-access-to-your-organizations-repositories/repository-roles-for-an-organization</p>"},{"location":"community/release-notes/","title":"Release notes","text":""},{"location":"community/release-notes/#version-080","title":"Version 0.8.0","text":""},{"location":"community/release-notes/#enhancements","title":"Enhancements","text":"<ul> <li>Labels can now be sorted in various way before being written to CSV, in order to reflect the ontological hierarchy flexibly. The default has changed to \"ascending\", meaning more specific to more generic labels (it was alphabetic before).</li> </ul>"},{"location":"community/release-notes/#contributors","title":"Contributors","text":"<ul> <li>Johann Dreo</li> </ul>"},{"location":"community/release-notes/#version-070","title":"Version 0.7.0","text":""},{"location":"community/release-notes/#enhancements_1","title":"Enhancements","text":"<ul> <li>Added <code>Ruff</code> as a tool for linting/formatting, including CI.</li> <li>BioCypher has support for NetworkX.</li> <li>BioChatter has improved token statistics handling (@shaohong feng) and has received a prototypic Python API calling module as a result of the German BioHackathon 3 in December. In this hackathon, we worked on integrating BioChatter-driven API calling with the scverse ecosystem (starting with core packages anndata and scanpy). Many thanks to all who contributed in the hackathon!</li> </ul>"},{"location":"community/release-notes/#contributors_1","title":"Contributors","text":"<ul> <li>Paul To</li> <li>Yaxi Liu</li> <li>Shaohong Feng</li> <li>Sebastian Lobentanzer</li> <li>Edwin Carre\u00f1o</li> </ul>"},{"location":"learn/quickstart/","title":"Quickstart","text":"<ul> <li> <p> Already Familiar?</p> <p>We have a project template (batteries included!)</p> <p> To the template</p> </li> <li> <p> New to BioCypher?</p> <p>Follow our detailed tutorial for on-boarding BioCypher.</p> <p> To the tutorial</p> </li> </ul> <p>Note</p> <p>If you already know how BioCypher works, we provide here a quickstart into the knowledge graph build process. We provide a template repository on GitHub, which you can use to get started with your own project. You can get it here. To set up a new project, simply follow the instructions in the README.</p> <p>If you are new to BioCypher and would like a step-by-step introduction to the package, please follow the tutorial.</p> <p>The BioCypher workflow of creating your own knowledge graph consists of three consecutive steps:</p> <ol> <li> <p>Clearly define the scope of your project, including the data sources you want to use, the entities and relationships you want to represent, and the ontologies that should inform these entities.</p> </li> <li> <p>Using these definitions, find existing adapters of data sources or, if necessary, create your own. For the data yielded by these adapters, create a schema configuration file that tells BioCypher how to represent the entities and relationships in the graph.</p> </li> <li> <p>Run BioCypher using the adapters and schema config to create the knowledge graph. If necessary, iterate over KG construction and configuration until you are satisfied with the result.</p> </li> </ol> <p>Graphical Interface Support</p> <p>We are currently working on a graphical user interface to support the complex process of defining and building a knowledge graph. Get in touch if you'd like to test or contribute to the development of this tool.</p>"},{"location":"learn/quickstart/#the-input-adapter","title":"The input adapter","text":"<p>BioCypher follows a modular approach to data inputs; to create a knowledge graph, we use at least one adapter module that provides a data stream to build the graph from. Examples for current adapters can be found on the GitHub project adapter view. This is the first place to look when creating your own KG; BioCypher adapters are meant to be reusable and a centralised way of maintaining access to data sources.</p> <p>Adapters can ingest data from many different input formats, including Python modules as in the CROssBAR adapter (which uses the OmniPath backend software, PyPath, for downloading and caching data), advanced file management formats such as Parquet as in the Open Targets adapter, or simple CSV files as in the Dependency Map adapter.</p> <p>The main function of the adapter is to pass data into BioCypher, usually as some form of iterable (commonly a list or generator of items). As a minimal example, we load a list of proteins with identifiers, trivial names, and molecular masses from a (fictional) CSV:</p> Adapter yielding nodes<pre><code># read data into df\nwith open(\"file.csv\", \"r\") as f:\n    proteins = pd.read_csv(f)\n\n# yield proteins from data frame\ndef node_generator():\n    for p in proteins:\n        _id = p[\"uniprot_id\"]\n        _type = \"protein\"\n        _props = {\n            \"name\": p[\"trivial_name\"],\n            \"mm\": p[\"molecular_mass\"]\n        }\n\n        yield (_id, _type, _props)\n</code></pre> <p>For nodes, BioCypher expects a tuple containing three entries; the preferred identifier of the node, the type of entity, and a dictionary containing all other properties (can be empty). What BioCypher does with the received information is determined largely by the schema configuration detailed below.</p> Adapter yielding edges<pre><code># read data into df\nwith open(\"file.csv\", \"r\") as f:\n    interactions = pd.read_csv(f)\n\n# yield interactions from data frame\ndef edge_generator():\n    for i in interactions:\n        _id = i[\"id\"]\n        _source = i[\"source\"]\n        _target = i[\"target\"]\n        _type = \"interaction\"\n        _props = {\n            \"type\": i[\"relationship_type\"],\n            \"score\": i[\"score\"],\n        }\n\n        yield (_id, _source, _target, _type, _props)\n</code></pre> <p>For edges, BioCypher expects a tuple containing five entries; the preferred identifier of the edge (can be <code>None</code>), the identifier of the source node (non-optional), the identifier of the target node (non-optional), the type of relationship, and a dictionary containing all other properties (can be empty).</p> <p>For advanced usage, the type of node or edge can be determined programatically. Properties do not need to be explicitly called one by one; they can be passed in as a complete dictionary of all entries and filtered inside BioCypher by detailing the desired properties per node type in the schema configuration file.</p>"},{"location":"learn/quickstart/#the-schema-configuration-yaml-file","title":"The schema configuration YAML file","text":"<p>The second important component of translation into a BioCypher-compatible knowledge graph is the specification of graph constituents and their mode of representation in the graph. To make this known to the BioCypher module, we use the schema-config.yaml, which details only the immediate constituents of the desired graph as the top-level entries in the YAML file. While each of these top-level entries is required to be found in the underlying ontology (for instance, the Biolink model), the <code>input_label</code> field is arbitrary and has to match the <code>_type</code> yielded by the adapter (compare above).</p> <p>Other fields of each entry can refer to the representation of the entity in the KG (<code>represented_as: node</code>), and the identifier namespace chosen for each entity type. For instance, a protein could be represented by a UniProt identifier, the corresponding ENSEMBL identifier, or an HGNC gene symbol. We prefer the CURIE prefix for unambiguous identification of entities. The CURIE prefix for \"Uniprot Protein\" is <code>uniprot</code>, so a consistent protein schema definition would be:</p> <pre><code>protein:                    # top-level entry, has to match ontology\n  represented_as: node      # mode of representation: node or edge\n  preferred_id: uniprot     # preferred identifier namespace\n  input_label: protein      # label that identifies members of this class (_type)\n</code></pre> <p>Note</p> <p>For BioCypher classes, similar to the internal representation in the Biolink model, we use lower sentence-case notation, e.g., <code>protein</code> and <code>small molecule</code>. For file names and Neo4j labels, these are converted to PascalCase. For more information, see the Ontology tutorial.</p> <p>The above configuration of the protein class specifies its representation as a node, that we wish to use the UniProt identifier as the main identifier for proteins, and that proteins in the data stream from the adapter carry the label (<code>_type</code>) <code>protein</code> (in lowercase). Should we want to use the ENSEMBL namespace instead of UniProt IDs, the corresponding CURIE prefix, in this case, <code>ensembl</code>, can be substituted:</p> <pre><code>protein:\n  represented_as: node\n  preferred_id: ensembl\n  input_label: protein\n</code></pre> <p>If there exists no identifier system that is suitable for coverage of the data (which is fairly common when it comes to relationships), <code>preferred_id</code> field can be omitted. This will lead to the creation of a generic <code>id</code> property on this node or edge type. For more explanation, see the Basic Tutorial.</p> <p>Rename incoming</p> <p>To denote the namespace of identifiers less ambiguously, we will rename the <code>preferred_id</code> field to <code>namespace</code>. The legacy field will still be supported and deprecated in a future release.</p>"},{"location":"learn/quickstart/#biocypher-api-documentation","title":"BioCypher API documentation","text":"<p>BioCypher is instantiated using the <code>BioCypher()</code> class, which can be called without arguments, given that the configuration files are either present in the working directory, or the pipeline should be run with default settings.</p> <pre><code>from biocypher import BioCypher\nbc = BioCypher()\n</code></pre> <p>BioCypher's main functionality is writing the graph (nodes and edges) to a database or files for database import. We exemplarise this using the Neo4j output format, writing CSV files formatted for the Neo4j admin import. In this example, <code>node_generator()</code> and <code>edge_generator()</code> are the adapter functions that yield nodes and edges, respectively (see above).</p> <pre><code>bc.write_nodes(node_generator())\nbc.write_edges(edge_generator())\n</code></pre> <p>Node and edge generators can contain arbitrarily many types of nodes and edges, which will be mapped via the schema configuration and sorted by BioCypher. One instance of the BioCypher class keeps track of the nodes and edges that have been written to the database, so that multiple calls to <code>write_nodes()</code> and <code>write_edges()</code> will not lead to duplicate entries in the database.</p> <p>For on-line writing to a database or a Pandas dataframe, we use the functions with <code>add</code> instead of <code>write</code>. For instance, to add nodes and edges to a Pandas dataframe, we can use:</p> <pre><code>bc.add_nodes(node_generator())\nbc.add_edges(edge_generator())\n</code></pre> <p>To retrieve the dataframe once all entities are in the graph, we can call <code>to_df()</code>:</p> <pre><code>df = bc.to_df()\n</code></pre> <p>For more information on the usage of these functions, please refer to the Tutorial section and the full API documentation.</p>"},{"location":"learn/quickstart/#the-biocypher-configuration-yaml-file","title":"The Biocypher configuration YAML file","text":"<p>Most of the configuration options for BioCypher can and should be specified in the configuration YAML file, <code>biocypher_config.yaml</code>. While BioCypher comes with default settings (the ones you can see in the Configuration section), we can override them by specifying the desired settings in the local configuration in the root or the <code>config</code> directory of the project. The primary BioCypher settings are found in the top-level entry <code>biocypher</code>. For instance, you can select your output format (<code>dbms</code>) and output path, the location of the schema configuration file, and the ontology to be used.</p> biocypher_config.yaml<pre><code>biocypher:\n  dbms: postgresql\n  output_path: postgres_out/\n  schema_config: config/schema-config.yaml\n  head_ontology:\n    url: https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl\n    root_node: entity\n</code></pre> <p>You can currently select between <code>postgresql</code>, <code>neo4j</code>, <code>rdf</code> (beta), and <code>arangodb</code> (beta) as your output format; more options will be added in the future. The <code>output_path</code> is relative to your working directory, as is the schema-config path. The <code>ontology</code> should be specified as a (preferably persistent) URL to the ontology file, and a <code>root_node</code> to specify the node from which the ontology should be traversed.  We recommend using a URL that specifies the exact version of the ontology, as in the example above.</p>"},{"location":"learn/quickstart/#dbms-specific-settings","title":"DBMS-specific settings","text":"<p>In addition to the general settings, you can specify settings specific to each DBMS (database management system) in the configuration file under the <code>postgresql</code>, <code>arangodb</code>, <code>rdf</code>, or <code>neo4j</code> entry. For instance, you can specify the database name, the host, the port, and the credentials for your database. You can also set delimiters for the entities and arrays in your import files. For a list of all settings, please refer to the Configuration section.</p> biocypher_config.yaml<pre><code>neo4j:\n  database_name: biocypher\n  uri: neo4j://localhost:7687\n  user: neo4j\n  password: neo4j\n  delimiter: ','\n</code></pre>"},{"location":"learn/quickstart/#additional-resources","title":"Additional Resources","text":"<ul> <li>BioCypher API Reference</li> <li>BioCypher Configuration Reference</li> <li>BioCypher Schema Reference</li> </ul>"},{"location":"learn/explanation/","title":"Index","text":"<p>Under Construction</p> <p>This section is currently under construction.</p>"},{"location":"learn/explanation/#purpose","title":"Purpose","text":"<p>The purpose of this documentation is to provide a comprehensive guide to understanding and working with knowledge graphs within the context of BioCypher.  It aims to explain the fundamental concepts of knowledge graphs and how they are represented in BioCypher.</p>"},{"location":"learn/explanation/#catalog","title":"Catalog","text":"<ul> <li>About Adapters</li> <li>About Ontologies</li> </ul>"},{"location":"learn/explanation/about-documentation/","title":"About the structure: tutorials, how-to guides, technical reference and explanations","text":"<p>Our documentation follows the \"Diataxis documentation framework\" proposed by Daniel Procida. This framework divide the documentation universe in four parts that represent different purposes or functions. They are: Tutorials, How-to guides, Technical Reference and Explanations.</p>"},{"location":"learn/explanation/about-documentation/#brief-summary","title":"Brief Summary:","text":"<p>Table 1. Diataxis documentation framework (extracted from link)</p> Tutorials How-to Guides Reference Explanation what they do introduce, educate, lead guide state, describe, inform explain, clarify, discuss answers the question Can you teach me to...? How do I...? \"What is...?\" \"Why...?\" oriented to learning a goal information understanding purpose to provide a learning experience to help achieve a particular goal to describe the machinery to illuminate a topic form a lesson a series of steps dry description discursive explanation analogy teaching a small child how to cook a recipe in a cookery book a reference encyclopedia article an article on culinary social history"},{"location":"learn/explanation/adapters/","title":"Adapters","text":""},{"location":"learn/explanation/adapters/#introduction","title":"Introduction","text":"<p>BioCypher is a modular framework, with the main purpose of avoiding redundant maintenance work for maintainers of secondary resources and end users alike. To achieve this, we use a collection of reusable \u201cadapters\u201d for the different sources of biomedical knowledge as well as for different ontologies. To see whether your favourite resource is already supported or currently in development, or if you would like to contribute to the development of a new adapter, please refer to this GitHub projects view (check the tabs for different views) or the meta-graph instance.</p> <p>Note</p> <p>We are currently working on adapter documentation, so the collection in the GitHub Projects view may be less than complete. Please get in touch if you want to make sure that your favourite resource is supported.</p> <ul> <li> <p> Adapter Tutorial</p> <p>For more information on developing your own adapters, please refer to this tutorial:</p> <p> To the Adapter Tutorial</p> </li> </ul> <p>The project view is built from issues in the BioCypher GitHub repository, which carry <code>Fields</code> (a GitHub Projects-specific attribute) to describe their category and features. In detail, these are as follows:</p> <ul> <li> <p><code>Component Type</code>: This refers to the class of component and can be one of <code>Adapter</code>, <code>Ontology</code>, or <code>Pipeline</code>.</p> </li> <li> <p><code>Adapter Granularity</code>: This is only applicable to adapters and can be either <code>Primary</code> (denoting an atomic resource that is represented by the adapter) or <code>Secondary</code> (denoting a composite resource, often pre-harmonised).</p> </li> <li> <p><code>Adapter Input Format</code> provides a drop-down menu of the different formats that can be ingested, such as <code>Flat File</code>, <code>API</code>, or <code>OWL</code>. Select the one that applies to the resource.</p> </li> <li> <p><code>Resource URL</code>: A free-text field to provide a link to the resource, also used for identification purposes.</p> </li> <li> <p><code>Resource Type</code>: Currently only <code>Database</code> or <code>Ontology</code>, but more granular reporting is planned.</p> </li> <li> <p><code>Data Type</code> provides a drop-down menu of the different data types that can be ingested, such as <code>Proteomics</code>, <code>Genetics</code>, or <code>Clinical</code>. Select the one that applies to the resource. This field primarily makes sense for primary adapters, but is interesting information particularly for the pipelines that use the adapters. For that reason, when building the meta-graph (see below), we propagate this information from the adapters to the pipelines.</p> </li> </ul> <p>Caution</p> <p>There is currently one type of meta-information that needs to be provided via free-text annotation in the text body of the issue: the links of pipelines to the input adapters and ontologies they use. For the meta-graph pipeline to work correctly, this information needs to be provided in the issue of the pipeline, in a line that starts with <code>Uses:</code>, followed by a space-separated list of issue numbers representing the used components.</p> <p>To make this annotation less error-prone, we use the auto-completion GitHub provides for referencing issues. Typing a <code>#</code> character and then a few characters of the title of the issue to be linked to the pipeline will show a list of possible matches. Select the correct one and the issue number will be inserted automatically.</p> <p>The meta-graph pipeline extracts this information and uses it to build the meta-graph described below.</p>"},{"location":"learn/explanation/adapters/#biocypher-meta-graph","title":"BioCypher meta-graph","text":"<ul> <li> <p> Repo with Docker setup</p> <p>The meta-graph is built from the GitHub API using a BioCypher pipeline.</p> <p> To the BioCypher Meta-graph</p> </li> </ul> <p>We have built a BioCypher pipeline (from the template repository) that fetches information about all adapters from the BioCypher GitHub repository via the GitHub API and builds a graph of all adapters and their dependencies.  Browsing this graph can give an overview of the current state of the adapters supported by BioCypher and the pipelines they are used in. The graph can be built locally by cloning the repository and running the pipeline using <code>docker compose up</code>. The graph is then available at <code>localhost:7474/browser/</code> in the Neo4j Browser.</p> <p>If you're unfamiliar with Neo4j, you can use the following Cypher query to retrieve an overview of all graph contents:</p> <pre><code>MATCH (n)\nRETURN n\n</code></pre> <p>For more information on how to use the graph, please refer to the Neo4j documentation.</p>"},{"location":"learn/explanation/basics-knowledge-graphs/","title":"Basics of Knowledge Graphs","text":"<ul> <li> <p>Nodes</p> </li> <li> <p>Edges</p> </li> <li> <p>Operations</p> </li> </ul>"},{"location":"learn/explanation/ontologies/","title":"Handling Ontologies","text":"<p>BioCypher relies on ontologies to ground the knowledge graph contents in biology. This has the advantages of providing machine readability and therefore automation capabilities as well as making working with BioCypher accessible to biologically oriented researchers. However, it also means that BioCypher requires a certain amount of knowledge about ontologies and how to use them. We try to make dealing with ontologies as easy as possible, but some basic understanding is required. In the following we will cover the basics of ontologies and how to use them in BioCypher.</p>"},{"location":"learn/explanation/ontologies/#what-is-an-ontology","title":"What is an ontology?","text":"<p>An ontology is a formal representation of a domain of knowledge. It is a hierarchical structure of concepts and relations. The concepts are organized into a hierarchy, where each concept is a subclass of a more general concept. For instance, a wardrobe is a subclass of a piece of furniture. Individual wardrobes, such as yours or mine, are instances of the concept wardrobe, and as such would be represented as Wardrobe nodes in a knowledge graph. In BioCypher, these nodes would additionally inherit the PieceOfFurniture label from the ontological hierarchy of things.</p> <p>Note</p> <p>Why is the class called piece of furniture but the label is PieceOfFurniture?</p> <p>The Biolink model uses two different case notations for its labels: the \"internal\" designation of classes is in lower sentence case (\"protein\", \"pairwise molecular interaction\"), while the \"external\" designation is in PascalCase (\"Protein\", \"PairwiseMolecularInteraction\"). BioCypher uses the same paradigm: in most cases (input, schema configuration, internally), the lower sentence case is used, while in the output (Neo4j labels, file system names) the PascalCase is more suitable; Neo4j labels and system file names don't deal well with spaces and special characters. Therefore, we check the output file names for their compliance with the Neo4j naming rules. All non compliant characters are removed from the file name (e.g. if the ontology class is called \"desk (piece of furniture)\", the brackets would be removed and the file name will be \"DeskPieceOfFurniture\"). We also remove the \"biolink:\" CURIE prefix for use in file names and Neo4j labels.</p> <p>The relations between concepts can also be organized into a hierarchy. In the specific case of a Neo4j graph, however, relationships cannot possess multiple labels; therefore, if concept inheritance is desired for relationships, they need to be \"reified\", i.e., turned into nodes. BioCypher provides a simple way of converting edges to nodes and vice versa (using the <code>represented_as</code> field). For a more in-depth explanation of ontologies, we recommend this introduction.</p>"},{"location":"learn/explanation/ontologies/#how-biocypher-uses-ontologies","title":"How BioCypher uses ontologies","text":"<p>BioCypher is agnostic to the choice of ontology. Practically, we have built our initial projects around the Biolink model, because it provides a large but shallow collection of concepts that are relevant to the biomedical domain. Other examples of generalist ontologies are the Experimental Factor Ontology and the Basic Formal Ontology. To account for the specific requirements of expert systems, it is possible to use multiple ontologies in the same project. For instance, one might want to extend the rather basic classes relating to molecular interactions in Biolink (the most specific being <code>pairwise molecular interaction</code>) with more specific classes from a more domain-specific ontology, such as the EBI molecular interactions ontology (PSI-MI). A different project may need to define very specific genetics concepts, and thus extend the Biolink model at the terminal node <code>sequence variant</code> with the corresponding subtree of the Sequence Ontology. The OBO Foundry and the BioPortal collect many such specialised ontologies.</p> <p>The default format for ingesting ontology definitions into BioCypher is the Web Ontology Language (OWL); BioCypher can read <code>.owl</code>, <code>.rdf</code>, and <code>.ttl</code> files. The preferred way to specify the ontology or ontologies to be used in a project is to specify them in the biocypher configuration file (<code>biocypher_config.yaml</code>). This file is used to specify the location of the ontology files, as well as the root node of the main (\"head\") ontology and join nodes as fusion points for all \"tail\" ontologies. For more info, see the section on hybridising ontologies.</p>"},{"location":"learn/explanation/ontologies/#visualising-ontologies","title":"Visualising ontologies","text":"<p>BioCypher provides a simple way of visualising the ontology hierarchy. This is useful for debugging and for getting a quick overview of the ontology and which parts are actually used in the knowledge graph to be created. Depending on your use case you can either visualise the parts of the ontology used in the knowledge graph (sufficient for most use cases) or the full ontology. If the used ontology is more complex and contains multiple inheritance please refer to the section on visualising complex ontologies.</p>"},{"location":"learn/explanation/ontologies/#visualise-only-the-parts-of-the-ontology-used-in-the-knowledge-graph","title":"Visualise only the parts of the ontology used in the knowledge graph","text":"<p>To get an overview of the structure of our project, we can run the following command via the interface:</p> Visualising the ontology hierarchy<pre><code>from biocypher import BioCypher\nbc = BioCypher(\n    offline=True,  # no need to connect or to load data\n    schema_config_path=\"tutorial/06_schema_config.yaml\",\n)\nbc.show_ontology_structure()\n</code></pre> <p>This will build the ontology scaffold and print a tree visualisation of its hierarchy to the console using the treelib library. You can see this in action in tutorial part 6 (<code>tutorial/06_relationships.py</code>). The output will look something like this:</p> <pre><code>Showing ontology structure, based on Biolink 3.0.3:\nentity\n\u251c\u2500\u2500 association\n\u2502   \u2514\u2500\u2500 gene to gene association\n\u2502       \u2514\u2500\u2500 pairwise gene to gene interaction\n\u2502           \u2514\u2500\u2500 pairwise molecular interaction\n\u2502               \u2514\u2500\u2500 protein protein interaction\n\u251c\u2500\u2500 mixin\n\u2514\u2500\u2500 named thing\n    \u2514\u2500\u2500 biological entity\n        \u2514\u2500\u2500 polypeptide\n            \u2514\u2500\u2500 protein\n                \u251c\u2500\u2500 entrez.protein\n                \u251c\u2500\u2500 protein isoform\n                \u2514\u2500\u2500 uniprot.protein\n</code></pre> <p>Note</p> <p>BioCypher will only show the parts of the ontology that are actually used in the knowledge graph with the exception of intermediary nodes that are needed to build a complete tree. For instance, the <code>protein</code> class is linked to the root class <code>entity</code> via <code>polypeptide</code>, <code>biological entity</code>, and <code>named thing</code>, all of which are not part of the input data.</p>"},{"location":"learn/explanation/ontologies/#visualise-the-full-ontology","title":"Visualise the full ontology","text":"<p>If you want to see the complete ontology tree, you can call <code>show_ontology_structure</code> with the parameter <code>full=True</code>.</p> Visualising the full ontology hierarchy<pre><code>from biocypher import BioCypher\nbc = BioCypher(\n    offline=True,  # no need to connect or to load data\n    schema_config_path=\"tutorial/06_schema_config.yaml\",\n)\nbc.show_ontology_structure(full=True)\n</code></pre>"},{"location":"learn/explanation/ontologies/#visualise-complex-ontologies","title":"Visualise complex ontologies","text":"<p>Not all ontologies can be easily visualised as a tree, such as ontologies with multiple inheritance, where classes in the ontology can have multiple parent classes. This violates the definition of a tree, where each node can only have one parent node. Consequently, ontologies with multiple inheritance cannot be visualised as a tree.</p> <p>BioCypher can still handle these ontologies, and you can call <code>show_ontology_structure()</code> to get a visualisation of the ontology. However, each ontology class will only be added to the hierarchy tree once (a class with multiple parent classes is only placed under one parent in the hierarchy tree). Since this will occur the first time the class is seen, the ontology class might not be placed where you would expect it. This only applies to the visualisation; the underlying ontology is still correct and contains all ontology classes and their relationships.</p> <p>Note</p> <p>When calling <code>show_ontology_structure()</code>, BioCypher automatically checks if the ontology contains multiple inheritance and logs a warning message if so.</p> <p>If you need to get a visualisation of the ontology with multiple inheritance, you can call <code>show_ontology_structure()</code> with the parameter <code>to_disk=/some/path/where_to_store_the_file</code>. This creates a <code>GraphML</code> file and stores it at the specified location.</p>"},{"location":"learn/explanation/ontologies/#using-ontologies-plain-biolink","title":"Using ontologies: plain Biolink","text":"<p>BioCypher maps any input data to the underlying ontology; in the basic case, the Biolink model. This mapping is defined in the schema configuration (<code>schema_config.yaml</code>, see also here). In the simplest case, the representation of a concept in the knowledge graph to be built and the Biolink model class representing this concept are synonymous. For instance, the concept protein is represented by the Biolink class protein. To introduce proteins into the knowledge graph, one would simply define a node constituent with the class label protein. This is the mechanism we implicitly used for proteins in the basic tutorial (part 1); to reiterate:</p> schema_config.yaml<pre><code>protein:\n  represented_as: node\n  # ...\n</code></pre>"},{"location":"learn/explanation/ontologies/#model-extensions","title":"Model extensions","text":"<p>There are multiple reasons why a user might want to modify the basic model of the ontology or ontologies used. A class that is relevant to the user's task might be missing (Explicit inheritance). A class might not be granular enough, and the user would like to split it into subclasses based on distinct inputs (Implicit inheritance). For some very common use cases, we recommend going one step further and, maybe after some testing using the above \"soft\" model extensions, proposing the introduction of a new class to the model itself. For instance, Biolink is an open source community project, and new classes can be requested by opening an issue or filing a pull request directly on the Biolink model GitHub repository. Similar mechanisms apply for OBO Foundry ontologies.</p> <p>BioCypher provides further methods for ontology manipulation. The name of a class of the model may be too unwieldy for the use inside the desired knowledge graph, and the user would like to introduce a synonym/alias (Synonyms). Finally, the user might want to extend the basic model with another, more specialised ontology (Hybridising ontologies).</p>"},{"location":"learn/explanation/ontologies/#explicit-inheritance","title":"Explicit inheritance","text":"<p>Explicit inheritance is the most straightforward way of extending the basic model. It is also the most common use case. For instance, the Biolink model does not contain a class for <code>protein isoform</code>, and neither does it contain a relationship class for <code>protein protein interaction</code>, both of which we have already used in the basic tutorial. Since protein isoforms are specific types of protein, it makes sense to extend the existing Biolink model class <code>protein</code> with the concept of protein isoforms. To do this, we simply add a new class <code>protein isoform</code> to the schema configuration, and specify that it is a subclass of <code>protein</code> using the (optional) <code>is_a</code> field:</p> schema_config.yaml<pre><code>protein isoform:\n  is_a: protein\n  represented_as: node\n  # ...\n</code></pre> <p>Explicit inheritance can also be used to introduce new relationship classes. However, if the output is a Neo4j graph, these relationships must be represented as nodes to provide full functionality, since edges do not allow multiple labels. This does not mean that explicit inheritance cannot be used in edges; it is even recommended to do so to situate all components of the knowledge graph in the ontological hierarchy. However, to have the ancestry represented in the resulting Neo4j graph DB, multiple labels are required. For instance, we have already used the <code>protein protein interaction</code> relationship in the basic tutorial (part 6), making it a child of the Biolink model class <code>pairwise molecular interaction</code>. To reiterate:</p> schema_config.yaml<pre><code>protein protein interaction:\n  is_a: pairwise molecular interaction\n  represented_as: node\n  # ...\n</code></pre> <p>The <code>is_a</code> field can be used to specify multiple inheritance, i.e., multiple ancestor classes and their direct parent-child relationships can be created by specifying multiple classes (as a list) in the <code>is_a</code> field. For instance, if we wanted to further extend the protein-protein interaction with a more specific <code>enzymatic interaction</code> class, we could do so as follows:</p> schema_config.yaml<pre><code>enzymatic interaction:\n  is_a: [protein protein interaction, pairwise molecular interaction]\n  represented_as: node\n  # ...\n</code></pre> <p>Note</p> <p>To create this multiple inheritance chain, we do not require the creation of a <code>protein protein interaction</code> class as shown above; all intermediary classes are automatically created by BioCypher and inserted into the ontological hierarchy. To obtain a continuous ontology tree, the target class (i.e., the last in the list) must be a real Biolink model class.</p>"},{"location":"learn/explanation/ontologies/#implicit-inheritance","title":"Implicit inheritance","text":"<p>The base model (in the standard case, Biolink) can also be extended without specifying an explicit <code>is_a</code> field. This \"implicit\" inheritance happens when a class has multiple input labels that each refer to a distinct preferred identifier. In other words, if both the <code>input_label</code> and the <code>preferred_id</code> fields of a schema configuration class are lists, BioCypher will automatically create a subclass for each of the preferred identifiers. This is demonstrated in part 3 of the basic tutorial.</p> <p>Caution</p> <p>If only the <code>input_label</code> field - but not the <code>preferred_id</code> field - is a list, BioCypher will merge the inputs instead. This is useful for cases where different input streams should be unified under the same class label. See part 2 of the basic tutorial for more information.</p> <p>To make this more concrete, let's consider the example of <code>pathway</code> annotations. There are multiple projects that provide pathway annotations, such as Reactome and Wikipathways, and, in contrast to proteins, pathways are not easily mapped one-to-one. For classes where mapping is difficult or even impossible, we can use implicit subclassing instead. The Biolink model contains a <code>pathway</code> class, which we can use as a parent class of the Reactome and Wikipathways classes; we simply need to provide the pathways as two separate inputs with their own labels (e.g., \"react\" and \"wiki\"), and specify a corresponding list of preferred identifiers in the <code>preferred_id</code> field:</p> schema_config.yaml<pre><code>pathway:\n  represented_as: node\n  preferred_id: [reactome, wikipathways]\n  input_label: [react, wiki]\n  # ...\n</code></pre> <p>This will prompt BioCypher to create two subclasses of <code>pathway</code>, one for each input, and to map the input data to these subclasses. In the resulting knowledge graph, the Reactome and Wikipathways pathways will be represented as distinct classes by prepending the preferred identifier to the class label: <code>Reactome.Pathway</code> and <code>Wikipathways.Pathway</code>. By virtue of BioCypher's multiple labelling paradigm, those nodes will also inherit the <code>Pathway</code> class label as well as all parent labels and mixins of <code>Pathway</code> (<code>BiologicalProcess</code>, etc.). This allows us to query the graph for all <code>Pathway</code> nodes as well as for specific datasets depending on the desired granularity.</p> <p>Note</p> <p>This also works for relationships, but in this case, not the preferred identifiers but the sources (defined in the <code>source</code> field) are used to create the subclasses.</p>"},{"location":"learn/explanation/ontologies/#synonyms","title":"Synonyms","text":"<p>Note: Tutorial Files</p> <p>The code for this tutorial can be found at <code>tutorial/07__synonyms.py</code>. Schema files are at <code>tutorial/07_schema_config.yaml</code>, configuration in <code>tutorial/07_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p> <p>In some cases, an ontology may contain a biological concept, but the name of the concept does for some reason not agree with the users desired knowledge graph structure. For instance, the user may not want to represent protein complexes in the graph as <code>macromolecular complex</code> nodes due to ease of use and/or readability criteria and rather call these nodes <code>complex</code>. In such cases, the user can introduce a synonym for the ontology class. This is done by selecting another, more desirable name for the respective class(es) and specifying the <code>synonym_for</code> field in their schema configuration. In this case, as we would like to represent protein complexes as <code>complex</code> nodes, we can do so as follows:</p> schema_config.yaml<pre><code>complex:\n  synonym_for: macromolecular complex\n  represented_as: node\n  # ...\n</code></pre> <p>Importantly, BioCypher preserves these mappings to enable compatibility between different structural instantiations of the ontology (or combination of ontologies). All entities that are mapped to ontology classes in any way can be harmonised even between different types of concrete representations.</p> <p>Note</p> <p>It is essential that the desired class name is used as the main class key in the schema configuration, and the ontology class name is given in the <code>synonym_for</code> field. The name given in the <code>synonym_for</code> field must be an existing class name (in this example, a real Biolink class).</p> <p>We can visualise the structure of the ontology as we have before. Instead of using <code>bc.show_ontology_structure()</code> however, we can use the <code>bc.summary()</code> method to show the structure and simultaneously check for duplicates and missing labels. This is useful for debugging purposes, and we can see that the import was completed without encountering duplicates, and all labels in the input are accounted for in the schema configuration. We also observe in the tree that the <code>complex</code> class is now a synonym for the <code>macromolecular complex</code> class (their being synonyms indicated as an equals sign):</p> <pre><code>Showing ontology structure based on https://raw.githubusercontent.com/biolink/biolink-model/v3.2.1/biolink-model.owl.ttl\nentity\n\u251c\u2500\u2500 association\n\u2502   \u2514\u2500\u2500 gene to gene association\n\u2502       \u2514\u2500\u2500 pairwise gene to gene interaction\n\u2502           \u2514\u2500\u2500 pairwise molecular interaction\n\u2502               \u2514\u2500\u2500 protein protein interaction\n\u2514\u2500\u2500 named thing\n    \u2514\u2500\u2500 biological entity\n        \u251c\u2500\u2500 complex = macromolecular complex\n        \u2514\u2500\u2500 polypeptide\n            \u2514\u2500\u2500 protein\n                \u251c\u2500\u2500 entrez.protein\n                \u251c\u2500\u2500 protein isoform\n                \u2514\u2500\u2500 uniprot.protein\n</code></pre>"},{"location":"learn/explanation/ontologies/#hybridising-ontologies","title":"Hybridising ontologies","text":"<p>A broad, general ontology is a useful tool for knowledge representation, but often the task at hand requires more specific and granular concepts. In such cases, it is possible to hybridise the general ontology with a more specific one. For instance, there are many different types of sequence variants in biology, but Biolink only provides a generic \"sequence variant\" class (and it clearly exceeds the scope of Biolink to provide granular classes for all thinkable cases). However, there are many specialist ontologies, such as the Sequence Ontology (SO), which provides a more granular representation of sequence variants, and MONDO, which provides a more granular representation of diseases.</p> <p>To hybridise the Biolink model with the SO and MONDO, we can use the generic ontology adapter class of BioCypher by providing \"tail ontologies\" as dictionaries consisting of an OWL format ontology file and a set of nodes, one in the head ontology (which by default is Biolink), and one in the tail ontology. Each of the tail ontologies will then be joined to the head ontology to form the hybridised ontology at the specified nodes. It is up to the user to make sure that the concept at which the ontologies shall be joined makes sense as a point of contact between the ontologies; ideally, it is the exact same concept.</p> <p>Hint</p> <p>If the concept does not exist in the head ontology, but is a feasible child class of an existing concept, you can set the <code>merge_nodes</code> option to <code>False</code> to prevent the merging of head and tail join nodes, but instead adding the tail join node as a child of the head join node you have specified. For instance, in the example below, we merge <code>sequence variant</code> from Biolink and <code>sequence_variant</code> from Sequence Ontology into a single node, but we add the MONDO subtree of <code>human disease</code> as a child of <code>disease</code> in Biolink.</p> <p><code>merge_nodes</code> is set to <code>True</code> by default, so there is no need to specify it in the configuration file if you want to merge the nodes.</p> <p>The ontology adapter also accepts any arbitrary \"head ontology\" as a base ontology, but if none is provided, the Biolink model is used as the default head ontology. However, it is strongly recommended to explicitly specify your desired ontology version here. These options can be provided to the BioCypher interface as parameters, or as options in the BioCypher configuration file, which is the preferred method for transparency reasons:</p> Using biocypher_config.yaml<pre><code># ...\n\nbiocypher:  # biocypher settings\n\n  # Ontology configuration\n  head_ontology:\n    url: https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl\n    root_node: entity\n\n  tail_ontologies:\n\n    so:\n      url: data/so.owl\n      head_join_node: sequence variant\n      tail_join_node: sequence_variant\n\n    mondo:\n      url: http://purl.obolibrary.org/obo/mondo.owl\n      head_join_node: disease\n      tail_join_node: human disease\n      merge_nodes: false\n\n# ...\n</code></pre> <p>Note</p> <p>The <code>url</code> parameter can be either a local path or a URL to a remote resource.</p> <p>If you need to pass the ontology configuration programmatically, you can do so as follows at BioCypher interface instantiation:</p> Programmatic usage<pre><code>bc = BioCypher(\n    # ...\n\n    head_ontology={\n      'url': 'https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl',\n      'root_node': 'entity',\n    },\n\n    tail_ontologies={\n        'so':\n            {\n                'url': 'test/ontologies/so.owl',\n                'head_join_node': 'sequence variant',\n                'tail_join_node': 'sequence_variant',\n            },\n        'mondo':\n            {\n                'url': 'test/ontologies/mondo.owl',\n                'head_join_node': 'disease',\n                'tail_join_node': 'human disease',\n                'merge_nodes': False,\n            }\n    },\n\n    # ...\n)\n</code></pre>"},{"location":"learn/guides/","title":"Index","text":"<p>Under Construction</p> <p>This section is currently under construction.</p> <ol> <li> <p>How do I create a standalone Docker image for a BioCypher KG?</p> </li> <li> <p>How do I use or create an Adapter to read data?</p> </li> <li> <p>How do I combine data from different Adapters?</p> </li> <li> <p>How do I create a graph from a CSV file</p> </li> <li> <p>How do I convert a graph to a Pandas Dataframe?</p> </li> <li> <p>How do I create a complete pipeline (from CSV file to Neo4j)</p> </li> <li> <p>How do I configure BioCypher</p> </li> <li> <p>How do I define a Schema for my Graph</p> </li> </ol>"},{"location":"learn/guides/htg001_standalone_docker_biocypher/","title":"Standalone Docker Image","text":"<p>In order to build a standalone Docker image for a BioCypher KG, you only need small modifications to the Docker image of the template. We will render the data that ususally is stored in the <code>biocypher_neo4j_volume</code> to our local disk by exchanging <code>biocypher_neo4j_volume</code> with a local directory, <code>./biocypher_neo4j_volume</code>. Then, we use a Dockerfile to build an image that contains the final database. This image can be used to deploy the database anywhere, without the need to run the BioCypher code. This process is demonstrated in the drug-interactions example repository.</p> <ol> <li> <p>Clone the example repository</p> <pre><code>git clone https://github.com/biocypher/drug-interactions.git\ncd drug-interactions\n</code></pre> </li> <li> <p>Attach volumes to disk by modifying the docker-compose.yml. In the example repository, we have created a dedicated compose file for the standalone image. You can see the differences between the standard and standalone compose files here. IMPORTANT: only run the standalone compose file once, as the data in the <code>./biocypher_neo4j_volume</code> directory is persistent and interferes with subsequent runs. If you want to run it again, you need to delete the <code>./biocypher_neo4j_volume</code> directory.</p> </li> <li> <p>Run the standalone compose file. This will create the <code>./biocypher_neo4j_volume</code> directory and store the data in it. You can stop the container after the database has been created.</p> <pre><code>docker compose -f docker-compose-local-disk.yml up -d\ndocker compose -f docker-compose-local-disk.yml down\n</code></pre> </li> <li> <p>Create standalone <code>Dockerfile</code> (example here):</p> <pre><code># Dockerfile\nFROM neo4j:4.4-enterprise\nCOPY ./biocypher_neo4j_volume /data\nRUN chown -R 7474:7474 /data\nEXPOSE 7474\nEXPOSE 7687\n</code></pre> </li> <li> <p>Build the standalone image.</p> <pre><code>docker build -t drug-interactions:latest .\n</code></pre> <p>This image can be deployed anywhere, without the need to run the BioCypher code. For example, you can add it to a Docker Compose file (example here):</p> <pre><code># docker-compose.yml\nversion: '3.9'\nservices:\n[other services ...]\n\nbiocypher:\n    container_name: biocypher\n    image: biocypher/drug-interactions:latest\n    environment:\n    NEO4J_dbms_security_auth__enabled: \"false\"\n    NEO4J_dbms_databases_default__to__read__only: \"false\"\n    NEO4J_ACCEPT_LICENSE_AGREEMENT: \"yes\"\n    ports:\n    - \"0.0.0.0:7474:7474\"\n    - \"0.0.0.0:7687:7687\"\n</code></pre> </li> </ol>"},{"location":"learn/tutorials/pandas_tutorial/","title":"Example Notebook: BioCypher and Pandas","text":"<p>Tip: You can run the tutorial interactively in Google Colab.</p> <p>While BioCypher was designed as a graph-focused framework, due to commonalities in bioinformatics workflows, BioCypher also supports Pandas DataFrames. This allows integration with methods that use tabular data, such as machine learning and statistical analysis, for instance in the scVerse framework.</p> <p>To run this tutorial interactively, you will first need to install perform some setup steps specific to running on Google Colab. You can collapse this section and run the setup steps with one click, as they are not required for the explanation of BioCyper's functionality. You can of course also run the steps one by one, if you want to see what is happening. The real tutorial starts with section 1, \"Adding data\" (do not follow this link on colab, as you will be taken back to the website; please scroll down instead).</p> In\u00a0[1]: Copied! <pre>!pip install biocypher\n</pre> !pip install biocypher <pre>Requirement already satisfied: biocypher in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (0.9.1)\nRequirement already satisfied: PyYAML&gt;=5.0 in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from biocypher) (6.0.2)\nRequirement already satisfied: appdirs in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from biocypher) (1.4.4)\nRequirement already satisfied: more_itertools in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from biocypher) (10.6.0)\nRequirement already satisfied: neo4j-utils==0.0.7 in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from biocypher) (0.0.7)\nRequirement already satisfied: networkx&lt;4.0,&gt;=3.0 in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from biocypher) (3.4.2)\nRequirement already satisfied: pandas&lt;3.0.0,&gt;=2.0.1 in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from biocypher) (2.2.3)\nRequirement already satisfied: pooch&lt;2.0.0,&gt;=1.7.0 in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from biocypher) (1.8.2)\nRequirement already satisfied: rdflib&lt;7.0.0,&gt;=6.2.0 in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from biocypher) (6.3.2)\nRequirement already satisfied: stringcase&lt;2.0.0,&gt;=1.2.0 in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from biocypher) (1.2.0)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.65.0 in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from biocypher) (4.67.1)\nRequirement already satisfied: treelib==1.6.4 in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from biocypher) (1.6.4)\nRequirement already satisfied: colorlog in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from neo4j-utils==0.0.7-&gt;biocypher) (6.9.0)\nRequirement already satisfied: neo4j&lt;5.0,&gt;=4.4 in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from neo4j-utils==0.0.7-&gt;biocypher) (4.4.12)\nRequirement already satisfied: toml in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from neo4j-utils==0.0.7-&gt;biocypher) (0.10.2)\nRequirement already satisfied: six in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from treelib==1.6.4-&gt;biocypher) (1.17.0)\nRequirement already satisfied: numpy&gt;=1.22.4 in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from pandas&lt;3.0.0,&gt;=2.0.1-&gt;biocypher) (2.2.1)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from pandas&lt;3.0.0,&gt;=2.0.1-&gt;biocypher) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from pandas&lt;3.0.0,&gt;=2.0.1-&gt;biocypher) (2024.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from pandas&lt;3.0.0,&gt;=2.0.1-&gt;biocypher) (2024.2)\nRequirement already satisfied: platformdirs&gt;=2.5.0 in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from pooch&lt;2.0.0,&gt;=1.7.0-&gt;biocypher) (4.3.6)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from pooch&lt;2.0.0,&gt;=1.7.0-&gt;biocypher) (24.2)\nRequirement already satisfied: requests&gt;=2.19.0 in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from pooch&lt;2.0.0,&gt;=1.7.0-&gt;biocypher) (2.32.3)\nRequirement already satisfied: isodate&lt;0.7.0,&gt;=0.6.0 in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from rdflib&lt;7.0.0,&gt;=6.2.0-&gt;biocypher) (0.6.1)\nRequirement already satisfied: pyparsing&lt;4,&gt;=2.1.0 in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from rdflib&lt;7.0.0,&gt;=6.2.0-&gt;biocypher) (3.2.1)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from requests&gt;=2.19.0-&gt;pooch&lt;2.0.0,&gt;=1.7.0-&gt;biocypher) (3.4.1)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from requests&gt;=2.19.0-&gt;pooch&lt;2.0.0,&gt;=1.7.0-&gt;biocypher) (3.10)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from requests&gt;=2.19.0-&gt;pooch&lt;2.0.0,&gt;=1.7.0-&gt;biocypher) (2.3.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/slobentanzer/GitHub/tmp/biocypher/.venv/lib/python3.10/site-packages (from requests&gt;=2.19.0-&gt;pooch&lt;2.0.0,&gt;=1.7.0-&gt;biocypher) (2024.12.14)\n\n[notice] A new release of pip is available: 24.3.1 -&gt; 25.0.1\n[notice] To update, run: pip install --upgrade pip\n</pre> In\u00a0[2]: Copied! <pre>import yaml\nimport requests\nimport subprocess\n\nschema_path = \"https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/\"\n</pre> import yaml import requests import subprocess  schema_path = \"https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/\" In\u00a0[3]: Copied! <pre>!wget -O data_generator.py \"https://github.com/biocypher/biocypher/raw/main/tutorial/data_generator.py\"\n</pre> !wget -O data_generator.py \"https://github.com/biocypher/biocypher/raw/main/tutorial/data_generator.py\" <pre>--2025-03-07 20:32:24--  https://github.com/biocypher/biocypher/raw/main/tutorial/data_generator.py\nResolving github.com (github.com)... 140.82.121.4\nConnecting to github.com (github.com)|140.82.121.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/data_generator.py [following]\n--2025-03-07 20:32:24--  https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/data_generator.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7083 (6,9K) [text/plain]\nSaving to: \u2018data_generator.py\u2019\n\ndata_generator.py   100%[===================&gt;]   6,92K  --.-KB/s    in 0s      \n\n2025-03-07 20:32:24 (56,8 MB/s) - \u2018data_generator.py\u2019 saved [7083/7083]\n\n</pre> In\u00a0[4]: Copied! <pre>owner = \"biocypher\"\nrepo = \"biocypher\"\npath = \"tutorial\"  # The path within the repository (optional, leave empty for the root directory)\ngithub_url = \"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n\napi_url = github_url.format(owner=owner, repo=repo, path=path)\nresponse = requests.get(api_url)\n\n# Get list of yaml files from the repo\nfiles = response.json()\nyamls = []\nfor file in files:\n    if file[\"type\"] == \"file\":\n        if file[\"name\"].endswith(\".yaml\"):\n            yamls.append(file[\"name\"])\n           \n# wget all yaml files \nfor yaml in yamls:\n    url_path = schema_path + yaml\n    subprocess.run([\"wget\", url_path])\n</pre> owner = \"biocypher\" repo = \"biocypher\" path = \"tutorial\"  # The path within the repository (optional, leave empty for the root directory) github_url = \"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"  api_url = github_url.format(owner=owner, repo=repo, path=path) response = requests.get(api_url)  # Get list of yaml files from the repo files = response.json() yamls = [] for file in files:     if file[\"type\"] == \"file\":         if file[\"name\"].endswith(\".yaml\"):             yamls.append(file[\"name\"])             # wget all yaml files  for yaml in yamls:     url_path = schema_path + yaml     subprocess.run([\"wget\", url_path]) <pre>--2025-03-07 20:32:25--  https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/01_biocypher_config.yaml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 64 [text/plain]\nSaving to: \u201801_biocypher_config.yaml.3\u2019\n\n     0K                                                       100% 3,39M=0s\n\n2025-03-07 20:32:25 (3,39 MB/s) - \u201801_biocypher_config.yaml.3\u2019 saved [64/64]\n\n--2025-03-07 20:32:25--  https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/01_biocypher_config_pandas.yaml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 43 [text/plain]\nSaving to: \u201801_biocypher_config_pandas.yaml.3\u2019\n\n     0K                                                       100% 4,56M=0s\n\n2025-03-07 20:32:25 (4,56 MB/s) - \u201801_biocypher_config_pandas.yaml.3\u2019 saved [43/43]\n\n--2025-03-07 20:32:25--  https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/01_schema_config.yaml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 93 [text/plain]\nSaving to: \u201801_schema_config.yaml.3\u2019\n\n     0K                                                       100% 9,85M=0s\n\n2025-03-07 20:32:25 (9,85 MB/s) - \u201801_schema_config.yaml.3\u2019 saved [93/93]\n\n--2025-03-07 20:32:25--  https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/02_biocypher_config.yaml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 58 [text/plain]\nSaving to: \u201802_biocypher_config.yaml.3\u2019\n\n     0K                                                       100% 9,22M=0s\n\n2025-03-07 20:32:25 (9,22 MB/s) - \u201802_biocypher_config.yaml.3\u2019 saved [58/58]\n\n--2025-03-07 20:32:25--  https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/02_biocypher_config_pandas.yaml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 43 [text/plain]\nSaving to: \u201802_biocypher_config_pandas.yaml.3\u2019\n\n     0K                                                       100% 2,73M=0s\n\n2025-03-07 20:32:25 (2,73 MB/s) - \u201802_biocypher_config_pandas.yaml.3\u2019 saved [43/43]\n\n--2025-03-07 20:32:25--  https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/02_schema_config.yaml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 111 [text/plain]\nSaving to: \u201802_schema_config.yaml.3\u2019\n\n     0K                                                       100% 15,1M=0s\n\n2025-03-07 20:32:25 (15,1 MB/s) - \u201802_schema_config.yaml.3\u2019 saved [111/111]\n\n--2025-03-07 20:32:25--  https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/03_biocypher_config.yaml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 69 [text/plain]\nSaving to: \u201803_biocypher_config.yaml.3\u2019\n\n     0K                                                       100% 6,58M=0s\n\n2025-03-07 20:32:25 (6,58 MB/s) - \u201803_biocypher_config.yaml.3\u2019 saved [69/69]\n\n--2025-03-07 20:32:25--  https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/03_biocypher_config_pandas.yaml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 43 [text/plain]\nSaving to: \u201803_biocypher_config_pandas.yaml.3\u2019\n\n     0K                                                       100% 3,42M=0s\n\n2025-03-07 20:32:25 (3,42 MB/s) - \u201803_biocypher_config_pandas.yaml.3\u2019 saved [43/43]\n\n--2025-03-07 20:32:25--  https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/03_schema_config.yaml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 121 [text/plain]\nSaving to: \u201803_schema_config.yaml.3\u2019\n\n     0K                                                       100% 10,5M=0s\n\n2025-03-07 20:32:25 (10,5 MB/s) - \u201803_schema_config.yaml.3\u2019 saved [121/121]\n\n--2025-03-07 20:32:25--  https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/04_biocypher_config.yaml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 63 [text/plain]\nSaving to: \u201804_biocypher_config.yaml.3\u2019\n\n     0K                                                       100% 7,51M=0s\n\n2025-03-07 20:32:25 (7,51 MB/s) - \u201804_biocypher_config.yaml.3\u2019 saved [63/63]\n\n--2025-03-07 20:32:25--  https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/04_biocypher_config_pandas.yaml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 43 [text/plain]\nSaving to: \u201804_biocypher_config_pandas.yaml.3\u2019\n\n     0K                                                       100% 3,73M=0s\n\n2025-03-07 20:32:25 (3,73 MB/s) - \u201804_biocypher_config_pandas.yaml.3\u2019 saved [43/43]\n\n--2025-03-07 20:32:25--  https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/04_schema_config.yaml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 221 [text/plain]\nSaving to: \u201804_schema_config.yaml.3\u2019\n\n     0K                                                       100% 19,2M=0s\n\n2025-03-07 20:32:25 (19,2 MB/s) - \u201804_schema_config.yaml.3\u2019 saved [221/221]\n\n--2025-03-07 20:32:25--  https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/05_biocypher_config.yaml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 72 [text/plain]\nSaving to: \u201805_biocypher_config.yaml.3\u2019\n\n     0K                                                       100% 6,24M=0s\n\n2025-03-07 20:32:25 (6,24 MB/s) - \u201805_biocypher_config.yaml.3\u2019 saved [72/72]\n\n--2025-03-07 20:32:26--  https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/05_biocypher_config_pandas.yaml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 43 [text/plain]\nSaving to: \u201805_biocypher_config_pandas.yaml.3\u2019\n\n     0K                                                       100% 8,20M=0s\n\n2025-03-07 20:32:26 (8,20 MB/s) - \u201805_biocypher_config_pandas.yaml.3\u2019 saved [43/43]\n\n--2025-03-07 20:32:26--  https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/05_schema_config.yaml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 370 [text/plain]\nSaving to: \u201805_schema_config.yaml.3\u2019\n\n     0K                                                       100% 70,6M=0s\n\n2025-03-07 20:32:26 (70,6 MB/s) - \u201805_schema_config.yaml.3\u2019 saved [370/370]\n\n--2025-03-07 20:32:26--  https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/06_biocypher_config.yaml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 66 [text/plain]\nSaving to: \u201806_biocypher_config.yaml.3\u2019\n\n     0K                                                       100% 3,00M=0s\n\n2025-03-07 20:32:26 (3,00 MB/s) - \u201806_biocypher_config.yaml.3\u2019 saved [66/66]\n\n--2025-03-07 20:32:26--  https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/06_biocypher_config_pandas.yaml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 43 [text/plain]\nSaving to: \u201806_biocypher_config_pandas.yaml.3\u2019\n\n     0K                                                       100% 3,73M=0s\n\n2025-03-07 20:32:26 (3,73 MB/s) - \u201806_biocypher_config_pandas.yaml.3\u2019 saved [43/43]\n\n--2025-03-07 20:32:26--  https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/06_schema_config.yaml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 579 [text/plain]\nSaving to: \u201806_schema_config.yaml.3\u2019\n\n     0K                                                       100% 69,0M=0s\n\n2025-03-07 20:32:26 (69,0 MB/s) - \u201806_schema_config.yaml.3\u2019 saved [579/579]\n\n--2025-03-07 20:32:26--  https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/06_schema_config_pandas.yaml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 579 [text/plain]\nSaving to: \u201806_schema_config_pandas.yaml.3\u2019\n\n     0K                                                       100% 42,5M=0s\n\n2025-03-07 20:32:26 (42,5 MB/s) - \u201806_schema_config_pandas.yaml.3\u2019 saved [579/579]\n\n--2025-03-07 20:32:26--  https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/07_biocypher_config.yaml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 79 [text/plain]\nSaving to: \u201807_biocypher_config.yaml.3\u2019\n\n     0K                                                       100% 10,8M=0s\n\n2025-03-07 20:32:26 (10,8 MB/s) - \u201807_biocypher_config.yaml.3\u2019 saved [79/79]\n\n--2025-03-07 20:32:26--  https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/07_biocypher_config_pandas.yaml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 43 [text/plain]\nSaving to: \u201807_biocypher_config_pandas.yaml.3\u2019\n\n     0K                                                       100% 8,20M=0s\n\n2025-03-07 20:32:26 (8,20 MB/s) - \u201807_biocypher_config_pandas.yaml.3\u2019 saved [43/43]\n\n--2025-03-07 20:32:26--  https://raw.githubusercontent.com/biocypher/biocypher/main/tutorial/07_schema_config.yaml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 711 [text/plain]\nSaving to: \u201807_schema_config.yaml.3\u2019\n\n     0K                                                       100% 56,5M=0s\n\n2025-03-07 20:32:26 (56,5 MB/s) - \u201807_schema_config.yaml.3\u2019 saved [711/711]\n\n</pre> <p>Let's also define functions with which we can visualize those</p> In\u00a0[5]: Copied! <pre># helper function to print yaml files\nimport yaml\ndef print_yaml(file_path):\n    with open(file_path, 'r') as file:\n        yaml_data = yaml.safe_load(file)\n\n    print(\"--------------\")\n    print(yaml.dump(yaml_data, sort_keys=False, indent=4))\n    print(\"--------------\")\n</pre> # helper function to print yaml files import yaml def print_yaml(file_path):     with open(file_path, 'r') as file:         yaml_data = yaml.safe_load(file)      print(\"--------------\")     print(yaml.dump(yaml_data, sort_keys=False, indent=4))     print(\"--------------\") In\u00a0[6]: Copied! <pre># create a list of proteins to be imported\nfrom data_generator import Protein\nn_proteins = 3\nproteins = [Protein() for _ in range(n_proteins)]\n</pre> # create a list of proteins to be imported from data_generator import Protein n_proteins = 3 proteins = [Protein() for _ in range(n_proteins)] <p>Each protein in our simulated data has a UniProt ID, a label (\"uniprot_protein\"), and a dictionary of properties describing it. This is - purely by coincidence - very close to the input BioCypher expects (for nodes):</p> <ul> <li>a unique identifier</li> <li>an input label (to allow mapping to the ontology, see the second step below)</li> <li>a dictionary of further properties (which can be empty)</li> </ul> <p>These should be presented to BioCypher in the form of a tuple. To achieve this representation, we can use a generator function that iterates through our simulated input data and, for each entity, forms the corresponding tuple. The use of a generator allows for efficient streaming of larger datasets where required.</p> In\u00a0[7]: Copied! <pre>def node_generator(proteins):\n    for protein in proteins:\n        yield (\n            protein.get_id(),\n            protein.get_label(),\n            protein.get_properties(),\n        )\nentities = node_generator(proteins)\n</pre> def node_generator(proteins):     for protein in proteins:         yield (             protein.get_id(),             protein.get_label(),             protein.get_properties(),         ) entities = node_generator(proteins) <p>The concept of an adapter can become arbitrarily complex and involve programmatic access to databases, API requests, asynchronous queries, context managers, and other complicating factors. However, it always boils down to providing the BioCypher driver with a collection of tuples, one for each entity in the input data. For more info, see the section on Adapters.</p> <p>As descibed above, nodes possess:</p> <ul> <li>a mandatory ID,</li> <li>a mandatory label, and</li> <li>a property dictionary,</li> </ul> <p>while edges possess:</p> <ul> <li>an (optional) ID,</li> <li>two mandatory IDs for source and target,</li> <li>a mandatory label, and</li> <li>a property dictionary.</li> </ul> <p>How these entities are mapped to the ontological hierarchy underlying a BioCypher graph is determined by their mandatory labels, which connect the input data stream to the schema configuration. This we will see in the following section.</p> In\u00a0[8]: Copied! <pre>print_yaml('01_schema_config.yaml')\n</pre> print_yaml('01_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id: uniprot\n    input_label: uniprot_protein\n\n--------------\n</pre> <p>The first line (<code>protein</code>) identifies our entity and connects to the ontological backbone; here we define the first class to be represented in the graph. In the configuration YAML, we represent entities\u202f\u2014 similar to the internal representation of Biolink\u202f\u2014 in lower sentence case (e.g., \"small molecule\"). Conversely, for class names, in file names, and property graph labels, we use PascalCase instead (e.g., \"SmallMolecule\") to avoid issues with handling spaces. The transformation is done by BioCypher internally. BioCypher does not strictly enforce the entities allowed in this class definition; in fact, we provide several methods of extending the existing ontological backbone ad hoc by providing custom inheritance or hybridising ontologies. However, every entity should at some point be connected to the underlying ontology, otherwise the multiple hierarchical labels will not be populated. Following this first line are three indented values of the protein class.</p> <p>The second line (<code>represented_as</code>) tells BioCypher in which way each entity should be represented in the graph; the only options are <code>node</code> and <code>edge</code>. Representation as an edge is only possible when source and target IDs are provided in the input data stream. Conversely, relationships can be represented as both <code>node</code> or <code>edge</code>, depending on the desired output. When a relationship should be represented as a node, i.e., \"reified\", BioCypher takes care to create a set of two edges and a node in place of the relationship. This is useful when we want to connect the relationship to other entities in the graph, for example literature references.</p> <p>The third line (<code>preferred_id</code>) informs the uniqueness of represented entities by selecting an ontological namespace around which the definition of uniqueness should revolve. In our example, if a protein has its own uniprot ID, it is understood to be a unique entity. When there are multiple protein isoforms carrying the same uniprot ID, they are understood to be aggregated to result in only one unique entity in the graph. Decisions around uniqueness of graph constituents sometimes require some consideration in task-specific applications. Selection of a namespace also has effects in identifier mapping; in our case, for protein nodes that do not carry a uniprot ID, identifier mapping will attempt to find a uniprot ID given the other identifiers of that node. To account for the broadest possible range of identifier systems while also dealing with parsing of namespace prefixes and validation, we refer to the Bioregistry project namespaces, which should be preferred values for this field.</p> <p>Finally, the fourth line (<code>input_label</code>) connects the input data stream to the configuration; here we indicate which label to expect in the input tuple for each class in the graph. In our case, we expect \"uniprot_protein\" as the label for each protein in the input data stream; all other input entities that do not carry this label are ignored as long as they are not in the schema configuration.</p> In\u00a0[9]: Copied! <pre>from biocypher import BioCypher\nbc = BioCypher(\n    biocypher_config_path='01_biocypher_config_pandas.yaml',\n    schema_config_path='01_schema_config.yaml',\n)\n# Add the entities that we generated above to the graph\nbc.add(entities)\n</pre> from biocypher import BioCypher bc = BioCypher(     biocypher_config_path='01_biocypher_config_pandas.yaml',     schema_config_path='01_schema_config.yaml', ) # Add the entities that we generated above to the graph bc.add(entities) <pre>INFO -- This is BioCypher v0.9.1.\nINFO -- Logging into `biocypher-log/biocypher-20250307-203226.log`.\nINFO -- Running BioCypher with schema configuration from 01_schema_config.yaml.\nINFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> In\u00a0[10]: Copied! <pre># Print the graph as a dictionary of pandas DataFrame(s) per node label\nbc.to_df()[\"protein\"]\n</pre> # Print the graph as a dictionary of pandas DataFrame(s) per node label bc.to_df()[\"protein\"] Out[10]: node_id node_label sequence description taxon id preferred_id 0 F6M7Z7 protein IGQSELYWNAWQEWLDYDNFVQIRGIRPQMQIDEFGLDMIDTRAFTE Lorem ipsum mbgrm 9606 F6M7Z7 uniprot 1 O8Y0M7 protein SCEMHQTMEMQLFGSWHYQPIQILFEIYYWPVD Lorem ipsum mwpax 9606 O8Y0M7 uniprot 2 O5P9G9 protein GNFFNKHGYTTQKKDCFRPKMCPLLMCRRIWHGVSSVVWRGSCMP Lorem ipsum wcjeq 9606 O5P9G9 uniprot In\u00a0[11]: Copied! <pre>from data_generator import Protein, EntrezProtein\n</pre> from data_generator import Protein, EntrezProtein In\u00a0[12]: Copied! <pre>print_yaml('02_schema_config.yaml')\n</pre> print_yaml('02_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id: uniprot\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n\n--------------\n</pre> In\u00a0[13]: Copied! <pre># Create a list of proteins to be imported\nproteins = [\n    p for sublist in zip(\n        [Protein() for _ in range(n_proteins)],\n        [EntrezProtein() for _ in range(n_proteins)],\n    ) for p in sublist\n]\n# Create a new BioCypher instance\nbc = BioCypher(\n    biocypher_config_path='02_biocypher_config_pandas.yaml',\n    schema_config_path='02_schema_config.yaml',\n)\n# Run the import\nbc.add(node_generator(proteins))\n</pre> # Create a list of proteins to be imported proteins = [     p for sublist in zip(         [Protein() for _ in range(n_proteins)],         [EntrezProtein() for _ in range(n_proteins)],     ) for p in sublist ] # Create a new BioCypher instance bc = BioCypher(     biocypher_config_path='02_biocypher_config_pandas.yaml',     schema_config_path='02_schema_config.yaml', ) # Run the import bc.add(node_generator(proteins)) <pre>INFO -- Running BioCypher with schema configuration from 02_schema_config.yaml.\nINFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> In\u00a0[14]: Copied! <pre>bc.to_df()[\"protein\"]\n</pre> bc.to_df()[\"protein\"] Out[14]: node_id node_label sequence description taxon id preferred_id 0 M5L6M9 protein VQLVILKLMFKAKLVANNLYAPWHH Lorem ipsum cjusl 9606 M5L6M9 uniprot 1 767429 protein WIMWMQHYCKIVQRTRQSCTGAIS Lorem ipsum bcuez 9606 767429 uniprot 2 G3H8N9 protein HQCIWAEPNSYEGEVHALFAAVGVTVHDVKNQIM Lorem ipsum otkfx 9606 G3H8N9 uniprot 3 774050 protein SHRTMQMYQRSVGKGIPCDSC Lorem ipsum byuxg 9606 774050 uniprot 4 C2G0M3 protein RRPIWQDYPNPTYTWSQCEVLSLIKYWC Lorem ipsum rygko 9606 C2G0M3 uniprot 5 264315 protein KEMHFLWKCQSFYFGFFEACRK Lorem ipsum kvuni 9606 264315 uniprot <p>This again creates a single DataFrame, now for both protein types, but now including both input streams (you should note both uniprot &amp; entrez style IDs in the id column). However, we are generating our <code>entrez</code> proteins as having entrez IDs, which could result in problems in querying. Additionally, a strict import mode including regex pattern matching of identifiers will fail at this point due to the difference in pattern of UniProt vs. Entrez IDs. This issue could be resolved by mapping the Entrez IDs to UniProt IDs, but we will instead use the opportunity to demonstrate how to merge data from different sources into the same ontological class using ad hoc subclasses.</p> <p>In the previous section, we saw how to merge data from different sources into the same ontological class. However, we did not resolve the issue of the <code>entrez</code> proteins living in a different namespace than the <code>uniprot</code> proteins, which could result in problems in querying. In proteins, it would probably be more appropriate to solve this problem using identifier mapping, but in other categories, e.g., pathways, this may not be possible because of a lack of one-to-one mapping between different data sources. Thus, if we so desire, we can merge datasets into the same ontological class by creating ad hoc subclasses implicitly through BioCypher, by providing multiple preferred identifiers. In our case, we update our schema configuration as follows:</p> In\u00a0[15]: Copied! <pre>print_yaml('03_schema_config.yaml')\n</pre> print_yaml('03_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id:\n    - uniprot\n    - entrez\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n\n--------------\n</pre> <p>This will \"implicitly\" create two subclasses of the <code>protein</code> class, which will inherit the entire hierarchy of the <code>protein</code> class. The two subclasses will be named using a combination of their preferred namespace and the name of the parent class, separated by a dot, i.e., <code>uniprot.protein</code> and <code>entrez.protein</code>. In this manner, they can be identified as proteins regardless of their sources by any queries for the generic <code>protein</code> class, while still carrying information about their namespace and avoiding identifier conflicts.</p>  The only change affected upon the code from the previous section is the referral to the updated schema configuration file.   In the output, we now generate two separate files for the `protein` class, one for each subclass (with names in PascalCase).  <p>Let's create a DataFrame with the same nodes as above, but with a different schema configuration:</p> In\u00a0[16]: Copied! <pre>bc = BioCypher(\n    biocypher_config_path='03_biocypher_config_pandas.yaml',\n    schema_config_path='03_schema_config.yaml',\n)\nbc.add(node_generator(proteins))\nfor name, df in bc.to_df().items():\n    print(name)\n    display(df)\n</pre> bc = BioCypher(     biocypher_config_path='03_biocypher_config_pandas.yaml',     schema_config_path='03_schema_config.yaml', ) bc.add(node_generator(proteins)) for name, df in bc.to_df().items():     print(name)     display(df) <pre>INFO -- Running BioCypher with schema configuration from 03_schema_config.yaml.\nINFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> <pre>uniprot.protein\n</pre> node_id node_label sequence description taxon id preferred_id 0 M5L6M9 uniprot.protein VQLVILKLMFKAKLVANNLYAPWHH Lorem ipsum cjusl 9606 M5L6M9 uniprot 1 G3H8N9 uniprot.protein HQCIWAEPNSYEGEVHALFAAVGVTVHDVKNQIM Lorem ipsum otkfx 9606 G3H8N9 uniprot 2 C2G0M3 uniprot.protein RRPIWQDYPNPTYTWSQCEVLSLIKYWC Lorem ipsum rygko 9606 C2G0M3 uniprot <pre>entrez.protein\n</pre> node_id node_label sequence description taxon id preferred_id 0 767429 entrez.protein WIMWMQHYCKIVQRTRQSCTGAIS Lorem ipsum bcuez 9606 767429 entrez 1 774050 entrez.protein SHRTMQMYQRSVGKGIPCDSC Lorem ipsum byuxg 9606 774050 entrez 2 264315 entrez.protein KEMHFLWKCQSFYFGFFEACRK Lorem ipsum kvuni 9606 264315 entrez <p>Now we see two separate DataFrames, one for each subclass of the <code>protein</code> class.</p> In\u00a0[17]: Copied! <pre>print_yaml('04_schema_config.yaml')\n</pre> print_yaml('04_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id:\n    - uniprot\n    - entrez\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n    properties:\n        sequence: str\n        description: str\n        taxon: str\n        mass: int\n\n--------------\n</pre> <p>This will add the <code>mass</code> property to all proteins (in addition to the three we had before); if not encountered, the column will be empty. Implicit subclasses will automatically inherit the property configuration; in this case, both <code>uniprot.protein</code> and <code>entrez.protein</code> will have the <code>mass</code> property, even though the <code>entrez</code> proteins do not have a <code>mass</code> value in the input data.</p>  If we wanted to ignore the mass value for all properties, we could simply remove the `mass` key from the `properties` dictionary.  In\u00a0[18]: Copied! <pre>from data_generator import EntrezProtein, RandomPropertyProtein\n</pre> from data_generator import EntrezProtein, RandomPropertyProtein In\u00a0[19]: Copied! <pre># Create a list of proteins to be imported (now with properties)\nproteins = [\n    p for sublist in zip(\n        [RandomPropertyProtein() for _ in range(n_proteins)],\n        [EntrezProtein() for _ in range(n_proteins)],\n    ) for p in sublist\n]\n# New instance, populated, and to DataFrame\nbc = BioCypher(\n    biocypher_config_path='04_biocypher_config_pandas.yaml',\n    schema_config_path='04_schema_config.yaml',\n)\nbc.add(node_generator(proteins))\nfor name, df in bc.to_df().items():\n    print(name)\n    display(df)\n</pre> # Create a list of proteins to be imported (now with properties) proteins = [     p for sublist in zip(         [RandomPropertyProtein() for _ in range(n_proteins)],         [EntrezProtein() for _ in range(n_proteins)],     ) for p in sublist ] # New instance, populated, and to DataFrame bc = BioCypher(     biocypher_config_path='04_biocypher_config_pandas.yaml',     schema_config_path='04_schema_config.yaml', ) bc.add(node_generator(proteins)) for name, df in bc.to_df().items():     print(name)     display(df) <pre>INFO -- Running BioCypher with schema configuration from 04_schema_config.yaml.\nINFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> <pre>uniprot.protein\n</pre> node_id node_label sequence description taxon mass id preferred_id 0 B4F6M1 uniprot.protein KQGAYLKNAHCLPAAMISPWSCSPNFVWKTKDNEDDILTEAAGEQWQS Lorem ipsum wqmjt 6116 NaN B4F6M1 uniprot 1 J8S4Q8 uniprot.protein EMYWSCPEVTHEGEMYPYADFYAFNLICIGKCRYLME Lorem ipsum pghkf 4135 NaN J8S4Q8 uniprot 2 D5M8K6 uniprot.protein KEHLAAMVTDPLGPWSMMGGLALFLPINSEEWLMMQYAYEHPQTNETDR Lorem ipsum taqsx 4535 6811.0 D5M8K6 uniprot <pre>entrez.protein\n</pre> node_id node_label sequence description taxon mass id preferred_id 0 285343 entrez.protein EMFSHFMMQLTDPWKNWNECHWRHSAPHPSIMLFTFSSPYNWIIEL Lorem ipsum wqrqh 9606 None 285343 entrez 1 678056 entrez.protein DFSKSCPEGGVTIPPLIYNIWDCKESAIWTHFRRDMMSNDEILQHW... Lorem ipsum ibutr 9606 None 678056 entrez 2 131909 entrez.protein KMCSINAQWAWCQPTGNAQWAPGN Lorem ipsum pbjcm 9606 None 131909 entrez In\u00a0[20]: Copied! <pre>from data_generator import RandomPropertyProteinIsoform\n</pre> from data_generator import RandomPropertyProteinIsoform In\u00a0[21]: Copied! <pre>print_yaml('05_schema_config.yaml')\n</pre> print_yaml('05_schema_config.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id:\n    - uniprot\n    - entrez\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n    properties:\n        sequence: str\n        description: str\n        taxon: str\n        mass: int\nprotein isoform:\n    is_a: protein\n    inherit_properties: true\n    represented_as: node\n    preferred_id: uniprot\n    input_label: uniprot_isoform\n\n--------------\n</pre> <p>This allows maintenance of property lists for many classes at once. If the child class has properties already, they will be kept (if they are not present in the parent class) or replaced by the parent class properties (if they are present).</p> <p>Again, apart from adding the protein isoforms to the input stream, the code for this example is identical to the previous one except for the reference to the updated schema configuration.</p> <p>We now create three separate DataFrames, all of which are children of the <code>protein</code> class; two implicit children (<code>uniprot.protein</code> and <code>entrez.protein</code>) and one explicit child (<code>protein isoform</code>).</p> In\u00a0[22]: Copied! <pre># create a list of proteins to be imported\nproteins = [\n    p for sublist in zip(\n        [RandomPropertyProtein() for _ in range(n_proteins)],\n        [RandomPropertyProteinIsoform() for _ in range(n_proteins)],\n        [EntrezProtein() for _ in range(n_proteins)],\n    ) for p in sublist\n]\n\n# Create BioCypher driver\nbc = BioCypher(\n    biocypher_config_path='05_biocypher_config_pandas.yaml',\n    schema_config_path='05_schema_config.yaml',\n)\n# Run the import\nbc.add(node_generator(proteins))\n\nfor name, df in bc.to_df().items():\n    print(name)\n    display(df)\n</pre> # create a list of proteins to be imported proteins = [     p for sublist in zip(         [RandomPropertyProtein() for _ in range(n_proteins)],         [RandomPropertyProteinIsoform() for _ in range(n_proteins)],         [EntrezProtein() for _ in range(n_proteins)],     ) for p in sublist ]  # Create BioCypher driver bc = BioCypher(     biocypher_config_path='05_biocypher_config_pandas.yaml',     schema_config_path='05_schema_config.yaml', ) # Run the import bc.add(node_generator(proteins))  for name, df in bc.to_df().items():     print(name)     display(df) <pre>INFO -- Running BioCypher with schema configuration from 05_schema_config.yaml.\nINFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> <pre>uniprot.protein\n</pre> node_id node_label sequence description taxon mass id preferred_id 0 F8X2E7 uniprot.protein VEYFPWPYEEWGQAITNEQEPKHDNSVHVNLGPRQWNFQNYT Lorem ipsum sucas 2449 NaN F8X2E7 uniprot 1 W9Y6I4 uniprot.protein IKDRRTDVNCSIRKTSNGEEECCPMWHMDYVLSSMACEKDQETW Lorem ipsum iimgv 9323 8324.0 W9Y6I4 uniprot 2 Z0Y5B3 uniprot.protein YQCVLFEMQVTLCITYIMSQENPNISI Lorem ipsum ljnbm 8891 NaN Z0Y5B3 uniprot <pre>protein isoform\n</pre> node_id node_label sequence description taxon mass id preferred_id 0 H8F9L4 protein isoform QVLIYDLIDLNCTRCWDWGTWWNL Lorem ipsum binuk 8081 NaN H8F9L4 uniprot 1 A0Q9J8 protein isoform KFSRHINNGGEKAVQEWEQTSEGLVMGFRNSRWPYKWQQY Lorem ipsum ebspk 5773 NaN A0Q9J8 uniprot 2 L1K7W3 protein isoform HVWMRWWFHGWINNYKEHWNSMMSITHASVLHKQEYQMEAGA Lorem ipsum fxyad 4565 7372.0 L1K7W3 uniprot <pre>entrez.protein\n</pre> node_id node_label sequence description taxon mass id preferred_id 0 814463 entrez.protein YRLSSQRYIYQKDGPDHQVL Lorem ipsum sfcix 9606 None 814463 entrez 1 887207 entrez.protein KFESNGEDSYHNYWDAQHYSMFVVRCPMPLGHGVNNWE Lorem ipsum jkddr 9606 None 887207 entrez 2 681494 entrez.protein KQFRNGMKHWMFANLEYKYWEHNPFRECT Lorem ipsum auede 9606 None 681494 entrez In\u00a0[23]: Copied! <pre>print_yaml('06_schema_config_pandas.yaml')\n</pre> print_yaml('06_schema_config_pandas.yaml') <pre>--------------\nprotein:\n    represented_as: node\n    preferred_id:\n    - uniprot\n    - entrez\n    input_label:\n    - uniprot_protein\n    - entrez_protein\n    properties:\n        sequence: str\n        description: str\n        taxon: str\n        mass: int\nprotein isoform:\n    is_a: protein\n    inherit_properties: true\n    represented_as: node\n    preferred_id: uniprot\n    input_label: uniprot_isoform\nprotein protein interaction:\n    is_a: pairwise molecular interaction\n    represented_as: edge\n    preferred_id: intact\n    input_label: interacts_with\n    properties:\n        method: str\n        source: str\n\n--------------\n</pre> <p>Now that we have added <code>protein protein interaction</code> as an edge, we have to simulate some interactions:</p> In\u00a0[24]: Copied! <pre>from data_generator import InteractionGenerator\n\n# Simulate edges for proteins we defined above\nppi = InteractionGenerator(\n    interactors=[p.get_id() for p in proteins],\n    interaction_probability=0.05,\n).generate_interactions()\n</pre> from data_generator import InteractionGenerator  # Simulate edges for proteins we defined above ppi = InteractionGenerator(     interactors=[p.get_id() for p in proteins],     interaction_probability=0.05, ).generate_interactions() In\u00a0[25]: Copied! <pre># naturally interactions/edges contain information about the interacting source and target nodes\n# let's look at the first one in the list\ninteraction = ppi[0]\nf\"{interaction.get_source_id()} {interaction.label} {interaction.get_target_id()}\"\n</pre> # naturally interactions/edges contain information about the interacting source and target nodes # let's look at the first one in the list interaction = ppi[0] f\"{interaction.get_source_id()} {interaction.label} {interaction.get_target_id()}\" Out[25]: <pre>'814463 interacts_with 681494'</pre> In\u00a0[26]: Copied! <pre># similarly to nodes, it also has a dictionary of properties\ninteraction.get_properties()\n</pre> # similarly to nodes, it also has a dictionary of properties interaction.get_properties() Out[26]: <pre>{'method': 'Lorem ipsum sqeyz'}</pre> <p>As with nodes, we add first createa a new BioCypher instance, and then populate it with nodes as well as edges:</p> In\u00a0[27]: Copied! <pre>bc = BioCypher(\n    biocypher_config_path='06_biocypher_config_pandas.yaml',\n    schema_config_path='06_schema_config_pandas.yaml',\n)\n</pre> bc = BioCypher(     biocypher_config_path='06_biocypher_config_pandas.yaml',     schema_config_path='06_schema_config_pandas.yaml', ) <pre>INFO -- Running BioCypher with schema configuration from 06_schema_config_pandas.yaml.\n</pre> In\u00a0[28]: Copied! <pre># Extract id, source, target, label, and property dictionary\ndef edge_generator(ppi):\n    for interaction in ppi:\n        yield (\n            interaction.get_id(),\n            interaction.get_source_id(),\n            interaction.get_target_id(),\n            interaction.get_label(),\n            interaction.get_properties(),\n        )\n\nbc.add(node_generator(proteins))\nbc.add(edge_generator(ppi))\n</pre> # Extract id, source, target, label, and property dictionary def edge_generator(ppi):     for interaction in ppi:         yield (             interaction.get_id(),             interaction.get_source_id(),             interaction.get_target_id(),             interaction.get_label(),             interaction.get_properties(),         )  bc.add(node_generator(proteins)) bc.add(edge_generator(ppi))  <pre>INFO -- Loading ontologies...\nINFO -- Instantiating OntologyAdapter class for https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl.\n</pre> <p>Let's look at the interaction DataFrame:</p> In\u00a0[29]: Copied! <pre>bc.to_df()[\"protein protein interaction\"]\n</pre> bc.to_df()[\"protein protein interaction\"] Out[29]: relationship_id source_id target_id relationship_label method source 0 None 814463 681494 protein protein interaction Lorem ipsum sqeyz None <p>Finally, it is worth noting that BioCypher relies on ontologies, which are machine readable representations of domains of knowledge that we use to ground the contents of our knowledge graphs. While details about ontologies are out of scope for this tutorial, and are described in detail in the BioCypher documentation, we can still have a glimpse at the ontology that we used implicitly in this tutorial:</p> In\u00a0[30]: Copied! <pre>bc.show_ontology_structure()\n</pre> bc.show_ontology_structure() <pre>INFO -- Showing ontology structure based on https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl\nINFO -- \nentity\n\u251c\u2500\u2500 association\n\u2502   \u2514\u2500\u2500 gene to gene association\n\u2502       \u2514\u2500\u2500 pairwise gene to gene interaction\n\u2502           \u2514\u2500\u2500 pairwise molecular interaction\n\u2502               \u2514\u2500\u2500 protein protein interaction\n\u2514\u2500\u2500 named thing\n    \u2514\u2500\u2500 biological entity\n        \u2514\u2500\u2500 polypeptide\n            \u2514\u2500\u2500 protein\n                \u251c\u2500\u2500 entrez.protein\n                \u251c\u2500\u2500 protein isoform\n                \u2514\u2500\u2500 uniprot.protein\n\n</pre> Out[30]: <pre>&lt;treelib.tree.Tree at 0x13058fd60&gt;</pre>"},{"location":"learn/tutorials/pandas_tutorial/#example-notebook-biocypher-and-pandas","title":"Example Notebook: BioCypher and Pandas\u00b6","text":""},{"location":"learn/tutorials/pandas_tutorial/#introduction","title":"Introduction\u00b6","text":"<p>The main purpose of BioCypher is to facilitate the pre-processing of biomedical data, and thus save development time in the maintenance of curated knowledge graphs, while allowing simple and efficient creation of task-specific lightweight knowledge graphs in a user-friendly and biology-centric fashion.</p> <p>We are going to use a toy example to familiarise the user with the basic functionality of BioCypher. One central task of BioCypher is the harmonisation of dissimilar datasets describing the same entities. Thus, in this example, the input data - which in the real-world use case could come from any type of interface - are represented by simulated data containing some examples of differently formatted biomedical entities such as proteins and their interactions.</p> <p>There are two other versions of this tutorial, which only differ in the output format. The first uses a CSV output format to write files suitable for Neo4j admin import, and the second creates an in-memory collection of Pandas dataframes. You can find the former in the tutorial directory of the BioCypher repository. This tutorial simply takes the latter, in-memory approach to a Jupyter notebook.</p>"},{"location":"learn/tutorials/pandas_tutorial/#setup","title":"Setup\u00b6","text":""},{"location":"learn/tutorials/pandas_tutorial/#tutorial-files","title":"Tutorial files\u00b6","text":"<p>In the <code>biocypher</code> root directory, you will find a <code>tutorial</code> directory with the files for this tutorial. The <code>data_generator.py</code> file contains the simulated data generation code, and the other files, specifically the <code>.yaml</code> files, are named according to the tutorial step they are used in.</p> <p>Let's download these:</p>"},{"location":"learn/tutorials/pandas_tutorial/#configuration","title":"Configuration\u00b6","text":"<p>BioCypher is configured using a YAML file; it comes with a default (which you can see in the Configuration section). You can use it, for instance, to select an output format, the output directory, separators, logging level, and other options. For this tutorial, we will use a dedicated configuration file for each of the steps. The configuration files are located in the <code>tutorial</code> directory, and are called using the <code>biocypher_config_path</code> argument at instantiation of the BioCypher interface. For more information, see also the Quickstart Configuration section.</p>"},{"location":"learn/tutorials/pandas_tutorial/#section-1-adding-data","title":"Section 1: Adding data\u00b6","text":""},{"location":"learn/tutorials/pandas_tutorial/#input-data-stream-adapter","title":"Input data stream (\"adapter\")\u00b6","text":"<p>The basic operation of adding data to the knowledge graph requires two components: an input stream of data (which we call adapter) and a configuration for the resulting desired output (the schema configuration). The former will be simulated by calling the <code>Protein</code> class of our data generator 10 times.</p>"},{"location":"learn/tutorials/pandas_tutorial/#schema-configuration","title":"Schema configuration\u00b6","text":"<p>How each BioCypher graph is structured is determined by the schema configuration YAML file that is given to the BioCypher interface. This also serves to ground the entities of the graph in the biomedical realm by using an ontological hierarchy. In this tutorial, we refer to the Biolink model as the general backbone of our ontological hierarchy. The basic premise of the schema configuration YAML file is that each component of the desired knowledge graph output should be configured here; if (and only if) an entity is represented in the schema configuration and is present in the input data stream, will it be part of our knowledge graph.</p> <p>In our case, since we only import proteins, we only require few lines of configuration:</p>"},{"location":"learn/tutorials/pandas_tutorial/#creating-the-graph-using-the-biocypher-interface","title":"Creating the graph (using the BioCypher interface)\u00b6","text":"<p>All that remains to be done now is to instantiate the BioCypher interface (as the main means of communicating with BioCypher) and call the function to create the graph.</p>"},{"location":"learn/tutorials/pandas_tutorial/#section-2-merging-data","title":"Section 2: Merging data\u00b6","text":""},{"location":"learn/tutorials/pandas_tutorial/#plain-merge","title":"Plain merge\u00b6","text":"<p>Using the workflow described above with minor changes, we can merge data from different input streams. If we do not want to introduce additional ontological subcategories, we can simply add the new input stream to the existing one and add the new label to the schema configuration (the new label being <code>entrez_protein</code>). In this case, we would add the following to the schema configuration:</p>"},{"location":"learn/tutorials/pandas_tutorial/#ad-hoc-subclassing","title":"Ad hoc subclassing\u00b6","text":""},{"location":"learn/tutorials/pandas_tutorial/#section-3-handling-properties","title":"Section 3: Handling properties\u00b6","text":"<p>While ID and label are mandatory components of our knowledge graph, properties are optional and can include different types of information on the entities. In source data, properties are represented in arbitrary ways, and designations rarely overlap even for the most trivial of cases (spelling differences, formatting, etc). Additionally, some data sources contain a large wealth of information about entities, most of which may not be needed for the given task. Thus, it is often desirable to filter out properties that are not needed to save time, disk space, and memory.</p> <p>Maintaining consistent properties per entity type is particularly important when using the admin import feature of Neo4j, which requires consistency between the header and data files. Properties that are introduced into only some of the rows will lead to column misalignment and import failure. In \"online mode\", this is not an issue.</p> <p>We will take a look at how to handle property selection in BioCypher in a way that is flexible and easy to maintain.</p>"},{"location":"learn/tutorials/pandas_tutorial/#designated-properties","title":"Designated properties\u00b6","text":"<p>The simplest and most straightforward way to ensure that properties are consistent for each entity type is to designate them explicitly in the schema configuration. This is done by adding a <code>properties</code> key to the entity type configuration. The value of this key is another dictionary, where in the standard case the keys are the names of the properties that the entity type should possess, and the values give the type of the property. Possible values are:</p> <ul> <li><p><code>str</code> (or <code>string</code>),</p> </li> <li><p><code>int</code> (or <code>integer</code>, <code>long</code>),</p> </li> <li><p><code>float</code> (or <code>double</code>, <code>dbl</code>),</p> </li> <li><p><code>bool</code> (or <code>boolean</code>),</p> </li> <li><p>arrays of any of these types (indicated by square brackets, e.g. <code>string[]</code>).</p> </li> </ul> <p>In the case of properties that are not present in (some of) the source data, BioCypher will add them to the output with a default value of <code>None</code>. Additional properties in the input that are not represented in these designated property names will be ignored. Let's imagine that some, but not all, of our protein nodes have a <code>mass</code> value. If we want to include the mass value on all proteins, we can add the following to our schema configuration:</p>"},{"location":"learn/tutorials/pandas_tutorial/#inheriting-properties","title":"Inheriting properties\u00b6","text":"<p>Sometimes, explicit designation of properties requires a lot of maintenance work, particularly for classes with many properties. In these cases, it may be more convenient to inherit properties from a parent class. This is done by adding a <code>properties</code> key to a suitable parent class configuration, and then defining inheritance via the <code>is_a</code> key in the child class configuration and setting the <code>inherit_properties</code> key to <code>true</code>.</p> <p>Let's say we have an additional <code>protein isoform</code> class, which can reasonably inherit from <code>protein</code> and should carry the same properties as the parent. We can add the following to our schema configuration:</p>"},{"location":"learn/tutorials/pandas_tutorial/#section-4-handling-relationships","title":"Section 4: Handling relationships\u00b6","text":"<p>Naturally, we do not only want nodes in our knowledge graph, but also edges. In BioCypher, the configuration of relationships is very similar to that of nodes, with some key differences. First the similarities: the top-level class configuration of edges is the same; class names refer to ontological classes or are an extension thereof. Similarly, the <code>is_a</code> key is used to define inheritance, and the <code>inherit_properties</code> key is used to inherit properties from a parent class. Relationships also possess a <code>preferred_id</code> key, an <code>input_label</code> key, and a <code>properties</code> key, which work in the same way as for nodes.</p> <p>Relationships also have a <code>represented_as</code> key, which in this case can be either <code>node</code> or <code>edge</code>. The <code>node</code> option is used to \"reify\" the relationship in order to be able to connect it to other nodes in the graph. In addition to the configuration of nodes, relationships also have fields for the <code>source</code> and <code>target</code> node types, which refer to the ontological classes of the respective nodes, and are currently optional.</p> <p>To add protein-protein interactions to our graph, we can modify the schema configuration above to the following:</p>"},{"location":"learn/tutorials/tutorial001_basics/","title":"Tutorial - Basics","text":"<p>The main purpose of BioCypher is to facilitate the pre-processing of biomedical data to save development time in the maintenance of curated knowledge graphs and to allow the simple and efficient creation of task-specific lightweight knowledge graphs in a user-friendly and biology-centric fashion.</p> <p>We are going to use a toy example to familiarise the user with the basic functionality of BioCypher. One central task of BioCypher is the harmonisation of dissimilar datasets describing the same entities. Thus, in this example, the input data\u2014which in the real-world use case could come from any type of interface\u2014are represented by simulated data containing some examples of differently formatted biomedical entities such as proteins and their interactions.</p> <p>Jupyter Notebook</p> <p>If you prefer to run the tutorial in a Jupyter Notebook or as a Google Colab in your browser, go here.</p> <p>There are two versions of this tutorial, which only differ in the output format. The first uses a CSV output format to write files suitable for Neo4j admin import, and the second creates an in-memory collection of Pandas dataframes. You can find both in the <code>tutorial</code> directory of the BioCypher repository; the Pandas version of each tutorial step is suffixed with <code>_pandas</code>.</p> <p>Neo4j</p> <p>While you can use the files generated to create an actual Neo4j database, it is not required for this tutorial. For checking the output, you can simply open the CSV files in a text editor or your IDE; by default, they will be written to the <code>biocypher-out</code> directory. If you simply want to run the tutorial to see how it works, you can also run the Pandas version.</p>"},{"location":"learn/tutorials/tutorial001_basics/#setup","title":"Setup","text":"<p>To run this tutorial, you will need to have cloned and installed the BioCypher repository on your machine. We recommend using Poetry:</p> <pre><code>git clone https://github.com/biocypher/biocypher.git\ncd biocypher\npoetry install\n</code></pre> <p>Poetry environment</p> <p>In order to run the tutorial code, you will need to activate the Poetry environment. This can be done by running <code>poetry shell</code> in the <code>biocypher</code> directory. Alternatively, you can run the code from within the Poetry environment by prepending <code>poetry run</code> to the command. For example, to run the tutorial code, you can run <code>poetry run python tutorial/01__basic_import.py</code>.</p> <p>In the <code>biocypher</code> root directory, you will find a <code>tutorial</code> directory with the files for this tutorial. The <code>data_generator.py</code> file contains the simulated data generation code, and the other files are named according to the tutorial step they are used in. The <code>biocypher-out</code> directory will be created automatically when you run the tutorial code.</p>"},{"location":"learn/tutorials/tutorial001_basics/#configuration","title":"Configuration","text":"<p>BioCypher is configured using a YAML file; it comes with a default (which you can see in the Configuration section). You can use it, for instance, to select an output format, the output directory, separators, logging level, and other options. For this tutorial, we will use a dedicated configuration file for each of the steps. The configuration files are located in the <code>tutorial</code> directory, and are called using the <code>biocypher_config_path</code> argument at instantiation of the BioCypher interface.</p>"},{"location":"learn/tutorials/tutorial001_basics/#section-1-adding-data","title":"Section 1: Adding data","text":"<p>Note: Poetry environment</p> <p>The code for this tutorial can be found at <code>tutorial/01__basic_import.py</code>. The schema is at <code>tutorial/01_schema_config.yaml</code>, configuration in <code>tutorial/01_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p>"},{"location":"learn/tutorials/tutorial001_basics/#input-data-stream-adapter","title":"Input data stream (\"adapter\")","text":"<p>The basic operation of adding data to the knowledge graph requires two components: an input stream of data (which we call adapter) and a configuration for the resulting desired output (the schema configuration). The former will be simulated by calling the <code>Protein</code> class of our data generator 10 times.</p> <pre><code>from tutorial.data_generator import Protein\nproteins = [Protein() for _ in range(10)]\n</code></pre> <p>Each protein in our simulated data has a UniProt ID, a label (\"uniprot_protein\"), and a dictionary of properties describing it. This is\u2014purely by coincidence\u2014very close to the input BioCypher expects (for nodes):</p> <ul> <li>a unique identifier</li> <li>an input label (to allow mapping to the ontology, see the second step below)</li> <li>a dictionary of further properties (which can be empty)</li> </ul> <p>These should be presented to BioCypher in the form of a tuple. To achieve this representation, we can use a generator function that iterates through our simulated input data and, for each entity, forms the corresponding tuple. The use of a generator allows for efficient streaming of larger datasets where required.</p> <pre><code>def node_generator():\n    for protein in proteins:\n        yield (\n            protein.get_id(),\n            protein.get_label(),\n            protein.get_properties()\n        )\n</code></pre> <p>The concept of an adapter can become arbitrarily complex and involve programmatic access to databases, API requests, asynchronous queries, context managers, and other complicating factors. However, it always boils down to providing the BioCypher driver with a collection of tuples, one for each entity in the input data. For more info, see the section on Adapters and the Adapter Tutorial.</p> <p>As descibed above, nodes possess:</p> <ul> <li>a mandatory ID,</li> <li>a mandatory label, and</li> <li>a property dictionary,</li> </ul> <p>while edges possess:</p> <ul> <li>an (optional) ID,</li> <li>two mandatory IDs for source and target,</li> <li>a mandatory label, and</li> <li>a property dictionary.</li> </ul> <p>How these entities are mapped to the ontological hierarchy underlying a BioCypher graph is determined by their mandatory labels, which connect the input data stream to the schema configuration. This we will see in the following section.</p>"},{"location":"learn/tutorials/tutorial001_basics/#schema-configuration","title":"Schema configuration","text":"<p>How each BioCypher graph is structured is determined by the schema configuration YAML file that is given to the BioCypher interface. This also serves to ground the entities of the graph in the biomedical realm by using an ontological hierarchy. In this tutorial, we refer to the Biolink model as the general backbone of our ontological hierarchy. The basic premise of the schema configuration YAML file is that each component of the desired knowledge graph output should be configured here; if (and only if) an entity is represented in the schema configuration and is present in the input data stream, it will be part of our knowledge graph.</p> <p>In our case, since we only import proteins, we only require few lines of configuration:</p> <pre><code>protein: # mapping\n  represented_as: node # schema configuration\n  preferred_id: uniprot # uniqueness\n  input_label: uniprot_protein # connection to input stream\n</code></pre> <p>The first line (<code>protein</code>) identifies our entity and connects to the ontological backbone; here we define the first class to be represented in the graph. In the configuration YAML, we represent entities\u202f\u2014 similar to the internal representation of Biolink\u202f\u2014 in lower sentence case (e.g., \"small molecule\"). Conversely, for class names, in file names, and property graph labels, we use PascalCase instead (e.g., \"SmallMolecule\") to avoid issues with handling spaces. The transformation is done by BioCypher internally. BioCypher does not strictly enforce the entities allowed in this class definition; in fact, we provide several methods for extending the existing ontological backbone ad hoc by providing custom inheritance or hybridising ontologies. However, every entity should at some point be connected to the underlying ontology, otherwise the multiple hierarchical labels will not be populated. Following this first line are three indented values of the protein class.</p> <p>The second line (<code>represented_as</code>) tells BioCypher in which way each entity should be represented in the graph; the only options are <code>node</code> and <code>edge</code>. Representation as an edge is only possible when source and target IDs are provided in the input data stream. Conversely, relationships can be represented as both <code>node</code> or <code>edge</code>, depending on the desired output. When a relationship should be represented as a node, i.e., \"reified\", BioCypher takes care to create a set of two edges and a node in place of the relationship. This is useful when we want to connect the relationship to other entities in the graph, for example literature references.</p> <p>The third line (<code>preferred_id</code>) relates to how entity uniqueness is determined in the knowledge graph. This field (which will be renamed to <code>namespace</code> in a future release to better reflect its purpose) specifies the ontological namespace that should be used to identify unique entities. In our example, specifying <code>uniprot</code> means:</p> <ol> <li>Proteins with the same UniProt ID will be considered the same entity and merged in the graph</li> <li>For identifier mapping purposes, the pipeline will attempt to find or convert to UniProt IDs when possible (this must be implemented in the adapter)</li> <li>The namespace serves as a prefix for disambiguation (similar to how <code>hgnc:TP53</code> is more specific than just <code>TP53</code>)</li> </ol> <p>The selection of a namespace requires consideration in task-specific applications. For the broadest possible range of identifier systems, we use Bioregistry project namespaces as preferred values for this field.</p> <p>While <code>preferred_id</code> is optional, it becomes particularly useful when merging data from different sources (for instance, when merging similar information from multiple resources) or when working with identifiers that aren't inherently unique across all contexts. For entities with well-established global identifiers (like UniProt for proteins), this helps maintain consistency throughout your knowledge graph.</p> <p>Finally, the fourth line (<code>input_label</code>) connects the input data stream to the configuration; here we indicate which label to expect in the input tuple for each class in the graph. In our case, we expect \"uniprot_protein\" as the label for each protein in the input data stream; all other input entities that do not carry this label are ignored as long as they are not in the schema configuration.</p>"},{"location":"learn/tutorials/tutorial001_basics/#creating-the-graph-using-the-biocypher-interface","title":"Creating the graph (using the BioCypher interface)","text":"<p>All that remains to be done now is to instantiate the BioCypher interface (as the main means of communicating with BioCypher) and call the function to create the graph. While this can be done \"online\", i.e., by connecting to a running DBMS instance, we will in this example use the offline mode of BioCypher, which does not require setting up a graph database instance. The following code will use the data stream and configuration set up above to write the files for knowledge graph creation:</p> <pre><code>import os\nos.chdir('../')\n</code></pre> <pre><code>from biocypher import BioCypher\n\nbc = BioCypher(\n    biocypher_config_path=\"tutorial/01_biocypher_config.yaml\",\n    schema_config_path=\"tutorial/01_schema_config.yaml\",\n)\n\nbc.write_nodes(node_generator())\n</code></pre> <p>We pass our configuration files at instantiation of the interface, and we pass the data stream to the <code>write_nodes</code> function. BioCypher will then create the graph and write it to the output directory, which is set to <code>biocypher-out/</code> by default, creating a subfolder with the current datetime for each driver instance.</p> <p>Note</p> <p>The <code>biocypher_config_path</code> parameter at instantiation of the <code>BioCypher</code> class should in most cases not be needed; we are using it here to increase convenience of the tutorial and to showcase its use. We are overriding the default value of only two settings: the offline mode (<code>offline</code> in <code>biocypher</code>) and the database name (<code>database_name</code> in <code>neo4j</code>).</p> <p>By default, BioCypher will look for a file named <code>biocypher_config.yaml</code> in the current working directory and in its subfolder <code>config</code>, as well as in various user directories. For more information, see the section on configuration.</p>"},{"location":"learn/tutorials/tutorial001_basics/#importing-data-into-neo4j","title":"Importing data into Neo4j","text":"<p>If you want to build an actual Neo4j graph from the tutorial output files, please follow the Neo4j import tutorial.</p>"},{"location":"learn/tutorials/tutorial001_basics/#quality-control-and-convenience-functions","title":"Quality control and convenience functions","text":"<p>BioCypher provides a number of convenience functions for quality control and data exploration. In addition to writing the import call for Neo4j, we can print a log of ontological classes that were present in the input data but are not accounted for in the schema configuration, as well as a log of duplicates in the input data (for the level of granularity that was used for the import). We can also print the ontological hierarchy derived from the underlying model(s) according to the classes that were given in the schema configuration:</p> <pre><code>bc.log_missing_input_labels()   # show input unaccounted for in the schema\nbc.log_duplicates()             # show duplicates in the input data\nbc.show_ontology_structure()    # show ontological hierarchy\n</code></pre>"},{"location":"learn/tutorials/tutorial001_basics/#section-2-merging-data","title":"Section 2: Merging data","text":""},{"location":"learn/tutorials/tutorial001_basics/#plain-merge","title":"Plain merge","text":"<p>Tutorial files</p> <p>The code for this tutorial can be found at <code>tutorial/02__merge.py</code>. Schema files are at <code>tutorial/02_schema_config.yaml</code>, configuration in <code>tutorial/02_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p> <p>Using the workflow described above with minor changes, we can merge data from different input streams. If we do not want to introduce additional ontological subcategories, we can simply add the new input stream to the existing one and add the new label to the schema configuration (the new label being <code>entrez_protein</code>). In this case, we would add the following to the schema configuration:</p> <pre><code>protein:\n  represented_as: node\n  preferred_id: uniprot\n  input_label: [uniprot_protein, entrez_protein]\n</code></pre> <p>This again creates a single output file, now for both protein types, including both input streams, and the graph can be created as before using the command line call created by BioCypher. However, we are generating our <code>entrez</code> proteins as having entrez IDs, which could result in problems in querying. Additionally, a strict import mode including regex pattern matching of identifiers will fail at this point due to the difference in pattern of UniProt vs. Entrez IDs. This issue could be resolved by mapping the Entrez IDs to UniProt IDs, but we will instead use the opportunity to demonstrate how to merge data from different sources into the same ontological class using ad hoc subclasses.</p>"},{"location":"learn/tutorials/tutorial001_basics/#ad-hoc-subclassing","title":"Ad hoc subclassing","text":"<p>Tutorial files</p> <p>The code for this tutorial can be found at <code>tutorial/03__implicit_subclass.py</code>. Schema files are at <code>tutorial/03_schema_config.yaml</code>, configuration in <code>tutorial/03_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p> <p>In the previous section, we saw how to merge data from different sources into the same ontological class. However, we did not resolve the issue of the <code>entrez</code> proteins living in a different namespace than the <code>uniprot</code> proteins, which could result in problems in querying. In proteins, it would probably be more appropriate to solve this problem using identifier mapping, but in other categories, e.g., pathways, this may not be possible because of a lack of one-to-one mapping between different data sources. Thus, if we so desire, we can merge datasets into the same ontological class by creating ad hoc subclasses implicitly through BioCypher, by providing multiple preferred identifiers. In our case, we update our schema configuration as follows:</p> <pre><code>protein:\n  represented_as: node\n  preferred_id: [uniprot, entrez]\n  input_label: [uniprot_protein, entrez_protein]\n</code></pre> <p>This will \"implicitly\" create two subclasses of the <code>protein</code> class, which will inherit the entire hierarchy of the <code>protein</code> class. The two subclasses will be named using a combination of their preferred namespace and the name of the parent class, separated by a dot, i.e., <code>uniprot.protein</code> and <code>entrez.protein</code>. In this manner, they can be identified as proteins regardless of their sources by any queries for the generic <code>protein</code> class, while still carrying information about their namespace and avoiding identifier conflicts.</p> <p>Note</p> <p>The only change affected upon the code from the previous section is the referral to the updated schema configuration file.</p> <p>Hint</p> <p>In the output, we now generate two separate files for the <code>protein</code> class, one for each subclass (with names in PascalCase).</p>"},{"location":"learn/tutorials/tutorial001_basics/#section-3-handling-properties","title":"Section 3: Handling properties","text":"<p>While ID and label are mandatory components of our knowledge graph, properties are optional and can include different types of information on the entities. In source data, properties are represented in arbitrary ways, and designations rarely overlap even for the most trivial of cases (spelling differences, formatting, etc). Additionally, some data sources contain a large wealth of information about entities, most of which may not be needed for the given task. Thus, it is often desirable to filter out properties that are not needed to save time, disk space, and memory.</p> <p>Note</p> <p>Maintaining consistent properties per entity type is particularly important when using the admin import feature of Neo4j, which requires consistency between the header and data files. Properties that are introduced into only some of the rows will lead to column misalignment and import failure. In \"online mode\", this is not an issue.</p> <p>We will take a look at how to handle property selection in BioCypher in a way that is flexible and easy to maintain.</p>"},{"location":"learn/tutorials/tutorial001_basics/#designated-properties","title":"Designated properties","text":"<p>Tutorial files</p> <p>The code for this tutorial can be found at <code>tutorial/04__properties.py</code>. Schema files are at <code>tutorial/04_schema_config.yaml</code>, configuration in <code>tutorial/04_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p> <p>The simplest and most straightforward way to ensure that properties are consistent for each entity type is to designate them explicitly in the schema configuration. This is done by adding a <code>properties</code> key to the entity type configuration. The value of this key is another dictionary, where in the standard case the keys are the names of the properties that the entity type should possess, and the values give the type of the property. Possible values are:</p> <ul> <li> <p><code>str</code> (or <code>string</code>),</p> </li> <li> <p><code>int</code> (or <code>integer</code>, <code>long</code>),</p> </li> <li> <p><code>float</code> (or <code>double</code>, <code>dbl</code>),</p> </li> <li> <p><code>bool</code> (or <code>boolean</code>),</p> </li> <li> <p>arrays of any of these types (indicated by square brackets, e.g. <code>string[]</code>).</p> </li> </ul> <p>In the case of properties that are not present in (some of) the source data, BioCypher will add them to the output with a default value of <code>None</code>. Additional properties in the input that are not represented in these designated property names will be ignored. Let's imagine that some, but not all, of our protein nodes have a <code>mass</code> value. If we want to include the mass value on all proteins, we can add the following to our schema configuration:</p> <pre><code>protein:\n  represented_as: node\n  preferred_id: [uniprot, entrez]\n  input_label: [uniprot_protein, entrez_protein]\n  properties:\n    sequence: str\n    description: str\n    taxon: str\n    mass: dbl\n</code></pre> <p>This will add the <code>mass</code> property to all proteins (in addition to the three we had before); if not encountered, the column will be empty. Implicit subclasses will automatically inherit the property configuration; in this case, both <code>uniprot.protein</code> and <code>entrez.protein</code> will have the <code>mass</code> property, even though the <code>entrez</code> proteins do not have a <code>mass</code> value in the input data.</p> <p>Note</p> <p>If we wanted to ignore the mass value for all properties, we could simply remove the <code>mass</code> key from the <code>properties</code> dictionary.</p> <p>Tip</p> <p>BioCypher provides feedback about property conflicts; try running the code for this example (<code>04__properties.py</code>) with the schema configuration of the previous section (<code>03_schema_config.yaml</code>) and see what happens.</p>"},{"location":"learn/tutorials/tutorial001_basics/#inheriting-properties","title":"Inheriting properties","text":"<p>Tutorial files</p> <p>The code for this tutorial can be found at <code>tutorial/05__property_inheritance.py</code>. Schema files are at <code>tutorial/05_schema_config.yaml</code>, configuration in <code>tutorial/05_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p> <p>Sometimes, explicit designation of properties requires a lot of maintenance work, particularly for classes with many properties. In these cases, it may be more convenient to inherit properties from a parent class. This is done by adding a <code>properties</code> key to a suitable parent class configuration, and then defining inheritance via the <code>is_a</code> key in the child class configuration and setting the <code>inherit_properties</code> key to <code>true</code>.</p> <p>Let's say we have an additional <code>protein isoform</code> class, which can reasonably inherit from <code>protein</code> and should carry the same properties as the parent. We can add the following to our schema configuration:</p> <pre><code>protein isoform:\n  is_a: protein\n  inherit_properties: true\n  represented_as: node\n  preferred_id: uniprot\n  input_label: uniprot_isoform\n</code></pre> <p>This allows maintenance of property lists for many classes at once. If the child class has properties already, they will be kept (if they are not present in the parent class) or replaced by the parent class properties (if they are present).</p> <p>Note</p> <p>Again, apart from adding the protein isoforms to the input stream, the code for this example is identical to the previous one except for the reference to the updated schema configuration.</p> <p>Hint</p> <p>We now create three separate data files, all of which are children of the <code>protein</code> class; two implicit children (<code>uniprot.protein</code> and <code>entrez.protein</code>) and one explicit child (<code>protein isoform</code>).</p>"},{"location":"learn/tutorials/tutorial001_basics/#section-4-handling-relationships","title":"Section 4: Handling relationships","text":"<p>Tutorial Files</p> <p>The code for this tutorial can be found at <code>tutorial/06__relationships.py</code>. Schema files are at <code>tutorial/06_schema_config.yaml</code>, configuration in <code>tutorial/06_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p> <p>Naturally, we do not only want nodes in our knowledge graph, but also edges. In BioCypher, the configuration of relationships is very similar to that of nodes, with some key differences. First the similarities: the top-level class configuration of edges is the same; class names refer to ontological classes or are an extension thereof. Similarly, the <code>is_a</code> key is used to define inheritance, and the <code>inherit_properties</code> key is used to inherit properties from a parent class. Relationships also possess a <code>preferred_id</code> key, an <code>input_label</code> key, and a <code>properties</code> key, which work in the same way as for nodes.</p> <p>Relationships also have a <code>represented_as</code> key, which in this case can be either <code>node</code> or <code>edge</code>. The <code>node</code> option is used to \"reify\" the relationship in order to be able to connect it to other nodes in the graph. In addition to the configuration of nodes, relationships also have fields for the <code>source</code> and <code>target</code> node types, which refer to the ontological classes of the respective nodes, and are currently optional.</p> <p>To add protein-protein interactions to our graph, we can add the following to the schema configuration above:</p> <pre><code>protein protein interaction:\n  is_a: pairwise molecular interaction\n  represented_as: node\n  preferred_id: intact\n  input_label: interacts_with\n  properties:\n    method: str\n    source: str\n</code></pre> <p>Here, we use explicit subclassing to define the protein-protein interaction, which is not represented in the basic Biolink model, as a direct child of the Biolink \"pairwise molecular interaction\" class. We also reify this relationship by representing it as a node. This allows us to connect it to other nodes in the graph, for example to evidences for each interaction. If we do not want to reify the relationship, we can set <code>represented_as</code> to <code>edge</code> instead.</p>"},{"location":"learn/tutorials/tutorial001_basics/#relationship-identifiers","title":"Relationship identifiers","text":"<p>In biomedical data, relationships often do not have curated unique identifiers. Nevertheless, we may want to be able to refer to them in the graph. Thus, edges possess an ID field similar to nodes, which can be supplied in the input data as an optional first element in the edge tuple. Generating this ID from the properties of the edge (source and target identifiers, and additionally any properties that the edge possesses) can be done, for instance, by using the MD5 hash of the concatenation of these values. Edge IDs are active by default, but can be deactivated by setting the <code>use_id</code> field to <code>false</code> in the <code>schema_config.yaml</code> file.</p> schema_config.yaml<pre><code>protein protein interaction:\n  is_a: pairwise molecular interaction\n  represented_as: edge\n  use_id: false\n  # ...\n</code></pre>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/","title":"Tutorial - Handling Ontologies","text":"<p>BioCypher relies on ontologies to ground the knowledge graph contents in biology. This has the advantages of providing machine readability and therefore automation capabilities as well as making working with BioCypher accessible to biologically oriented researchers. However, it also means that BioCypher requires a certain amount of knowledge about ontologies and how to use them. We try to make dealing with ontologies as easy as possible, but some basic understanding is required. In the following we will cover the basics of ontologies and how to use them in BioCypher.</p>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#what-is-an-ontology","title":"What is an ontology?","text":"<p>An ontology is a formal representation of a domain of knowledge. It is a hierarchical structure of concepts and relations. The concepts are organized into a hierarchy, where each concept is a subclass of a more general concept. For instance, a wardrobe is a subclass of a piece of furniture. Individual wardrobes, such as yours or mine, are instances of the concept wardrobe, and as such would be represented as Wardrobe nodes in a knowledge graph. In BioCypher, these nodes would additionally inherit the PieceOfFurniture label from the ontological hierarchy of things.</p> <p>Note</p> <p>Why is the class called piece of furniture but the label is PieceOfFurniture?</p> <p>The Biolink model uses two different case notations for its labels: the \"internal\" designation of classes is in lower sentence case (\"protein\", \"pairwise molecular interaction\"), while the \"external\" designation is in PascalCase (\"Protein\", \"PairwiseMolecularInteraction\"). BioCypher uses the same paradigm: in most cases (input, schema configuration, internally), the lower sentence case is used, while in the output (Neo4j labels, file system names) the PascalCase is more suitable; Neo4j labels and system file names don't deal well with spaces and special characters. Therefore, we check the output file names for their compliance with the Neo4j naming rules. All non compliant characters are removed from the file name (e.g. if the ontology class is called \"desk (piece of furniture)\", the brackets would be removed and the file name will be \"DeskPieceOfFurniture\"). We also remove the \"biolink:\" CURIE prefix for use in file names and Neo4j labels.</p> <p>The relations between concepts can also be organized into a hierarchy. In the specific case of a Neo4j graph, however, relationships cannot possess multiple labels; therefore, if concept inheritance is desired for relationships, they need to be \"reified\", i.e., turned into nodes. BioCypher provides a simple way of converting edges to nodes and vice versa (using the <code>represented_as</code> field). For a more in-depth explanation of ontologies, we recommend this introduction.</p>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#how-biocypher-uses-ontologies","title":"How BioCypher uses ontologies","text":"<p>BioCypher is agnostic to the choice of ontology. Practically, we have built our initial projects around the Biolink model, because it provides a large but shallow collection of concepts that are relevant to the biomedical domain. Other examples of generalist ontologies are the Experimental Factor Ontology and the Basic Formal Ontology. To account for the specific requirements of expert systems, it is possible to use multiple ontologies in the same project. For instance, one might want to extend the rather basic classes relating to molecular interactions in Biolink (the most specific being <code>pairwise molecular interaction</code>) with more specific classes from a more domain-specific ontology, such as the EBI molecular interactions ontology (PSI-MI). A different project may need to define very specific genetics concepts, and thus extend the Biolink model at the terminal node <code>sequence variant</code> with the corresponding subtree of the Sequence Ontology. The OBO Foundry and the BioPortal collect many such specialised ontologies.</p> <p>The default format for ingesting ontology definitions into BioCypher is the Web Ontology Language (OWL); BioCypher can read <code>.owl</code>, <code>.rdf</code>, and <code>.ttl</code> files. The preferred way to specify the ontology or ontologies to be used in a project is to specify them in the biocypher configuration file (<code>biocypher_config.yaml</code>). This file is used to specify the location of the ontology files, as well as the root node of the main (\"head\") ontology and join nodes as fusion points for all \"tail\" ontologies. For more info, see the section on hybridising ontologies.</p>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#visualising-ontologies","title":"Visualising ontologies","text":"<p>BioCypher provides a simple way of visualising the ontology hierarchy. This is useful for debugging and for getting a quick overview of the ontology and which parts are actually used in the knowledge graph to be created. Depending on your use case you can either visualise the parts of the ontology used in the knowledge graph (sufficient for most use cases) or the full ontology. If the used ontology is more complex and contains multiple inheritance please refer to the section on visualising complex ontologies.</p>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#visualise-only-the-parts-of-the-ontology-used-in-the-knowledge-graph","title":"Visualise only the parts of the ontology used in the knowledge graph","text":"<p>To get an overview of the structure of our project, we can run the following command via the interface:</p> Visualising the ontology hierarchy<pre><code>from biocypher import BioCypher\nbc = BioCypher(\n    offline=True,  # no need to connect or to load data\n    schema_config_path=\"tutorial/06_schema_config.yaml\",\n)\nbc.show_ontology_structure()\n</code></pre> <p>This will build the ontology scaffold and print a tree visualisation of its hierarchy to the console using the treelib library. You can see this in action in tutorial part 6 (<code>tutorial/06_relationships.py</code>). The output will look something like this:</p> <pre><code>Showing ontology structure, based on Biolink 3.0.3:\nentity\n\u251c\u2500\u2500 association\n\u2502   \u2514\u2500\u2500 gene to gene association\n\u2502       \u2514\u2500\u2500 pairwise gene to gene interaction\n\u2502           \u2514\u2500\u2500 pairwise molecular interaction\n\u2502               \u2514\u2500\u2500 protein protein interaction\n\u251c\u2500\u2500 mixin\n\u2514\u2500\u2500 named thing\n    \u2514\u2500\u2500 biological entity\n        \u2514\u2500\u2500 polypeptide\n            \u2514\u2500\u2500 protein\n                \u251c\u2500\u2500 entrez.protein\n                \u251c\u2500\u2500 protein isoform\n                \u2514\u2500\u2500 uniprot.protein\n</code></pre> <p>Note</p> <p>BioCypher will only show the parts of the ontology that are actually used in the knowledge graph with the exception of intermediary nodes that are needed to build a complete tree. For instance, the <code>protein</code> class is linked to the root class <code>entity</code> via <code>polypeptide</code>, <code>biological entity</code>, and <code>named thing</code>, all of which are not part of the input data.</p>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#visualise-the-full-ontology","title":"Visualise the full ontology","text":"<p>If you want to see the complete ontology tree, you can call <code>show_ontology_structure</code> with the parameter <code>full=True</code>.</p> Visualising the full ontology hierarchy<pre><code>from biocypher import BioCypher\nbc = BioCypher(\n    offline=True,  # no need to connect or to load data\n    schema_config_path=\"tutorial/06_schema_config.yaml\",\n)\nbc.show_ontology_structure(full=True)\n</code></pre>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#visualise-complex-ontologies","title":"Visualise complex ontologies","text":"<p>Not all ontologies can be easily visualised as a tree, such as ontologies with multiple inheritance, where classes in the ontology can have multiple parent classes. This violates the definition of a tree, where each node can only have one parent node. Consequently, ontologies with multiple inheritance cannot be visualised as a tree.</p> <p>BioCypher can still handle these ontologies, and you can call <code>show_ontology_structure()</code> to get a visualisation of the ontology. However, each ontology class will only be added to the hierarchy tree once (a class with multiple parent classes is only placed under one parent in the hierarchy tree). Since this will occur the first time the class is seen, the ontology class might not be placed where you would expect it. This only applies to the visualisation; the underlying ontology is still correct and contains all ontology classes and their relationships.</p> <p>Note</p> <p>When calling <code>show_ontology_structure()</code>, BioCypher automatically checks if the ontology contains multiple inheritance and logs a warning message if so.</p> <p>If you need to get a visualisation of the ontology with multiple inheritance, you can call <code>show_ontology_structure()</code> with the parameter <code>to_disk=/some/path/where_to_store_the_file</code>. This creates a <code>GraphML</code> file and stores it at the specified location.</p>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#using-ontologies-plain-biolink","title":"Using ontologies: plain Biolink","text":"<p>BioCypher maps any input data to the underlying ontology; in the basic case, the Biolink model. This mapping is defined in the schema configuration (<code>schema_config.yaml</code>, see also here). In the simplest case, the representation of a concept in the knowledge graph to be built and the Biolink model class representing this concept are synonymous. For instance, the concept protein is represented by the Biolink class protein. To introduce proteins into the knowledge graph, one would simply define a node constituent with the class label protein. This is the mechanism we implicitly used for proteins in the basic tutorial (part 1); to reiterate:</p> schema_config.yaml<pre><code>protein:\n  represented_as: node\n  # ...\n</code></pre>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#model-extensions","title":"Model extensions","text":"<p>There are multiple reasons why a user might want to modify the basic model of the ontology or ontologies used. A class that is relevant to the user's task might be missing (Explicit inheritance). A class might not be granular enough, and the user would like to split it into subclasses based on distinct inputs (Implicit inheritance). For some very common use cases, we recommend going one step further and, maybe after some testing using the above \"soft\" model extensions, proposing the introduction of a new class to the model itself. For instance, Biolink is an open source community project, and new classes can be requested by opening an issue or filing a pull request directly on the Biolink model GitHub repository. Similar mechanisms apply for OBO Foundry ontologies.</p> <p>BioCypher provides further methods for ontology manipulation. The name of a class of the model may be too unwieldy for the use inside the desired knowledge graph, and the user would like to introduce a synonym/alias (Synonyms). Finally, the user might want to extend the basic model with another, more specialised ontology (Hybridising ontologies).</p>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#explicit-inheritance","title":"Explicit inheritance","text":"<p>Explicit inheritance is the most straightforward way of extending the basic model. It is also the most common use case. For instance, the Biolink model does not contain a class for <code>protein isoform</code>, and neither does it contain a relationship class for <code>protein protein interaction</code>, both of which we have already used in the basic tutorial. Since protein isoforms are specific types of protein, it makes sense to extend the existing Biolink model class <code>protein</code> with the concept of protein isoforms. To do this, we simply add a new class <code>protein isoform</code> to the schema configuration, and specify that it is a subclass of <code>protein</code> using the (optional) <code>is_a</code> field:</p> schema_config.yaml<pre><code>protein isoform:\n  is_a: protein\n  represented_as: node\n  # ...\n</code></pre> <p>Explicit inheritance can also be used to introduce new relationship classes. However, if the output is a Neo4j graph, these relationships must be represented as nodes to provide full functionality, since edges do not allow multiple labels. This does not mean that explicit inheritance cannot be used in edges; it is even recommended to do so to situate all components of the knowledge graph in the ontological hierarchy. However, to have the ancestry represented in the resulting Neo4j graph DB, multiple labels are required. For instance, we have already used the <code>protein protein interaction</code> relationship in the basic tutorial (part 6), making it a child of the Biolink model class <code>pairwise molecular interaction</code>. To reiterate:</p> schema_config.yaml<pre><code>protein protein interaction:\n  is_a: pairwise molecular interaction\n  represented_as: node\n  # ...\n</code></pre> <p>The <code>is_a</code> field can be used to specify multiple inheritance, i.e., multiple ancestor classes and their direct parent-child relationships can be created by specifying multiple classes (as a list) in the <code>is_a</code> field. For instance, if we wanted to further extend the protein-protein interaction with a more specific <code>enzymatic interaction</code> class, we could do so as follows:</p> schema_config.yaml<pre><code>enzymatic interaction:\n  is_a: [protein protein interaction, pairwise molecular interaction]\n  represented_as: node\n  # ...\n</code></pre> <p>Note</p> <p>To create this multiple inheritance chain, we do not require the creation of a <code>protein protein interaction</code> class as shown above; all intermediary classes are automatically created by BioCypher and inserted into the ontological hierarchy. To obtain a continuous ontology tree, the target class (i.e., the last in the list) must be a real Biolink model class.</p>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#implicit-inheritance","title":"Implicit inheritance","text":"<p>The base model (in the standard case, Biolink) can also be extended without specifying an explicit <code>is_a</code> field. This \"implicit\" inheritance happens when a class has multiple input labels that each refer to a distinct preferred identifier. In other words, if both the <code>input_label</code> and the <code>preferred_id</code> fields of a schema configuration class are lists, BioCypher will automatically create a subclass for each of the preferred identifiers. This is demonstrated in part 3 of the basic tutorial.</p> <p>Caution</p> <p>If only the <code>input_label</code> field - but not the <code>preferred_id</code> field - is a list, BioCypher will merge the inputs instead. This is useful for cases where different input streams should be unified under the same class label. See part 2 of the basic tutorial for more information.</p> <p>To make this more concrete, let's consider the example of <code>pathway</code> annotations. There are multiple projects that provide pathway annotations, such as Reactome and Wikipathways, and, in contrast to proteins, pathways are not easily mapped one-to-one. For classes where mapping is difficult or even impossible, we can use implicit subclassing instead. The Biolink model contains a <code>pathway</code> class, which we can use as a parent class of the Reactome and Wikipathways classes; we simply need to provide the pathways as two separate inputs with their own labels (e.g., \"react\" and \"wiki\"), and specify a corresponding list of preferred identifiers in the <code>preferred_id</code> field:</p> schema_config.yaml<pre><code>pathway:\n  represented_as: node\n  preferred_id: [reactome, wikipathways]\n  input_label: [react, wiki]\n  # ...\n</code></pre> <p>This will prompt BioCypher to create two subclasses of <code>pathway</code>, one for each input, and to map the input data to these subclasses. In the resulting knowledge graph, the Reactome and Wikipathways pathways will be represented as distinct classes by prepending the preferred identifier to the class label: <code>Reactome.Pathway</code> and <code>Wikipathways.Pathway</code>. By virtue of BioCypher's multiple labelling paradigm, those nodes will also inherit the <code>Pathway</code> class label as well as all parent labels and mixins of <code>Pathway</code> (<code>BiologicalProcess</code>, etc.). This allows us to query the graph for all <code>Pathway</code> nodes as well as for specific datasets depending on the desired granularity.</p> <p>Note</p> <p>This also works for relationships, but in this case, not the preferred identifiers but the sources (defined in the <code>source</code> field) are used to create the subclasses.</p>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#synonyms","title":"Synonyms","text":"<p>Note: Tutorial Files</p> <p>The code for this tutorial can be found at <code>tutorial/07__synonyms.py</code>. Schema files are at <code>tutorial/07_schema_config.yaml</code>, configuration in <code>tutorial/07_biocypher_config.yaml</code>. Data generation happens in <code>tutorial/data_generator.py</code>.</p> <p>In some cases, an ontology may contain a biological concept, but the name of the concept does for some reason not agree with the users desired knowledge graph structure. For instance, the user may not want to represent protein complexes in the graph as <code>macromolecular complex</code> nodes due to ease of use and/or readability criteria and rather call these nodes <code>complex</code>. In such cases, the user can introduce a synonym for the ontology class. This is done by selecting another, more desirable name for the respective class(es) and specifying the <code>synonym_for</code> field in their schema configuration. In this case, as we would like to represent protein complexes as <code>complex</code> nodes, we can do so as follows:</p> schema_config.yaml<pre><code>complex:\n  synonym_for: macromolecular complex\n  represented_as: node\n  # ...\n</code></pre> <p>Importantly, BioCypher preserves these mappings to enable compatibility between different structural instantiations of the ontology (or combination of ontologies). All entities that are mapped to ontology classes in any way can be harmonised even between different types of concrete representations.</p> <p>Note</p> <p>It is essential that the desired class name is used as the main class key in the schema configuration, and the ontology class name is given in the <code>synonym_for</code> field. The name given in the <code>synonym_for</code> field must be an existing class name (in this example, a real Biolink class).</p> <p>We can visualise the structure of the ontology as we have before. Instead of using <code>bc.show_ontology_structure()</code> however, we can use the <code>bc.summary()</code> method to show the structure and simultaneously check for duplicates and missing labels. This is useful for debugging purposes, and we can see that the import was completed without encountering duplicates, and all labels in the input are accounted for in the schema configuration. We also observe in the tree that the <code>complex</code> class is now a synonym for the <code>macromolecular complex</code> class (their being synonyms indicated as an equals sign):</p> <pre><code>Showing ontology structure based on https://raw.githubusercontent.com/biolink/biolink-model/v3.2.1/biolink-model.owl.ttl\nentity\n\u251c\u2500\u2500 association\n\u2502   \u2514\u2500\u2500 gene to gene association\n\u2502       \u2514\u2500\u2500 pairwise gene to gene interaction\n\u2502           \u2514\u2500\u2500 pairwise molecular interaction\n\u2502               \u2514\u2500\u2500 protein protein interaction\n\u2514\u2500\u2500 named thing\n    \u2514\u2500\u2500 biological entity\n        \u251c\u2500\u2500 complex = macromolecular complex\n        \u2514\u2500\u2500 polypeptide\n            \u2514\u2500\u2500 protein\n                \u251c\u2500\u2500 entrez.protein\n                \u251c\u2500\u2500 protein isoform\n                \u2514\u2500\u2500 uniprot.protein\n</code></pre>"},{"location":"learn/tutorials/tutorial002_handling_ontologies/#hybridising-ontologies","title":"Hybridising ontologies","text":"<p>A broad, general ontology is a useful tool for knowledge representation, but often the task at hand requires more specific and granular concepts. In such cases, it is possible to hybridise the general ontology with a more specific one. For instance, there are many different types of sequence variants in biology, but Biolink only provides a generic \"sequence variant\" class (and it clearly exceeds the scope of Biolink to provide granular classes for all thinkable cases). However, there are many specialist ontologies, such as the Sequence Ontology (SO), which provides a more granular representation of sequence variants, and MONDO, which provides a more granular representation of diseases.</p> <p>To hybridise the Biolink model with the SO and MONDO, we can use the generic ontology adapter class of BioCypher by providing \"tail ontologies\" as dictionaries consisting of an OWL format ontology file and a set of nodes, one in the head ontology (which by default is Biolink), and one in the tail ontology. Each of the tail ontologies will then be joined to the head ontology to form the hybridised ontology at the specified nodes. It is up to the user to make sure that the concept at which the ontologies shall be joined makes sense as a point of contact between the ontologies; ideally, it is the exact same concept.</p> <p>Hint</p> <p>If the concept does not exist in the head ontology, but is a feasible child class of an existing concept, you can set the <code>merge_nodes</code> option to <code>False</code> to prevent the merging of head and tail join nodes, but instead adding the tail join node as a child of the head join node you have specified. For instance, in the example below, we merge <code>sequence variant</code> from Biolink and <code>sequence_variant</code> from Sequence Ontology into a single node, but we add the MONDO subtree of <code>human disease</code> as a child of <code>disease</code> in Biolink.</p> <p><code>merge_nodes</code> is set to <code>True</code> by default, so there is no need to specify it in the configuration file if you want to merge the nodes.</p> <p>The ontology adapter also accepts any arbitrary \"head ontology\" as a base ontology, but if none is provided, the Biolink model is used as the default head ontology. However, it is strongly recommended to explicitly specify your desired ontology version here. These options can be provided to the BioCypher interface as parameters, or as options in the BioCypher configuration file, which is the preferred method for transparency reasons:</p> Using biocypher_config.yaml<pre><code># ...\n\nbiocypher:  # biocypher settings\n\n  # Ontology configuration\n  head_ontology:\n    url: https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl\n    root_node: entity\n\n  tail_ontologies:\n\n    so:\n      url: data/so.owl\n      head_join_node: sequence variant\n      tail_join_node: sequence_variant\n\n    mondo:\n      url: http://purl.obolibrary.org/obo/mondo.owl\n      head_join_node: disease\n      tail_join_node: human disease\n      merge_nodes: false\n\n# ...\n</code></pre> <p>Note</p> <p>The <code>url</code> parameter can be either a local path or a URL to a remote resource.</p> <p>If you need to pass the ontology configuration programmatically, you can do so as follows at BioCypher interface instantiation:</p> Programmatic usage<pre><code>bc = BioCypher(\n    # ...\n\n    head_ontology={\n      'url': 'https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl',\n      'root_node': 'entity',\n    },\n\n    tail_ontologies={\n        'so':\n            {\n                'url': 'test/ontologies/so.owl',\n                'head_join_node': 'sequence variant',\n                'tail_join_node': 'sequence_variant',\n            },\n        'mondo':\n            {\n                'url': 'test/ontologies/mondo.owl',\n                'head_join_node': 'disease',\n                'tail_join_node': 'human disease',\n                'merge_nodes': False,\n            }\n    },\n\n    # ...\n)\n</code></pre>"},{"location":"learn/tutorials/tutorial003_adapters/","title":"Tutorial - Adapters","text":"<p>Existing and planned adapters\u2014overview</p> <p>For a list of existing and planned adapters, please see here. You can also get an overview of pipelines and the adapters they use in our meta graph.</p> <p></p> <p>Project Template</p> <p>To facilitate the creation of a BioCypher pipeline, we have created a template repository that can be used as a starting point for your own adapter. It contains a basic structure for an adapter, as well as a script that can be used as a blueprint for a build pipeline. The repository can be found here.</p> <p>A \"BioCypher adapter\" is a python program responsible for connecting to the BioCypher core and providing it with the data from its associated resource. In doing so, it should adhere to several design principles to ensure simple interoperability between the core and multiple adapters. In essence, an adapter should conform to an interface that is defined by the core to give information about the nodes and edges the adapter provides to enable automatic harmonisation of the contents. An adapter can be \"primary\", i.e., responsible for a single \"atomic\" resource (e.g. UniProt, Reactome, etc.), or \"secondary\", i.e., connecting to a resource that is itself a combination of multiple primary resources (e.g. OmniPath, Open Targets, etc.). Due to extensive prior harmonisation, the latter is often easier to implement and thus is a good starting point that can be subsequently extended to and replaced by primary adapters.</p> <p>Warning</p> <p>The adapter interface is still under development and may change rapidly.</p>"},{"location":"learn/tutorials/tutorial003_adapters/#adapter-philosophy","title":"Adapter philosophy","text":"<p>There are currently two 'flavours' of adapters. The first is simpler and used in workflows that are similar to harmonisation scripts, where the BioCypher interface is instantiated in the same script as the adapter(s). In the second, the BioCypher interface is contained in the adapter class, which makes for a more complex architecture, but allows for more involved workflows. In pseudo-code, the two approaches look like this:</p> Simple adapter<pre><code>from biocypher import BioCypher\nfrom adapter import Adapter\n\nbc = BioCypher()\nadapter = Adapter()\n\nbc.write_nodes(adapter.get_nodes())\n</code></pre> <p>Here, the script file is the central point of control, orchestrating the entire interaction between the BioCypher core and the adapter(s). Examples of this simpler format are the Open Targets KG and the CROssBAR v2.</p> <p>On the other hand, the more involved approach looks like this:</p> Adapter base class<pre><code>from biocypher import BioCypher\n\nclass Adapter:\n    __bc = BioCypher()\n\n    def __init__(self):\n        # setup\n\n    @classmethod\n    def get_biocypher(cls):\n        return Adapter.__bc\n\n    def get_nodes(self):\n        # ...\n        return nodes\n\n    def write_nodes(self):\n        Adapter.get_biocypher.write_nodes(self.get_nodes())\n</code></pre> <p>Here, the adapter class (and adapters inheriting from it) contains a singleton instance of the BioCypher interface. Thus, the adapter needs to provide BioCypher functionality to the outside via dedicated methods. This allows for more complex workflows, for instance, reducing clutter when executing multiple adapters in a single for-loop, or writing from a stream of data, e.g. in a Neo4j transaction (which happens inside the adapter).</p> Main script<pre><code>from adapters import AdapterChild1, AdapterChild2\n\nadapters = [AdapterChild1(), AdapterChild2()]\n\nfor adapter in adapters:\n    adapter.write_nodes()\n</code></pre> <p>Examples of this approach are the IGVF Knowledge Graph and the Clinical Knowledge Graph migration.</p> <p>Note</p> <p>While there are differences in implementation details, both approaches are largely functionally equivalent. At the current time, there is no clear preference for one over the other; both are used. As the ecosystem matures and more high-level functionality is added (e.g. the pipeline), advantages of one approach over the other may become more apparent.</p>"},{"location":"learn/tutorials/tutorial003_adapters/#adapter-functions","title":"Adapter functions","text":"<p>In general, a single adapter fulfils the following tasks:</p> <ol> <li>Loading the data from the primary resource, for instance by using the BioCypher <code>Resource</code> download / caching functions (as used in the CollecTRI example), by using columnar distributed data formats such as Parquet (as in the Open Targets example adapter), by using a running database instance (as in the CKG example adapter), or by simply reading a file from disk (as in the Dependency Map example adapter). Generally, any method that allows the efficient transfer of the data from adapter to BioCypher core is acceptable.</li> </ol> <ol> <li> <p>Passing the data to BioCypher as a stream or list to be written to the used DBMS (or application) via a Python driver (\"online\") or via batch import (e.g. from CSV files). The latter has the advantage of high throughput and a low memory footprint, while the former allows for a more interactive workflow but is often much slower, thus making it better suited for small incremental updates.</p> </li> <li> <p>Providing or connecting to additional functionality that is useful for the creation of knowledge graphs, such as identifier translation (e.g. via pypath.mapping as in the UniProt example adapter), or identifier and prefix standardisation and validation (e.g. via Bioregistry as in the UniProt example adapter and others).</p> </li> </ol> <p>For developers</p> <p>We follow a design philosophy of \"separation of concerns\" in BioCypher. This means that the core should not be concerned with the details of how data is loaded, but only with the data itself. This is why the core does not contain any code for loading data from a resource, but only for writing it to the database. The adapter is responsible for loading the data and passing it to the core, which allows for a more modular design and makes it easier to maintain, extend, and reuse the code.</p> <p>For introduction of new features, we recommend to first implement them in the adapter, and to move them to the core only if they have shown to be useful for multiple adapters.</p>"},{"location":"learn/tutorials/tutorial003_adapters/#1-loading-the-data","title":"1. Loading the Data","text":"<p>Depending on the data source, it is up to the developer of the adapter to find and define a suitable representation to be piped into BioCypher; for instance, in our <code>pypath</code> adapter, we load the entire <code>pypath</code> object into memory to be passed to BioCypher using a generator that evaluates each <code>pypath</code> object and transforms it to the tuple representation described below. This is made possible by the \"pre-harmonised\" form in which the data is represented within <code>pypath</code>. For more heterogeneous data representations, additional transformations may be necessary before piping into BioCypher.</p> <p>For larger datasets, it can be beneficial to adopt a streaming approach or batch processing, as demonstrated in the Open Targets adapter and the CKG adapter. BioCypher can handle input streams of arbitrary length via Python generators.</p>"},{"location":"learn/tutorials/tutorial003_adapters/#2-passing-the-data","title":"2. Passing the Data","text":"<p>We currently pass data into BioCypher as a collection of tuples. Nodes are represented as 3-tuples, containing: - the node ID (unique in the space of the knowledge graph, ideally a CURIE with   a prefix registered in the Bioregistry) - the node type, i.e., its label (this is the string that is mapped to an   ontological class via the <code>input_label</code> field in the schema configuration) - a dictionary of node attributes</p> <p>While edges are represented as 5-tuples, containing: - the (optional) relationship ID (unique in the space of the KG) - the source node ID (referring to a unique node ID in the KG) - the target node ID (referring to a unique node ID in the KG) - the relationship type, i.e., its label (this is the string that is mapped to   an ontological class via the <code>input_label</code> field in the schema configuration) - a dictionary of relationship attributes</p> <p>Standardised node and edge representation</p> <p>This representation will probably be subject to change soon and yield to a more standardised interface for nodes and edges, derived from a BioCypher core class. We refer to this development in an issue.</p>"},{"location":"learn/tutorials/tutorial003_adapters/#strict-mode","title":"Strict mode","text":"<p>We can activate BioCypher strict mode with the <code>strict_mode</code> option in the configuration. In strict mode, BioCypher will raise an error if it encounters a node or edge without data source, version, and licence. These currently need to be provided as part of the node and edge attribute dictionaries, with the reserved keywords <code>source</code>, <code>version</code>, and <code>licence</code> (or <code>license</code>). This may change to a more rigorous implementation in the future.</p>"},{"location":"reference/","title":"Reference Index","text":"<ul> <li> <p> API Reference</p> <p>The BioCypher API reference provides a comprehensive overview of the API, detailing its methods, available parameters, and their functionality. This guide is designed for users who have a foundational understanding of key BioCypher concepts and developers.</p> <p> To the reference guide</p> </li> </ul> <ul> <li> <p> BioCypher Configuration</p> <p>The BioCypher configuration reference offers an in-depth guide to the settings, fields, and parameters that control BioCypher\u2019s behavior and execution. It serves as a resource for customizing BioCypher to fit specific use cases and workflows.</p> <p> To the configuration reference</p> </li> </ul> <ul> <li> <p> Output Configuration</p> <p>The output configuration reference describes the options available for BioCypher's output formats.</p> <p> To the output configuration reference</p> </li> </ul> <ul> <li> <p> Schema Configuration</p> <p>The schema configuration reference explains the parameters used in the YAML file that defines the structure of the data model. It provides guidance on mapping entities and relationships to ensure seamless data integration.</p> <p> To the schema configuration reference</p> </li> </ul>"},{"location":"reference/biocypher-config/","title":"BioCypher Configuration Reference","text":"<p>BioCypher comes with a default set of configuration parameters. You can overwrite them by creating a <code>biocypher_config.yaml</code> file in the root directory or the <code>config</code> directory of your project. You only need to specify the ones you wish to override from default. If you want to create global user settings, you can create a <code>biocypher_config.yaml</code> in your default BioCypher user directory (as found using <code>appdirs.user_config_dir('biocypher')</code>). For instance, on Mac OS, this would be <code>~/Library/Caches/biocypher/biocypher_config.yaml</code>. Finally, you can also point an instance of the BioCypher class to any YAML file using the <code>biocypher_config_path</code> parameter.</p> <p>Note</p> <p>It is important to follow the rules of indentation in the YAML file. BioCypher module configuration is found under the top-level keyword <code>biocypher</code>, while the settings for DBMS systems (e.g., Neo4j) are found under their respective keywords (e.g., <code>neo4j</code>).</p> <p>Quote characters</p> <p>If possible, avoid using quote characters in your YAML files. If you need to quote, for instance a tab delimiter (<code>\\t</code>), use single quotes (<code>'</code>), since double quotes (<code>\"</code>) allow parsing of escape characters in YAML, which can cause issues downstream. It is safe to use double quotes to quote a single quote character (<code>\"'\"</code>).</p> <p>Configuration files are read in the order <code>default -&gt; user level -&gt; project level</code>, with the later ones overriding the preceding.</p>"},{"location":"reference/biocypher-config/#configuration-structure","title":"Configuration Structure","text":"<p>The configuration file is structured into several sections:</p> <ol> <li>BioCypher Core Settings (<code>biocypher:</code>) - Core settings for BioCypher functionality<ul> <li>choose <code>dbms</code> to select one of either the available DBMSs (2.) or data models (3.)</li> </ul> </li> <li>Database Management Systems - Settings specific to each supported DBMS:<ul> <li>Neo4j (<code>neo4j:</code>)</li> <li>PostgreSQL (<code>postgresql:</code>)</li> <li>SQLite (<code>sqlite:</code>)</li> </ul> </li> <li>Data Models - Settings for different data models:<ul> <li>RDF (<code>rdf:</code>)</li> <li>NetworkX (<code>networkx:</code>)</li> <li>CSV (<code>csv:</code>)</li> </ul> </li> </ol>"},{"location":"reference/biocypher-config/#default-configuration","title":"Default Configuration","text":"<p>Below is the default configuration that comes with BioCypher. This represents all available options with their default values. Some options (like tail ontologies) are commented out in the default configuration as they are optional and specific to certain use cases.</p> Default biocypher_config.yaml<pre><code>biocypher:\n  #---- REQUIRED PARAMETERS\n\n  dbms: neo4j\n  schema_config_path: config/schema_config.yaml\n  offline: true\n  strict_mode: false\n  head_ontology:\n    url: https://github.com/biolink/biolink-model/raw/v3.2.1/biolink-model.owl.ttl\n    root_node: entity\n    switch_label_and_id: true\n\n  #---- OPTIONAL PARAMETERS\n  log_to_disk: true\n\n  debug: true\n\n  log_directory: biocypher-log\n\n  output_directory: biocypher-out\n\n  cache_directory: .cache\n\n  #---- OPTIONAL TAIL ONTOLOGIES\n\n  # tail_ontologies:\n  #   so:\n  #     url: test/ontologies/so.owl\n  #     head_join_node: sequence variant\n  #     tail_join_node: sequence_variant\n  #     switch_label_and_id: true\n  #   mondo:\n  #     url: test/ontologies/mondo.owl\n  #     head_join_node: disease\n  #     tail_join_node: disease\n  #     switch_label_and_id: true\n\n#-------------------------------------------------------------------\n#-----------------       OUTPUT Configuration      -----------------\n#-------------------------------------------------------------------\n#---- NEO4J database management system\nneo4j:\n  database_name: neo4j\n  wipe: true\n\n  uri: neo4j://localhost:7687\n  user: neo4j\n  password: neo4j\n\n  delimiter: \";\"\n  array_delimiter: \"|\"\n  quote_character: \"'\"\n\n  multi_db: true\n\n  skip_duplicate_nodes: false\n  skip_bad_relationships: false\n\n  # import_call_bin_prefix: bin/\n  # import_call_file_prefix: path/to/files/\n\n#---- PostgreSQL database management system\npostgresql:\n  database_name: postgres\n\n  host: localhost # host\n  port: 5432 # port\n\n  user: postgres\n  password: postgres # password\n\n  quote_character: '\"'\n  delimiter: '\\t'\n  # import_call_bin_prefix: '' # path to \"psql\"\n  # import_call_file_prefix: '/path/to/files'\n\n#---- SQLite database management system\nsqlite:\n  ### SQLite configuration ###\n\n  # SQLite connection credentials\n  database_name: sqlite.db # DB name\n\n  # SQLite import batch writer settings\n  quote_character: '\"'\n  delimiter: '\\t'\n  # import_call_bin_prefix: '' # path to \"sqlite3\"\n  # import_call_file_prefix: '/path/to/files'\n\n#---- RDF (Resource Description Framework) data model\nrdf:\n  ### RDF configuration ###\n  rdf_format: turtle\n\n#---- NetworkX graph data model\nnetworkx:\n  ### NetworkX configuration ###\n  some_config: some_value # placeholder for technical reasons TODO\n\n#---- CSV (Comma-Separated Values) text file format\ncsv:\n  ### CSV/Pandas configuration ###\n  delimiter: \",\"\n</code></pre>"},{"location":"reference/biocypher-config/#configuration-parameters-reference","title":"Configuration Parameters Reference","text":""},{"location":"reference/biocypher-config/#biocypher-core-parameters","title":"BioCypher Core Parameters","text":"Parameter Description Type Default <code>dbms</code> Specifies which database management system to use string <code>\"neo4j\"</code> <code>schema_config_path</code> Path to the schema configuration file string <code>\"config/schema_config.yaml\"</code> <code>offline</code> Whether to run in offline mode (no running DBMS or in-memory object) boolean <code>true</code> <code>strict_mode</code> Whether to enforce strict schema validation boolean <code>false</code> <code>head_ontology.url</code> URL or file path to the main ontology file string Biolink model URL <code>head_ontology.root_node</code> The root node of the ontology to use string <code>\"entity\"</code> <code>head_ontology.switch_label_and_id</code> Whether to switch label and ID in the ontology boolean <code>true</code> <code>log_to_disk</code> Whether to save logs to disk boolean <code>true</code> <code>debug</code> Whether to enable debug logging boolean <code>true</code> <code>log_directory</code> Directory for log files string <code>\"biocypher-log\"</code> <code>output_directory</code> Directory for output files string <code>\"biocypher-out\"</code> <code>cache_directory</code> Directory for cache files string <code>\".cache\"</code> <code>tail_ontologies</code> Additional ontologies to use (optional) object -"},{"location":"reference/biocypher-config/#neo4j-configuration","title":"Neo4j Configuration","text":"Parameter Description Type Default <code>database_name</code> Name of the Neo4j database string <code>\"neo4j\"</code> <code>wipe</code> Whether to wipe the database before import boolean <code>true</code> <code>uri</code> Connection URI for Neo4j string <code>\"neo4j://localhost:7687\"</code> <code>user</code> Username for Neo4j authentication string <code>\"neo4j\"</code> <code>password</code> Password for Neo4j authentication string <code>\"neo4j\"</code> <code>delimiter</code> Field delimiter for CSV import files string <code>\";\"</code> <code>array_delimiter</code> Delimiter for array values string <code>\"\\|\"</code> <code>quote_character</code> Character used for quoting string values string <code>\"'\"</code> <code>multi_db</code> Whether to use multi-database support boolean <code>true</code> <code>skip_duplicate_nodes</code> Whether to skip duplicate nodes during import boolean <code>false</code> <code>skip_bad_relationships</code> Whether to skip relationships with missing endpoints boolean <code>false</code> <code>import_call_bin_prefix</code> Prefix for the import command binary (optional) string - <code>import_call_file_prefix</code> Prefix for import files (optional) string -"},{"location":"reference/biocypher-config/#postgresql-configuration","title":"PostgreSQL Configuration","text":"Parameter Description Type Default <code>database_name</code> Name of the PostgreSQL database string <code>\"postgres\"</code> <code>host</code> Host address for PostgreSQL server string <code>\"localhost\"</code> <code>port</code> Port for PostgreSQL server integer <code>5432</code> <code>user</code> Username for PostgreSQL authentication string <code>\"postgres\"</code> <code>password</code> Password for PostgreSQL authentication string <code>\"postgres\"</code> <code>quote_character</code> Character used for quoting identifiers string <code>\"\\\"\"</code> <code>delimiter</code> Field delimiter for import files string <code>\"\\t\"</code> <code>import_call_bin_prefix</code> Path to psql (optional) string - <code>import_call_file_prefix</code> Prefix for import files (optional) string -"},{"location":"reference/biocypher-config/#sqlite-configuration","title":"SQLite Configuration","text":"Parameter Description Type Default <code>database_name</code> Name of the SQLite database file string <code>\"sqlite.db\"</code> <code>quote_character</code> Character used for quoting identifiers string <code>\"\\\"\"</code> <code>delimiter</code> Field delimiter for import files string <code>\"\\t\"</code> <code>import_call_bin_prefix</code> Path to sqlite3 (optional) string - <code>import_call_file_prefix</code> Prefix for import files (optional) string -"},{"location":"reference/biocypher-config/#rdf-configuration","title":"RDF Configuration","text":"Parameter Description Type Default <code>rdf_format</code> Format for RDF output string <code>\"turtle\"</code>"},{"location":"reference/biocypher-config/#networkx-configuration","title":"NetworkX Configuration","text":"Parameter Description Type Default <code>some_config</code> Placeholder configuration string <code>\"some_value\"</code>"},{"location":"reference/biocypher-config/#csv-configuration","title":"CSV Configuration","text":"Parameter Description Type Default <code>delimiter</code> Field delimiter for CSV files string <code>\",\"</code>"},{"location":"reference/schema-config/","title":"Schema Configuration Reference","text":""},{"location":"reference/schema-config/#purpose","title":"Purpose:","text":"<p>The schema file defines the structure of a BioCypher knowledge graph, specifying which entities and relationships are included and how they are represented. It ensures alignment with biomedical ontologies like the Biolink model, serving as a blueprint for constructing a domain-specific knowledge graph.</p>"},{"location":"reference/schema-config/#convention-for-naming","title":"Convention for naming:","text":"<ul> <li>Entities in the <code>schema_config.yaml</code> file should be represented in lower sentence case (e.g., <code>small molecule</code>), similar to the internal representation in Biolink.</li> <li>Class names, in file names, and property graph labels they are represented in PascalCase (e.g., <code>SmallMolecule</code>).</li> </ul>"},{"location":"reference/schema-config/#skeleton","title":"Skeleton:","text":"<pre><code>#-------------------------------------------------------------------\n#---- Title: Schema Configuration file example\n#---- Authors: &lt;author 1&gt;, &lt;author 2&gt;\n#---- Description: Schema to load information related to proteins, and\n#                  and their interactions.\n#\n#-------------------------------------------------------------------\n\n#-------------------------------------------------------------------\n#-------------------------      NODES      -------------------------\n#-------------------------------------------------------------------\n#=========    PARENT NODES\nprotein:\n  represented_as: node\n  preferred_id: [uniprot, entrez]\n  input_label: [uniprot_protein, entrez_protein]\n  properties:\n    sequence: str\n    description: str\n    taxon: str\n    mass: int\n\n#=========    INHERITED NODES\nprotein isoform:\n  is_a: protein\n  inherit_properties: true\n  represented_as: node\n  preferred_id: uniprot\n  input_label: uniprot_isoform\n\n#-------------------------------------------------------------------\n#------------------      RELATIONSHIPS (EDGES)     -----------------\n#-------------------------------------------------------------------\n#=========    PARENT EDGES\nprotein protein interaction:\n  is_a: pairwise molecular interaction\n  represented_as: edge\n  preferred_id: intact\n  input_label: interacts_with\n  properties:\n      method: str\n      source: str\n\n#=========    INHERITED EDGES\n\n#=========    EDGES AS NODES\n\n\n#--------------------------------------------------------------------\n#---- Dictionary of custom keywords: add additional keywords if you\n#     need it. Please document each new keyword as in the following\n#     template. DO NOT DELETE the hash symbol (#) in each line.\n\n# &lt;keyword's name&gt;\n#    Description:\n#    Possible values:\n#        - possible value 1 [*datatype*]\n#        - possible value 2 [*datatype*]\n#\n</code></pre>"},{"location":"reference/schema-config/#fields-reference","title":"Fields reference:","text":""},{"location":"reference/schema-config/#exclude_properties","title":"<code>exclude_properties</code>","text":"<ul> <li>Description: Specifies properties that should be excluded from the current entity or relation, preventing them from being inherited or used.</li> <li>Possible values:</li> <li>A list of property names to be excluded, such as <code>[category, references]</code>.</li> </ul>"},{"location":"reference/schema-config/#inherit_properties","title":"<code>inherit_properties</code>","text":"<ul> <li>Description:  Defines whether and which properties should be inherited from a parent entity. This applies when using <code>is_a</code>.</li> <li>Possible values:</li> <li><code>true</code></li> <li><code>false</code></li> </ul>"},{"location":"reference/schema-config/#input_label","title":"<code>input_label</code>","text":"<ul> <li>Description: A human-readable label used when referring to this entity in a UI or input form.</li> <li>Possible values:</li> <li>A string, such as <code>Gene Symbol</code>, <code>TF Category</code>, or <code>Regulation Type</code>.</li> </ul>"},{"location":"reference/schema-config/#is_a","title":"<code>is_a</code>","text":"<ul> <li>Description: Defines a hierarchical relationship by specifying the parent class from which the current entity inherits.</li> <li>Possible values:</li> <li>A reference to another entity, such as <code>gene</code> or <code>pairwise gene to gene interaction</code>.</li> </ul>"},{"location":"reference/schema-config/#label_as_edge","title":"<code>label_as_edge</code>","text":"<ul> <li>Description: Indicates the label to be used in the output graph if the entity is represented as an edge. Used to override the default name (PascalCase conversion of the class). Typical use case: adhere to labelled property graph naming scheme (uppercase verbs). Only applied to edges.</li> <li>Possible values:</li> <li>A string describing the edge label, for instance, <code>PERTURBED_IN</code> or <code>TARGETS</code>.</li> </ul>"},{"location":"reference/schema-config/#preferred_id","title":"<code>preferred_id</code>","text":"<ul> <li>Description: Specifies the primary identifier used for this entity, typically referencing a standardized database ID.</li> <li>Possible values:</li> <li>A string referring to a standardized identifier, such as <code>hgnc.symbol</code>.</li> </ul>"},{"location":"reference/schema-config/#properties","title":"<code>properties</code>","text":"<ul> <li>Description: Defines attributes associated with an entity or relationship.</li> <li>Possible values:</li> <li>String: <code>str</code> (or <code>string</code>)</li> <li>Integer: <code>int</code> (or <code>integer</code>, <code>long</code>).</li> <li>String: <code>float</code> (or <code>double</code>, <code>dbl</code>).</li> <li>Boolean: <code>bool</code> (or <code>boolean</code>).</li> <li>Arrays of any of these types (indicated by square brackets, e.g. <code>string[]</code>).</li> </ul>"},{"location":"reference/schema-config/#represented_as","title":"<code>represented_as</code>","text":"<ul> <li>Description: Specifies whether the entity should be represented as a node or an edge in a graph-based structure. An entity can be represented as an edge only if source and target IDs are provided in the input data stream. Conversely, relationships can be represented as either a node or an edge, depending on the desired output.</li> <li>Possible values:</li> <li><code>node</code></li> <li><code>edge</code></li> </ul>"},{"location":"reference/schema-config/#source","title":"<code>source</code>","text":"<ul> <li>Description: [optional] For relationships (edges), defines the starting entity in the relationship.</li> <li>Possible values:</li> <li>A reference to an entity, such as <code>transcription factor</code>.</li> </ul>"},{"location":"reference/schema-config/#synonym_for","title":"<code>synonym_for</code>","text":"<ul> <li>Description: Indicates that this entity or property serves as a synonym for another term or entity.</li> <li>Possible values:</li> <li>A string or list of strings representing alternative names, such as <code>[TF, transcription regulator]</code>.</li> </ul>"},{"location":"reference/schema-config/#target","title":"<code>target</code>","text":"<ul> <li>Description: [optional] For relationships (edges), defines the destination entity in the relationship.</li> <li>Possible values:</li> <li>A reference to an entity, such as <code>gene</code>.</li> </ul>"},{"location":"reference/schema-config/#add-custom-fields","title":"Add custom fields","text":"<p>Tip</p> <p>Do not forget to document your custom keywords at the end of the schema config file, this is especially useful if you share your schema configuration file with others. They will understand what is the purpose of those new keywords.</p> <p>You can use other keywords for local functionalities without interfering with the default ones. For instance, a particular user has added the <code>db_collection_name</code> field for its own purposes.</p> Example: schema configuration with a custom keyword<pre><code>#...\nprotein:\n  represented_as: node\n  preferred_id: uniprot\n  input_label: protein\n  db_collection_name: proteins\n  properties:\n    name: str\n    score: float\n    taxon: int\n    genes: str[]\n#...\n</code></pre>"},{"location":"reference/outputs/","title":"Overview","text":"<p>BioCypher development was initially centred around a Neo4j graph database output due to the migration of OmniPath to a Neo4j backend. Importantly, we understand BioCypher as an abstraction of the build process of a biomedical knowledge graph, and thus are open towards any output format for the knowledge representation.</p> <p>The used output format is specified via the <code>dbms</code> parameter in the <code>biocypher_config.yaml</code> (see the Configuration for an example).  Currently supported are:</p> <ul> <li><code>neo4j</code></li> <li><code>arangodb</code></li> <li><code>rdf</code></li> <li><code>owl</code></li> <li><code>postgres</code></li> <li><code>sqlite</code></li> <li><code>tabular</code></li> <li><code>csv</code></li> <li><code>pandas</code></li> <li><code>networkx</code></li> </ul> <p>Furthermore, you can specify whether to use the <code>offline</code> or <code>online</code> mode.</p> <ul> <li> <p>For the online mode set <code>offline: false</code>. The behavior of the online mode   depends on the specified <code>dbms</code>. If the specified <code>dbms</code> is an in-memory   database (e.g. <code>csv</code>, <code>networkx</code>), the in-memory Knowledge Graph can   directly be accessed from the BioCypher instance. If the specified <code>dbms</code> is   a database (e.g. <code>neo4j</code>), the online mode requires a running database   instance and BioCypher will connect to this instance and directly writes the   output to the database.</p> </li> <li> <p>For the offline mode set <code>offline: true</code>. BioCypher will <code>write</code> the   knowledge graph to files in a designated output folder (standard being   <code>biocypher-out/</code> and the current datetime). Furthermore, you can generate a   bash script to insert the knowledge graph files into the specified <code>dbms</code> by   running <code>bc.write_import_call()</code>.</p> </li> </ul> <p>Warning</p> <p>The <code>online</code> mode is currently only supported for <code>neo4j</code>, <code>tabular</code>, <code>csv</code>, <code>pandas</code>, and <code>networkx</code>.</p> <p>Details about the usage of the <code>online</code> and <code>offline</code> mode and the different supported output formats are described on individual pages for each output format.</p>"},{"location":"reference/outputs/#available-output-formats","title":"Available Output formats","text":"Method Offline mode Online mode (in-memory) <code>Neo4j</code> <code>ArangoDB</code>  (pending) <code>RDF</code> / <code>OWL</code>  (pending) <code>PostgreSQL</code>  (pending) <code>SQLite</code>  (pending) <code>Tabular</code> <code>CSV</code> <code>Pandas</code> <code>NetworkX</code>"},{"location":"reference/outputs/arangodb-output/","title":"ArangoDB","text":"<p>The documentation related to ArangoDB in BioCypher is under construction</p>"},{"location":"reference/outputs/neo4j-output/","title":"Neo4j","text":"<p>In the following section, we give an overview of interacting with Neo4j from the perspective of BioCypher, but we refer the reader to the Neo4j documentation for more details.</p>"},{"location":"reference/outputs/neo4j-output/#install-neo4j","title":"Install Neo4j","text":"<p>Neo4j provide a Neo4j Desktop application that can be used to create a local instance of Neo4j. The desktop application provides information about the DBMS folder and can open a terminal at the DBMS root location.</p> <p>Neo4j is also available as a command line interface (CLI) tool. To use the CLI with the BioCypher admin import call, directory structures and permissions need to be set up correctly. The Neo4j CLI tool can be downloaded from the Neo4j download center. Please follow the Neo4j documentation for correct setup and usage on your system.</p> <p>Be mindful that different versions of Neo4j may differ in features and thus are also documented differently.</p> <p>Note</p> <p>We use the APOC library for Neo4j, which is not included automatically, but needs to be installed as a plugin to the DMBS. For more information, please refer to the APOC documentation.</p>"},{"location":"reference/outputs/neo4j-output/#neo4j-settings","title":"Neo4j settings","text":"<p>To overwrite the standard settings of Neo4j, add a <code>neo4j</code> section to the <code>biocypher_config.yaml</code> file.  The following settings are possible:</p> biocypher_config.yaml<pre><code>neo4j:  ### Neo4j configuration ###\n\n  # Database name\n  database_name: neo4j\n\n  # Wipe DB before import (offline mode: --force)\n  wipe: true\n\n  # Neo4j authentication\n  uri: neo4j://localhost:7687\n  user: neo4j\n  password: neo4j\n\n  # Neo4j admin import batch writer settings\n  delimiter: ';'\n  array_delimiter: '|'\n  quote_character: \"'\"\n\n  # How to write the labels in the export files.\n  labels_order: \"Ascending\" # Default: From more specific to more generic.\n  # Or:\n  # labels_order: \"Descending\" # From more generic to more specific.\n  # labels_order: \"Alphabetical\" # Alphabetically. Legacy option.\n  # labels_order: \"Leaves\" # Only the more specific label.\n\n\n  # MultiDB functionality\n  # Set to false for using community edition or older versions of Neo4j\n  multi_db: true\n\n  # Import options\n  skip_duplicate_nodes: false\n  skip_bad_relationships: false\n\n  # Import call prefixes to adjust the autogenerated shell script\n  import_call_bin_prefix: bin/  # path to \"neo4j-admin\"\n  import_call_file_prefix: path/to/files/\n</code></pre>"},{"location":"reference/outputs/neo4j-output/#offline-mode","title":"Offline mode","text":"<p>Particularly if the data are very extensive (or performance is of the utmost priority), BioCypher can be used to facilitate a speedy and safe import of the data using the <code>neo4j-admin import</code> console command. Admin Import is a particularly fast method of writing data to a newly created graph database (the database needs to be completely empty) that gains most of its performance advantage from turning off safety features regarding input data consistency. Therefore, a sound and consistent representation of the nodes and edges in the graph is of high importance in this process, which is why the BioCypher export functionality has been specifically designed to perform type and content checking for all data to be written to the graph.</p> <p>Data input from the source database is exactly as in the case of interacting with a running database, with the data representation being converted to a series of CSV files in a designated output folder (standard being <code>biocypher-out/</code> and the current datetime).  BioCypher creates separate header and data files for all node and edge types to be represented in the graph. Additionally, it creates a file called <code>neo4j-admin-import-call.sh</code> containing the console command for creating a new database, which only has to be executed from the directory of the currently running Neo4j database.</p> <p>The name of the database to be created is given by the <code>db_name</code> setting, and can be arbitrary. In case the <code>db_name</code> is not the default Neo4j database name, <code>neo4j</code>, the database needs to be created in Neo4j before or after using the <code>neo4j-admin import</code> statement. This can be done by executing, in the running database (<code>&lt;db_name&gt;</code> being the name assigned in the method):</p> <ol> <li><code>:use system</code></li> <li><code>create database &lt;db_name&gt;</code></li> <li><code>:use &lt;db_name&gt;</code></li> </ol> <p>After writing knowledge graph files with BioCypher in offline mode for the Neo4j DBMS (database management system), the graph can now be imported into Neo4j using the <code>neo4j-admin</code> command line tool. This is not necessary if the graph is created in online mode. For convenience, BioCypher provides the command line call required to import the data into Neo4j:</p> <pre><code>bc.write_import_call()\n</code></pre> <p>This creates an executable shell script in the output directory that can be executed from the location of the database folder (or copied into the Neo4j terminal) to import the graph into Neo4j. Since BioCypher creates separate header and data files for each entity type, the import call conveniently aggregates this information into one command, detailing the location of all files on disk, so no data need to be copied around.</p> <p>Note</p> <p>The generated import call differs between Neo4j version 4 and 5. Starting from major version 5, Neo4j <code>import</code> command needs the <code>database</code> scope. BioCypher takes care of this. The generated import script <code>neo4j-admin-import-call.sh</code> first checks the Neo4j database version and uses the correct import statement for the detected version. Therefore, make sure to run the script from the targeted DBMS root location.</p> <p>Neo4j can manage multiple projects, each with multiple DBMS (database management system) instances, each of which can house multiple databases. The screenshot below shows a project managed by Neo4j Desktop (project name \"BioCypher\") containing a DBMS instance (called \"Test DBMS\") with multiple named databases in it: the non-removable \"system\" DB, the default \"neo4j\" DB, and several user-created databases.</p> <p></p> <p>Crucially, the import call generated by BioCypher needs to be executed by the <code>neo4j-admin</code> binary in the DBMS folder (in the <code>bin/</code> directory of the root of the DBMS folder). The DBMS folder is the folder that contains the <code>data/</code> directory, which in turn contains the <code>databases/</code> folder, which is where the graph data is stored in individual folders corresponding to the DB names in the DBMS. On Neo4j Desktop, you can open a terminal at the DBMS root location by clicking on the three dots next to the DBMS name and selecting \"Terminal\" (see screenshot below).</p> <p></p> <p>Since the import call should be executed in the root of the DMBS folder, BioCypher generates the import call starting with <code>bin/neo4j-admin</code>, which can be copied into the terminal opened at the DBMS root location. For other operating systems and Neo4j installations (e.g. in Docker containers), please refer to the Neo4j documentation to find the correct location of your DMBS. We are working on extensions of the BioCypher process to improve interoperability with Neo4j as well as other storage systems.</p>"},{"location":"reference/outputs/neo4j-output/#online-mode","title":"Online mode","text":"<p>BioCypher provides a Python driver for interacting with Neo4j, which is accessed through the <code>BioCypher</code> class when setting <code>offline</code> to <code>False</code>. More details can be found in the API docs.</p> <p>If there exists no BioCypher graph in the currently active database, or if the user explicitly specifies so using the <code>wipe</code> attribute of the driver, a new BioCypher database is created using the schema configuration specified in the schema-config.yaml.</p>"},{"location":"reference/outputs/neo4j-output/#note-on-labels-order","title":"Note on labels order","text":"<p>Neo4j does not support managing the hierarchy of types of the vocabulary given by the input ontology. What it does is to attach to nodes and edges each type label of all the ancestors in the types hierarchy.</p> <p>By default, the Neo4j driver exports those type labels as a sorted list (<code>Ascending</code>), which means the labels are ordered from most specific to most general in the ontology. You can set <code>labels_order</code> to either <code>Ascending</code>, <code>Descending</code>, or <code>Alphabetical</code>.</p> <p>To get even simpler labels, you can set <code>labels_order: Leaves</code>, which will remove all but the most specific type label (the \"leaf\" of the types tree). Be warned that the resulting export will completely lose the ontological information, hence making it impossible to query the graph on high-level types.</p>"},{"location":"reference/outputs/networkx-output/","title":"NetworkX","text":"<p>When setting the <code>dbms</code> parameter in the <code>biocypher_config.yaml</code> to <code>networkx</code>, the BioCypher Knowledge Graph is transformed into a NetworkX DiGraph.</p>"},{"location":"reference/outputs/networkx-output/#networkx-settings","title":"NetworkX settings","text":"<p>At the moment, we don't implement any specific configuration options for NetworkX. Feel free to reach out and create issues or pull requests if you need specific configuration options. They would be added to the configuration similarly as for other outputs:</p> biocypher_config.yaml<pre><code>networkx:\n  ### NetworkX configuration ###\n</code></pre>"},{"location":"reference/outputs/networkx-output/#offline-mode","title":"Offline mode","text":""},{"location":"reference/outputs/networkx-output/#running-biocypher","title":"Running BioCypher","text":"<p>After running BioCypher with the <code>offline</code> parameter set to <code>true</code> and the <code>dbms</code> set to <code>networkx</code>, the output folder contains:</p> <ul> <li> <p><code>networkx_graph.pkl</code>: The pickle file containing with the BioCypher Knowledge Graph as NetworkX DiGraph.</p> </li> <li> <p><code>import_networkx.py</code>: A Python script to load the created pickle file.</p> </li> </ul> <p>Note</p> <p>If any of the files is missing make sure to run <code>bc.write_import_call()</code>.</p>"},{"location":"reference/outputs/networkx-output/#online-mode","title":"Online mode","text":"<p>After running BioCypher with the <code>offline</code> parameter set to <code>false</code> and the <code>dbms</code> set to <code>networkx</code>, you can get the in-memory networkx representation of the Knowledge Graph directly from BioCypher by calling the <code>get_kg()</code> function.</p> <pre><code># Initialize BioCypher\nbc = BioCypher(\n    biocypher_config_path=\"biocypher_config.yaml\",\n    schema_config_path=\"schema_config.yaml\",\n)\n\n# Add nodes and edges\nbc.add_nodes(nodes)\nbc.add_edges(edges)\n\n# Get the in-memory representation of the Knowledge Graph\nin_memory_kg = bc.get_kg()\n</code></pre>"},{"location":"reference/outputs/owl-output/","title":"Web Ontology Language (OWL)","text":"<p>The Web Ontology Language (OWL) is a (family of) knowledge representation language(s) for authoring ontologies. BioCypher can use taxonomies written in OWL as an input, and it can also output a knowledge graph in an OWL file.</p> <p>OWL is one of the most common knowledge representation languages. It is built on the Resource Description Framework (RDF) and is partly compatible with the RDF Schema data model. It can be serialized in several formats (the most well-known being XML and Turtle). The Prot\u00e9g\u00e9 software is the de facto standard graphical user interface to design OWL ontologies.</p> <p>In BioCypher, selecting the <code>owl</code> output format will call the <code>_OWLWriter</code> class and generate a self-contained OWL file. The file is said to be \"self-contained\" because it holds both the vocabulary (i.e. a part of the hierarchy of classes from the input ontology) and the instances (i.e. \"nodes\", for BioCypher).</p>"},{"location":"reference/outputs/owl-output/#edge-model","title":"Edge Model","text":"<p>The behavior of edge creation in the RDF output relies mainly on the <code>edge_model</code> parameter, which can take two values: \"ObjectProperty\" or \"Association\".</p>"},{"location":"reference/outputs/owl-output/#note-on-vocabulary","title":"Note on vocabulary","text":"<p>To understand the following rationale, note that OWL uses a different vocabulary than BioCypher (which is more aligned with labelled property graphs); a rough translation is:</p> BioCypher OWL node individual edge object property label class property annotation / data property ID label / IRI <p>Note</p> <p>There is a particular danger of ambiguity in the term \"label\"; in labelled property graphs, this is the type or class of entity, while in OWL it refers to the identifier of a single entity.</p>"},{"location":"reference/outputs/owl-output/#in-a-nutshell","title":"In a nutshell","text":"<p>When using <code>edge_model: ObjectProperty</code>, the resulting ontology will follow more closely the spirit of the OWL modelling approach; but the ID and the properties attached to the edges are lost.</p> <p>Example:</p> <pre><code>graph LR\n    A[\"My_source&lt;br/&gt;&lt;i&gt;my_prop: this&lt;/i&gt;\"] --&gt;|toward| B[\"My_target\"]</code></pre> <p>When using <code>edge_model: Association</code>, the edges are created as OWL individuals, with attached annotations, and an IRI; however, this introduces two object properties around a new individual, between pairs of individuals. This is very similar to the \"reification\" that BioCypher does for relationships that are set to <code>represented_as: node</code> in the schema configuration.</p> <p>Example:</p> <pre><code>graph LR\n    A[\"My_source&lt;br/&gt;&lt;i&gt;my_prop: this&lt;/i&gt;\"] --&gt;|edge_source| B[\"My_edge&lt;br/&gt;&lt;i&gt;my_edge_prop: that&lt;/i&gt;\"] --&gt;|edge_target| C[\"My_target\"]</code></pre>"},{"location":"reference/outputs/owl-output/#objectproperty","title":"ObjectProperty","text":"<p>This edge model translates BioCypher's edges into OWL's \"object properties\" (if they are available under the selected root term). Object properties are the natural way to model edges in OWL, but they do not support annotation, thus being incompatible with having BioCypher's \"properties\" on edges.</p>"},{"location":"reference/outputs/owl-output/#example","title":"Example","text":"<p>For instance, the following BioCypher tuples (two nodes and one edge): <pre><code># Nodes:\n# ID           label           properties\n(\"My_source\", \"thisNodeType\", {\"my_prop\":\"this\"}),\n(\"My_target\", \"thatNodeType\", {}),\n\n# Edge:\n# ID         source ID    target ID    properties               label\n(\"My_edge\", \"My_source\", \"My_target\", {\"my_edge_prop\":\"that\"}, \"toward\")\n</code></pre> would be translated into the following OWL statements (here shown in the Turtle format, not showing the taxonomy ancestors): <pre><code># Declaration of types:\n:toward a owl:ObjectProperty ;\n    rdfs:range :thisNodeType ;\n    rdfs:domain :thatNodetype ;\n    rdfs:subPropertyOf owl:topObjectProperty ;\n\n# Actual data:\n:My_source a :thisNodeType, owl:NamedIndividual ;\n    biocypher:my_prop \"this\" ;\n    :toward :My_target\n\n:My_target a :thatNodeType, owl:NamedIndividual ;\n</code></pre> Note how the properties of the edge are lost.</p>"},{"location":"reference/outputs/owl-output/#root-node-and-objectproperty","title":"Root node and ObjectProperty","text":"<p>As most OWL files do not model a common term on top of both <code>owl:topObjectProperty</code> and <code>owl:Thing</code>, you may need to ensure that the input OWL contains a \"meta-root\", that is, a common ancestor honoring both:</p> <ul> <li>owl:Thing rdfs:subClassOf  <li>owl:topObjectProperty rdfs:subPropertyOf  <p>It is this meta-root that you should select as a <code>root_node</code> in your BioCypher configuration.</p> <p>For example, a classical OWL taxonomy is often structured like: <pre><code> - owl:Thing\n   \u251c Entity\n   \u251c My_class\n   \u2514 etc.\n - owl:topObjectProperty\n   \u251c My_link_type\n   \u2514 etc.\n</code></pre></p> <p>To allow BioCypher to \"see\" both the <code>owl:Thing</code> and <code>owl:topObjectProperty</code> subtrees, you need to add your own root node: <pre><code>- my_meta_root\n  \u251c owl:Thing\n  \u2502 \u251c Entity\n  \u2502 \u251c My_class\n  \u2502 \u2514 etc.\n  \u2514 owl:topObjectProperty\n    \u251c My_link_type\n    \u2514 etc.\n</code></pre> and then set <code>root_node: my_meta_root</code> in BioCypher's configuration.</p>"},{"location":"reference/outputs/owl-output/#association","title":"Association","text":"<p>This edge model (the default) translates BioCypher's edges into OWL's class instances. Those edge instances are inserted in between the instances coming from BioCypher's nodes. This allows to retain edge properties, but adds OWL instances to model relationships, which does not follow the classical OWL model.</p> <p>In this approach, all OWL instances are linked with a generic \"edge_source\" (linking source instance to the association instance) and \"edge_target\" (linking the association instance to the target instance). Both inherit from \"edge\" and are in the biocypher namespace.</p>"},{"location":"reference/outputs/owl-output/#example_1","title":"Example","text":"<p>For instance, the following BioCypher tuples (two nodes and one edge): <pre><code># Nodes:\n# ID           label           properties\n(\"My_source\", \"thisNodeType\", {\"my_prop\":\"this\"}),\n(\"My_target\", \"thatNodeType\", {}),\n\n# Edges:\n# ID         source ID    target ID    properties               label\n(\"My_edge\", \"My_source\", \"My_target\", {\"my_edge_prop\":\"that\"}, \"toward\")\n</code></pre> would be translated into the following OWL statements (here shown in the Turtle format, not showing the taxonomy ancestors): <pre><code># Declaration of BioCypher's generic edge types:\nbiocypher:edge a owl:ObjectProperty ;\n    rdfs:subPropertyOf owl:topObjectProperty ;\n\nbiocypher:edge_source a biocypher:edge ;\n\nbiocypher:edge_target a biocypher:edge ;\n\n# The edge type becomes an OWL class:\n:toward a owl:Class ;\n\n# Actual data:\n:My_source a :thisNodeType, owl:NamedIndividual ;\n    biocypher:my_prop \"this\" ;\n\n:My_target a :thatNodeType, owl:NamedIndividual ;\n\n# An edge is an OWL individual, with properties:\n:My_edge a :toward, owl:NamedIndividual ;\n    biocypher:edge_source :My_source ;\n    biocypher:edge_target :My_target ;\n    biocypher:my_edge_prop \"that\" ;\n</code></pre> Note how the properties of the edge are kept.</p>"},{"location":"reference/outputs/owl-output/#root-node-and-association","title":"Root node and Association","text":"<p>If you use this edge model, you may select one of the subclasses of owl:Thing as a <code>root_node</code>, and not select any part of the object property tree.</p> <p>For instance, if you have a taxonomy with a common root node, the \"Association\" edge model only requires that you select a subclass of owl:Thing, and you do not need to select the meta root: <pre><code>- my_meta_root\n  \u251c owl:Thing   &lt;= it is only necessary to use `root_node: Thing`\n  \u2502 \u251c Entity\n  \u2502 \u251c My_class\n  \u2502 \u2514 etc.\n  \u2514 owl:topObjectProperty  &lt;= This subtree will not be used.\n    \u251c My_link_type\n    \u2514 etc.\n</code></pre></p>"},{"location":"reference/outputs/owl-output/#taxonomy-management","title":"Taxonomy Management","text":"<p>This class takes care of keeping the vocabulary underneath the selected root node and exports it along the instances in the resulting OWL file. It discards all terms that are not in the tree below the selected root node.</p> <p>The configuration parameter <code>rdf_namespaces</code> can be used to specify which namespaces exist in the input ontology (or the data). If the data contain IDs with a given prefix, they will be converted into valid Internationalized Resource Identifiers (IRI) to allow referencing. If no namespace is specified, BioCypher will search for them in the input ontology.</p>"},{"location":"reference/outputs/owl-output/#settings","title":"Settings","text":"<p>Important parameters are:</p> <ul> <li><code>root_node</code>, which must be a meta-root on top of both owl:Thing and   owl:topObjectProperty.</li> <li><code>edge_model</code> heavily impacts the output ontology, most notably the graph   structure, and thus the queries that can be made on it (see above).</li> <li><code>file_stem</code> is the name of the output file (without the extension or the path)   which will be written in the output directory.</li> <li><code>file_format</code> is the output serialization format. Note that if set to \"turtle\",   the output file extension will be \".ttl\".</li> </ul>"},{"location":"reference/outputs/owl-output/#for-the-objectproperty-edge-model","title":"For the ObjectProperty edge model","text":"<pre><code>:caption: biocypher_config.yaml\n\nbiocypher:\n    strict_mode: true\n    schema_config_path: config/schema_config.yaml\n    dbms: owl # &lt;- Use the OWL output writer.\n\n    head_ontology:\n        url: file:///home/superb/owl_file.ttl\n        root_node: BioCypherRoot # &lt;- The \"meta-root\" class.\n\nowl:\n    file_format: turtle\n    # Can be either: xml, n3, turtle or ttl, nt, pretty-xml, trix, trig, nquads, json-ld\n\n    edge_model: ObjectProperty\n    # Can also be: Association (the default)\n\n    file_stem: my_ontology # \"biocypher\" by default, do not put an extension\n\n    # Optional:\n    rdf_namespaces:\n        so: http://purl.obolibrary.org/obo/SO_\n        efo: http://www.ebi.ac.uk/efo/EFO_\n</code></pre>"},{"location":"reference/outputs/owl-output/#for-the-association-edge-model","title":"For the Association edge model","text":"<pre><code>:caption: biocypher_config.yaml\n\nbiocypher:\n    strict_mode: true\n    schema_config_path: config/schema_config.yaml\n    dbms: owl # &lt;- Use the OWL output writer.\n\n    head_ontology:\n        url: file:///home/superb/owl_file.ttl\n        root_node: Entity # &lt;- NOT the meta-root!\n\nowl:\n    file_format: turtle\n    # Can be either: xml, n3, turtle or ttl, nt, pretty-xml, trix, trig, nquads, json-ld\n\n    edge_model: Association\n\n    file_stem: my_ontology # \"biocypher\" by default, do not put an extension\n\n    # Optional:\n    rdf_namespaces:\n        so: http://purl.obolibrary.org/obo/SO_\n        efo: http://www.ebi.ac.uk/efo/EFO_\n</code></pre>"},{"location":"reference/outputs/owl-output/#possible-issues","title":"Possible Issues","text":"<p>BioCypher is not able to read all OWL ontologies, and not all of the terms hosted in an OWL ontology. Most notably, it only reads (a part of) the taxonomy to build up its input. Some logical predicates may also be incompatible with the selected edge model (especially \"Association\").</p> <p>Note that Prot\u00e9g\u00e9 may show a couple of impediments:</p> <ul> <li>It displays owl:Entity as if it inherits from owl:Thing, but that is not   necessarily actually implemented by a predicate. You may have to add it   manually.</li> <li>It displays all owl:ObjectProperty as if they inherit from   owl:topObjectProperty, but you may also have to add the predicate manually.</li> <li>It provides no easy way to add a meta-root on top of both classes, and a   manually added one will appear as a subclass of both owl:Thing and   owl:topObjectProperty.</li> </ul> <p>Double-checking the ontology file source code itself should help you ensure compatibility with BioCypher's constraints.</p> <p>Also, note that BioCypher requires that classes (and object properties) have an RDFS label, and will use it (and not the IRI) to find the necessary types.</p>"},{"location":"reference/outputs/postgresql-output/","title":"PostgreSQL","text":"<p>When setting the <code>dbms</code> parameter in the <code>biocypher_config.yaml</code> to <code>postgresql</code>, the BioCypher Knowledge Graph is written to a PostgreSQL database. PostgreSQL is a relational database management system.</p>"},{"location":"reference/outputs/postgresql-output/#install-postgresql","title":"Install PostgreSQL","text":"<p>To get a PostgreSQL instance running quickly, you can use Docker. The following command will start a PostgreSQL instance with the password <code>postgres</code> and the port <code>5432</code> exposed to the host system.</p> <pre><code>docker run --restart always \\\n           --publish=5432:5432 \\\n           --env POSTGRES_PASSWORD=postgres \\\n           -d postgres\n</code></pre> <p>The <code>postgresql-client</code> (also known as <code>psql</code> command line tool) can be used to connect and interact with the running PostgreSQL database instance. Installation instructions can be found here.</p>"},{"location":"reference/outputs/postgresql-output/#postgresql-settings","title":"PostgreSQL settings","text":"<p>To overwrite the standard settings of PostgreSQL, add a <code>postgresql</code> section to the <code>biocypher_config.yaml</code> file. The following settings are possible:</p> biocypher_config.yaml<pre><code>postgresql:  ### PostgreSQL configuration ###\n\n  # PostgreSQL connection credentials\n  database_name: postgres\n  user: postgres\n  password: postgres\n  port: 5432\n\n  # PostgreSQL import batch writer settings\n  quote_character: '\"'\n  delimiter: '\\t'\n\n  # Import call prefixes to adjust the autogenerated shell script\n  import_call_bin_prefix: ''  # path to \"psql\"\n  import_call_file_prefix: /path/to/files/\n</code></pre>"},{"location":"reference/outputs/postgresql-output/#offline-mode","title":"Offline mode","text":""},{"location":"reference/outputs/postgresql-output/#running-biocypher","title":"Running BioCypher","text":"<p>After running BioCypher with the <code>offline</code> parameter set to <code>true</code> and the <code>dbms</code> set to <code>postgres</code>, the output folder contains:</p> <ul> <li> <p><code>entity-create_table.sql</code>: The SQL scripts to create the tables for the nodes/edges. Entity is replaced by your nodes and edges and for each node and edge type an own SQL script is generated.</p> </li> <li> <p><code>entity-part000.csv</code>: The CSV file containing the data for the entity.</p> </li> <li> <p><code>postgres-import-call.sh</code>: The import script to create a database with the SQL scripts and insert the data from the CSV files.</p> </li> </ul> <p>Note</p> <p>If the <code>postgres-import-call.sh</code> is missing, you can create it by running <code>bc.write_import_call()</code>.</p>"},{"location":"reference/outputs/postgresql-output/#create-the-postgresql-database","title":"Create the PostgreSQL database","text":"<p>A running PostgreSQL instance (e.g. in a Docker container created with the command from above) is required to create the database and import the data. By running the import script <code>postgres-import-call.sh</code>, the content should be written to the PostgreSQL database.</p>"},{"location":"reference/outputs/postgresql-output/#access-the-postgresql-database","title":"Access the PostgreSQL database","text":"<p>To check the content of the database, you can use the <code>psql</code> command line tool.</p> <p>First connect to the running PostgreSQL database instance: <pre><code>psql -h localhost -p 5432 -U postgres\n</code></pre></p> <p>Then you can execute SQL queries. For example, you can list all tables in the database by running the following command in the terminal: <pre><code>SELECT table_name\nFROM information_schema.tables\nWHERE table_schema = 'public';\n</code></pre></p>"},{"location":"reference/outputs/rdf-output/","title":"Resource Description Framework (RDF)","text":"<p>In this section, we will learn how to use and implement the output to RDF using the <code>_RDFWriter</code> module.</p>"},{"location":"reference/outputs/rdf-output/#rdf-settings","title":"RDF settings","text":"<p>To write your output to RDF, you have to specify some RDF settings in the <code>biocypher_config.yaml</code>. Using <code>file_format</code>, you can choose to export to <code>XML</code>, <code>Turtle</code> or any other format <code>rdflib</code> supports. The second configuration is the <code>rdf_namespaces</code>, where you can specify which namespaces exist in your data. If, for instance, your data contain SO (Sequence ontology) terms such as <code>SO:0000001</code>, IDs will be converted into valid URIs to allow referencing. Thus, <code>SO:0000001</code> will be converted into <code>http://purl.obolibrary.org/obo/SO_0000001</code>. When a node cannot be converted, a default URI will be used (<code>https://biocypher.org/biocypher#&lt;node_id&gt;</code>). Running the pipeline, the <code>_RDFWriter</code> will create a file for every node and relationship type you have specified.</p> biocypher_config.yaml<pre><code>biocypher:\n  strict_mode: true\n  schema_config_path: config/schema_config.yaml\n  dbms: rdf\n\n### RDF configuration ###\nrdf:\n  file_format: turtle\n  # options: xml, n3, turtle or ttl, nt, pretty-xml, trix, trig, nquads, json-ld\n  rdf_namespaces:\n    so: http://purl.obolibrary.org/obo/SO_\n    efo: http://www.ebi.ac.uk/efo/EFO_\n</code></pre>"},{"location":"reference/outputs/sqlite-output/","title":"SQLite","text":"<p>When setting the <code>dbms</code> parameter in the <code>biocypher_config.yaml</code> to <code>sqlite</code>, the BioCypher Knowledge Graph is written to a SQLite database. SQLite is a lightweight relational database management system. It is suitable for fast prototyping and development. For more mature applications have a look at PostgreSQL.</p>"},{"location":"reference/outputs/sqlite-output/#sqlite-settings","title":"SQLite settings","text":"<p>To overwrite the standard settings of SQLite, add a <code>sqlite</code> section to the <code>biocypher_config.yaml</code> file. The following settings are possible:</p> biocypher_config.yaml<pre><code>sqlite:\n  ### SQLite configuration ###\n\n  database_name: sqlite.db # DB name\n\n  # SQLite import batch writer settings\n  quote_character: '\"'\n  delimiter: '\\t'\n  # import_call_bin_prefix: '' # path to \"sqlite3\"\n  # import_call_file_prefix: '/path/to/files'\n</code></pre>"},{"location":"reference/outputs/sqlite-output/#offline-mode","title":"Offline mode","text":""},{"location":"reference/outputs/sqlite-output/#running-biocypher","title":"Running BioCypher","text":"<p>After running BioCypher with the <code>offline</code> parameter set to <code>true</code> and the <code>dbms</code> set to <code>sqlite</code>, the output folder contains:</p> <ul> <li> <p><code>entity-create_table.sql</code>: The SQL scripts to create the tables for the nodes/edges. Entity is replaced by your nodes and edges and for each node and edge type an own SQL script is generated.</p> </li> <li> <p><code>entity-part000.csv</code>: The CSV file containing the data for the entity.</p> </li> <li> <p><code>sqlite-import-call.sh</code>: The import script to create a database with the SQL scripts and insert the data from the CSV files.</p> </li> </ul> <p>Note</p> <p>If the <code>sqlite-import-call.sh</code> is missing, you can create it by running <code>bc.write_import_call()</code>.</p>"},{"location":"reference/outputs/sqlite-output/#create-the-sqlite-database","title":"Create the SQLite database","text":"<p>To create the SQLite database, run the import script <code>sqlite-import-call.sh</code>. In the default case (without any changes to the <code>database_name</code>in the configuration), the file containing the database is created with the name <code>sqlite.db</code>.</p> <p>Note</p> <p>The import script expects, that the sqlite3 command line tool is installed on your system.</p>"},{"location":"reference/outputs/sqlite-output/#access-the-sqlite-database","title":"Access the SQLite database","text":"<p>Now you can access the created SQLite database. This can be done with the sqlite3 command line tool. For example, you can list all tables in the database by running the following command in the terminal:</p> <pre><code>sqlite3 sqlite.db \"SELECT name FROM sqlite_master WHERE type='table';\"\n</code></pre>"},{"location":"reference/outputs/tabular-output/","title":"Tabular Format","text":"<p>When setting the <code>dbms</code> parameter in the <code>biocypher_config.yaml</code> to <code>tabular</code>, <code>csv</code>, or <code>pandas</code>, the BioCypher Knowledge Graph is created in one of several possible tabular formats.</p>"},{"location":"reference/outputs/tabular-output/#tabular-output-settings","title":"Tabular output settings","text":"<p>To overwrite the standard settings of the CSV writer, add a <code>csv</code> section to the <code>biocypher_config.yaml</code> file. The following settings are possible:</p> biocypher_config.yaml<pre><code>csv:\n  ### CSV/Pandas configuration ###\n  delimiter: ','  # The delimiter to be used in the CSV files. Default is ','.\n</code></pre>"},{"location":"reference/outputs/tabular-output/#offline-mode","title":"Offline mode","text":""},{"location":"reference/outputs/tabular-output/#running-biocypher","title":"Running BioCypher","text":"<p>After running BioCypher with the <code>offline</code> parameter set to <code>true</code> and the <code>dbms</code> set to <code>tabular</code>, <code>csv</code>, or <code>pandas</code>, the output folder contains:</p> <ul> <li> <p><code>*.csv</code>: The CSV files containing the node/edge data.</p> </li> <li> <p><code>import_pandas_csv.csv</code>: A Python script to load the created CSV files into Pandas DataFrames.</p> </li> </ul>"},{"location":"reference/outputs/tabular-output/#online-mode","title":"Online mode","text":"<p>After running BioCypher with the <code>offline</code> parameter set to <code>false</code> and the <code>dbms</code> set to <code>tabular</code>, <code>csv</code>, or <code>pandas</code>, you can get the in-memory representation of the Knowledge Graph directly from BioCypher by calling the <code>get_kg()</code> function. This returns a dictionary with the corresponding data type (e.g., <code>Pandas</code> dataframes) for every node and edge type.</p> <pre><code># Initialize BioCypher\nbc = BioCypher(\n    biocypher_config_path=\"biocypher_config.yaml\",\n    schema_config_path=\"schema_config.yaml\",\n)\n\n# Add nodes and edges\nbc.add_nodes(nodes)\nbc.add_edges(edges)\n\n# Get the in-memory representation of the Knowledge Graph\nin_memory_kg = bc.get_kg()\n</code></pre>"},{"location":"reference/source/","title":"API Reference","text":""},{"location":"reference/source/#the-main-biocypher-interface","title":"The main BioCypher interface","text":"<p>Create a BioCypher instance by running:</p> <pre><code>from biocypher import BioCypher\nbc = BioCypher()\n</code></pre> <p>Most of the settings should be configured by YAML files. See BioCypher for details of the BioCypher class.</p>"},{"location":"reference/source/#database-creation-by-file","title":"Database creation by file","text":"<p>Using the BioCypher instance, you can create a database by writing files by using the <code>write_nodes</code> and <code>write_edges</code> methods, which accept collections of nodes and edges either as tuples or as <code>BioCypherNode</code> and <code>BioCypherEdge</code> objects. For example:</p> <pre><code># given lists of nodes and edges\nbc.write_nodes(node_list)\nbc.write_edges(edge_list)\n</code></pre> <p>Note</p> <p>To facilitate the interaction with the various database management systems (DBMSs), BioCypher provides utility functions, such as writing a Neo4j admin import statement to be used for creating a Neo4j database (<code>write_import_call</code>). The most commonly used utility functions are also available in the wrapper function <code>summary</code>. See the BioCypher class for more information.</p> <p>Details about the output writing modules responsible for these methods can be found here.</p>"},{"location":"reference/source/#in-memory-pandas-knowledge-graph","title":"In-memory Pandas knowledge graph","text":"<p>BioCypher provides a wrapper around the <code>pandas.DataFrame</code> class to facilitate the creation of a knowledge graph in memory. This is useful for testing, small datasets, and for workflows that should remain purely in Python. Example usage:</p> <pre><code>from biocypher import BioCypher\nbc = BioCypher()\n# given lists of nodes and edges\nbc.add(node_list)\nbc.add(edge_list)\n# show list of dataframes (one per node/edge type)\ndfs = bc.to_df()\n</code></pre> <p>Details about the in-memory module responsible for these methods can be found here.</p>"},{"location":"reference/source/#database-creation-and-manipulation-by-driver","title":"Database creation and manipulation by Driver","text":"<p>BioCypher also provides a driver for each of the supported DBMSs. The driver can be used to create a database and to write nodes and edges to it, as well as allowing more subtle manipulation usually not encountered in creating a database from scratch as in the file-based workflow. This includes merging (creation of entities only if they don't exist) and deletion. For example:</p> <pre><code>from biocypher import BioCypher\nbc = BioCypher()\n# given lists of nodes and edges\nbc.merge_nodes(node_set_1)\nbc.merge_edges(edge_set_1)\nbc.merge_nodes(node_set_2)\nbc.merge_edges(edge_set_2)\n</code></pre> <p>Details about the connector module responsible for these methods can be found here.</p>"},{"location":"reference/source/#download-and-cache-functionality","title":"Download and cache functionality","text":"<p>BioCypher provides a download and cache functionality for resources. Resources are defined via the abstract <code>Resource</code> class, which have a name, a (set of) URL(s), and a lifetime (in days, set to 0 for infinite). Two classes inherit from the <code>Resource</code> class, the <code>FileDownload</code> class and <code>APIRequest</code> class. The <code>Downloader</code> can deal with single files, lists of files, compressed files, and directories (which needs to be indicated using the <code>is_dir</code> parameter of the <code>FileDownload</code>). It uses Pooch under the hood to handle the downloading of files and Python's requests library to perform API requests. Example usage:</p> <pre><code>from biocypher import BioCypher, FileDownload, APIRequest\nbc = BioCypher()\n\nresource1 = FileDownload(\n    name=\"file_list_resource\",\n    url_s=[\n        \"https://example.com/file_download1.txt\",\n        \"https://example.com/file_download2.txt\"\n    ],\n    lifetime=1\n)\nresource2 = FileDownload(\n    name=\"zipped_resource\",\n    url_s=\"https://example.com/file_download3.zip\",\n    lifetime=7\n)\nresource3 = FileDownload(\n    name=\"directory_resource\",\n    url_s=\"https://example.com/file_download4/\",\n    lifetime=7,\n    is_dir=True,\n)\nresource4 = APIRequest(\n    name=\"list_api_request\",\n    url_s=[\n        \"https://api.example.org/api_request1\",\n        \"https://api.example.org/api_request2\",\n    ],\n    life_time=7,\n)\nresource5 = APIRequest(\n    name=\"api_request\",\n    url_s=\"https://api.example.org/api_request1\",\n    life_time=7,\n)\nresource_list = [resource1, resource2, resource3, resource4, resource5]\npaths = bc.download(resource_list)\n</code></pre> <p>The files and API requests will be stored in the cache directory, in subfolders according to the names of the resources, and additionally determined by Pooch (e.g., extraction of zip files can result in multiple new files). All paths of downloaded files are returned by the <code>download</code> method. The <code>Downloader</code> class can also be used directly, without the BioCypher instance. You can set the cache directory in the configuration file; if not set, it will use the <code>TemporaryDirectory.name()</code> method from the <code>tempfile</code> module. More details about the <code>Resource</code>, <code>FileDownload</code>, <code>APIRequest</code> and <code>Downloader</code> classes can be found here.</p>"},{"location":"reference/source/#ontology-ingestion-parsing-and-manipulation","title":"Ontology ingestion, parsing, and manipulation","text":"<p>BioCypher has dedicated modules for processing ontologies, details here.</p>"},{"location":"reference/source/#base-classes-for-node-and-edge-representations-in-biocypher","title":"Base classes for node and edge representations in BioCypher","text":"<p>BioCypher has abstract graph handling for internal use, details here.</p>"},{"location":"reference/source/#translation-functionality-for-implemented-types-of-representation","title":"Translation functionality for implemented types of representation","text":"<p>BioCypher translates between input data types and the ontology-mapped internal representation of the graph, details here.</p>"},{"location":"reference/source/biocypher/","title":"BioCypher","text":"<p>Orchestration of BioCypher operations.</p> <p>Instantiate this class to interact with BioCypher.</p> <pre><code>dbms (str): The database management system to use. For supported\n    systems see SUPPORTED_DBMS.\n\noffline (bool): Whether to run in offline mode. In offline mode\n    the Knowledge Graph is written to files. In online mode, it\n    is written to a database or hold in memory.\n\nstrict_mode (bool): Whether to run in strict mode. If True, the\n    translator will raise an error if a node or edge does not\n    provide source, version, and licence information.\n\nbiocypher_config_path (str): Path to the BioCypher config file.\n\nschema_config_path (str): Path to the user schema config\n    file.\n\nhead_ontology (dict): The head ontology defined by URL ('url') and root\n    node ('root_node').\n\ntail_ontologies (dict): The tail ontologies defined by URL and\n    join nodes for both head and tail ontology.\n\noutput_directory (str): Path to the output directory. If not\n    provided, the default value 'biocypher-out' will be used.\n\ncache_directory (str): Path to the cache directory.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>class BioCypher:\n    \"\"\"Orchestration of BioCypher operations.\n\n    Instantiate this class to interact with BioCypher.\n\n    Args:\n    ----\n        dbms (str): The database management system to use. For supported\n            systems see SUPPORTED_DBMS.\n\n        offline (bool): Whether to run in offline mode. In offline mode\n            the Knowledge Graph is written to files. In online mode, it\n            is written to a database or hold in memory.\n\n        strict_mode (bool): Whether to run in strict mode. If True, the\n            translator will raise an error if a node or edge does not\n            provide source, version, and licence information.\n\n        biocypher_config_path (str): Path to the BioCypher config file.\n\n        schema_config_path (str): Path to the user schema config\n            file.\n\n        head_ontology (dict): The head ontology defined by URL ('url') and root\n            node ('root_node').\n\n        tail_ontologies (dict): The tail ontologies defined by URL and\n            join nodes for both head and tail ontology.\n\n        output_directory (str): Path to the output directory. If not\n            provided, the default value 'biocypher-out' will be used.\n\n        cache_directory (str): Path to the cache directory.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        dbms: str = None,\n        offline: bool = None,\n        strict_mode: bool = None,\n        biocypher_config_path: str = None,\n        schema_config_path: str = None,\n        head_ontology: dict = None,\n        tail_ontologies: dict = None,\n        output_directory: str = None,\n        cache_directory: str = None,\n        # legacy params\n        db_name: str = None,\n    ):\n        # Update configuration if custom path is provided\n        if biocypher_config_path:\n            _file_update(biocypher_config_path)\n\n        if db_name:\n            logger.warning(\n                \"The parameter `db_name` is deprecated. Please set the \"\n                \"`database_name` setting in the `biocypher_config.yaml` file \"\n                \"instead.\",\n            )\n            _config(**{db_name: {\"database_name\": db_name}})\n\n        # Load configuration\n        self.base_config = _config(\"biocypher\")\n\n        # Check for required configuration\n        for key in REQUIRED_CONFIG:\n            if key not in self.base_config:\n                msg = f\"Configuration key {key} is required.\"\n                raise ValueError(msg)\n\n        # Set configuration - mandatory\n        self._dbms = dbms or self.base_config[\"dbms\"]\n\n        if offline is None:\n            self._offline = self.base_config[\"offline\"]\n        else:\n            self._offline = offline\n\n        # Check if pandas/tabular is being used in offline mode\n        if self._offline and self._dbms.lower() in [\"pandas\", \"tabular\"]:\n            msg = (\n                f\"The '{self._dbms}' DBMS is only available in online mode. \"\n                f\"If you want to write CSV files, use 'csv' as the DBMS. \"\n                f\"If you want to use pandas, set 'offline: false' in your configuration.\"\n            )\n            raise ValueError(msg)\n\n        if strict_mode is None:\n            self._strict_mode = self.base_config[\"strict_mode\"]\n        else:\n            self._strict_mode = strict_mode\n\n        self._schema_config_path = schema_config_path or self.base_config.get(\n            \"schema_config_path\",\n        )\n\n        if not self._schema_config_path:\n            logger.warning(\"Running BioCypher without schema configuration.\")\n        else:\n            logger.info(\n                f\"Running BioCypher with schema configuration from {self._schema_config_path}.\",\n            )\n\n        self._head_ontology = head_ontology or self.base_config[\"head_ontology\"]\n\n        # Set configuration - optional\n        self._output_directory = output_directory or self.base_config.get(\n            \"output_directory\",\n        )\n        self._cache_directory = cache_directory or self.base_config.get(\n            \"cache_directory\",\n        )\n        self._tail_ontologies = tail_ontologies or self.base_config.get(\n            \"tail_ontologies\",\n        )\n\n        if self._dbms not in SUPPORTED_DBMS:\n            msg = f\"DBMS {self._dbms} not supported. Please select from {SUPPORTED_DBMS}.\"\n            raise ValueError(msg)\n\n        # Initialize\n        self._ontology_mapping = None\n        self._deduplicator = None\n        self._translator = None\n        self._downloader = None\n        self._ontology = None\n        self._writer = None\n        self._driver = None\n        self._in_memory_kg = None\n\n        self._in_memory_kg = None\n        self._nodes = None\n        self._edges = None\n\n    def _initialize_in_memory_kg(self) -&gt; None:\n        \"\"\"Create in-memory KG instance.\n\n        Set as instance variable `self._in_memory_kg`.\n        \"\"\"\n        if not self._in_memory_kg:\n            self._in_memory_kg = get_in_memory_kg(\n                dbms=self._dbms,\n                deduplicator=self._get_deduplicator(),\n            )\n\n    def add_nodes(self, nodes) -&gt; None:\n        \"\"\"Add new nodes to the internal representation.\n\n        Initially, receive nodes data from adaptor and create internal\n        representation for nodes.\n\n        Args:\n        ----\n            nodes(iterable): An iterable of nodes\n\n        \"\"\"\n        if isinstance(nodes, list):\n            self._nodes = list(itertools.chain(self._nodes, nodes))\n        else:\n            self._nodes = itertools.chain(self._nodes, nodes)\n\n    def add_edges(self, edges) -&gt; None:\n        \"\"\"Add new edges to the internal representation.\n\n        Initially, receive edges data from adaptor and create internal\n        representation for edges.\n\n        Args:\n        ----\n             edges(iterable): An iterable of edges.\n\n        \"\"\"\n        if isinstance(edges, list):\n            self._edges = list(itertools.chain(self._edges, edges))\n        else:\n            self._edges = itertools.chain(self._edges, edges)\n\n    def to_df(self):\n        \"\"\"Create DataFrame using internal representation.\n\n        TODO: to_df implies data frame, should be specifically that use case\n        \"\"\"\n        return self._to_KG()\n\n    def to_networkx(self):\n        \"\"\"Create networkx using internal representation.\"\"\"\n        return self._to_KG()\n\n    def _to_KG(self):\n        \"\"\"Convert the internal representation to knowledge graph.\n\n        The knowledge graph is returned based on the `dbms` parameter in\n        the biocypher configuration file.\n\n        TODO: These conditionals are a hack, we need to refactor the in-memory\n        KG to be generic, and simplify access and conversion to output formats.\n\n        Returns\n        -------\n             Any: knowledge graph.\n\n        \"\"\"\n        # If we're using an in-memory KG and it already exists, return it directly\n        if self._in_memory_kg and self._is_online_and_in_memory():\n            return self._in_memory_kg.get_kg()\n\n        # Otherwise, initialize and populate the in-memory KG\n        if not self._in_memory_kg:\n            self._initialize_in_memory_kg()\n        if not self._translator:\n            self._get_translator()\n\n        # These attributes might not exist when using in-memory KG directly\n        if hasattr(self, \"_nodes\") and hasattr(self, \"_edges\"):\n            tnodes = self._translator.translate_entities(self._nodes)\n            tedges = self._translator.translate_entities(self._edges)\n            self._in_memory_kg.add_nodes(tnodes)\n            self._in_memory_kg.add_edges(tedges)\n\n        return self._in_memory_kg.get_kg()\n\n    def _get_deduplicator(self) -&gt; Deduplicator:\n        \"\"\"Create deduplicator if not exists and return.\"\"\"\n        if not self._deduplicator:\n            self._deduplicator = Deduplicator()\n\n        return self._deduplicator\n\n    def _get_ontology_mapping(self) -&gt; OntologyMapping:\n        \"\"\"Create ontology mapping if not exists and return.\"\"\"\n        if not self._schema_config_path:\n            self._ontology_mapping = OntologyMapping()\n\n        if not self._ontology_mapping:\n            self._ontology_mapping = OntologyMapping(\n                config_file=self._schema_config_path,\n            )\n\n        return self._ontology_mapping\n\n    def _get_ontology(self) -&gt; Ontology:\n        \"\"\"Create ontology if not exists and return.\"\"\"\n        if not self._ontology:\n            self._ontology = Ontology(\n                ontology_mapping=self._get_ontology_mapping(),\n                head_ontology=self._head_ontology,\n                tail_ontologies=self._tail_ontologies,\n            )\n\n        return self._ontology\n\n    def _get_translator(self) -&gt; Translator:\n        \"\"\"Create translator if not exists and return.\"\"\"\n        if not self._translator:\n            self._translator = Translator(\n                ontology=self._get_ontology(),\n                strict_mode=self._strict_mode,\n            )\n\n        return self._translator\n\n    def _initialize_writer(self) -&gt; None:\n        \"\"\"Create writer if not online.\n\n        Set as instance variable `self._writer`.\n        \"\"\"\n        if self._offline:\n\n            def timestamp() -&gt; str:\n                return datetime.now().strftime(\"%Y%m%d%H%M%S\")\n\n            outdir = self._output_directory or os.path.join(\n                \"biocypher-out\",\n                timestamp(),\n            )\n            self._output_directory = os.path.abspath(outdir)\n\n            self._writer = get_writer(\n                dbms=self._dbms,\n                translator=self._get_translator(),\n                deduplicator=self._get_deduplicator(),\n                output_directory=self._output_directory,\n                strict_mode=self._strict_mode,\n            )\n        else:\n            msg = \"Cannot get writer in online mode.\"\n            raise NotImplementedError(msg)\n\n    def _get_driver(self):\n        \"\"\"Create driver if not exists.\n\n        Set as instance variable `self._driver`.\n        \"\"\"\n        if not self._offline:\n            self._driver = get_connector(\n                dbms=self._dbms,\n                translator=self._get_translator(),\n            )\n        else:\n            msg = \"Cannot get driver in offline mode.\"\n            raise NotImplementedError(msg)\n\n        return self._driver\n\n    def _get_in_memory_kg(self):\n        \"\"\"Create in-memory KG instance.\n\n        Set as instance variable `self._in_memory_kg`.\n        \"\"\"\n        if not self._in_memory_kg:\n            self._in_memory_kg = get_in_memory_kg(\n                dbms=self._dbms,\n                deduplicator=self._get_deduplicator(),\n            )\n\n        return self._in_memory_kg\n\n    def _add_nodes(\n        self,\n        nodes,\n        batch_size: int = int(1e6),\n        force: bool = False,\n    ):\n        \"\"\"Add nodes to the BioCypher KG.\n\n        First uses the `_translator` to translate the nodes to `BioCypherNode`\n        objects. Depending on the configuration the translated nodes are then\n        passed to the\n\n        - `_writer`: if `_offline` is set to `False`\n\n        - `_in_memory_kg`: if `_offline` is set to `False` and the `_dbms` is an\n            `IN_MEMORY_DBMS`\n\n        - `_driver`: if `_offline` is set to `True` and the `_dbms` is not an\n            `IN_MEMORY_DBMS`\n\n        \"\"\"\n        if not self._translator:\n            self._get_translator()\n        translated_nodes = self._translator.translate_entities(nodes)\n\n        if self._offline:\n            if not self._writer:\n                self._initialize_writer()\n            passed = self._writer.write_nodes(\n                translated_nodes,\n                batch_size=batch_size,\n                force=force,\n            )\n        elif self._is_online_and_in_memory():\n            passed = self._get_in_memory_kg().add_nodes(translated_nodes)\n        else:\n            passed = self._get_driver().add_biocypher_nodes(translated_nodes)\n\n        return passed\n\n    def _add_edges(self, edges, batch_size: int = int(1e6)):\n        \"\"\"Add edges to the BioCypher KG.\n\n        First uses the `_translator` to translate the edges to `BioCypherEdge`\n        objects. Depending on the configuration the translated edges are then\n        passed to the\n\n        - `_writer`: if `_offline` is set to `False`\n\n        - `_in_memory_kg`: if `_offline` is set to `False` and the `_dbms` is an\n            `IN_MEMORY_DBMS`\n\n        - `_driver`: if `_offline` is set to `True` and the `_dbms` is not an\n            `IN_MEMORY_DBMS`\n\n        \"\"\"\n        if not self._translator:\n            self._get_translator()\n        translated_edges = self._translator.translate_entities(edges)\n\n        if self._offline:\n            if not self._writer:\n                self._initialize_writer()\n            passed = self._writer.write_edges(\n                translated_edges,\n                batch_size=batch_size,\n            )\n        elif self._is_online_and_in_memory():\n            if not self._in_memory_kg:\n                self._initialize_in_memory_kg()\n            passed = self._in_memory_kg.add_edges(translated_edges)\n        else:\n            if not self._driver:\n                self._initialize_driver()\n            passed = self._driver.add_biocypher_nodes(translated_edges)\n\n        return passed\n\n    def _is_online_and_in_memory(self) -&gt; bool:\n        \"\"\"Return True if in online mode and in-memory dbms is used.\"\"\"\n        return (not self._offline) &amp; (self._dbms in IN_MEMORY_DBMS)\n\n    def write_nodes(\n        self,\n        nodes,\n        batch_size: int = int(1e6),\n        force: bool = False,\n    ) -&gt; bool:\n        \"\"\"Write nodes to database.\n\n        Either takes an iterable of tuples (if given, translates to\n        ``BioCypherNode`` objects) or an iterable of ``BioCypherNode`` objects.\n\n        Args:\n        ----\n            nodes (iterable): An iterable of nodes to write to the database.\n            batch_size (int): The batch size to use when writing to disk.\n            force (bool): Whether to force writing to the output directory even\n                if the node type is not present in the schema config file.\n\n        Returns:\n        -------\n            bool: True if successful.\n\n        \"\"\"\n        return self._add_nodes(nodes, batch_size=batch_size, force=force)\n\n    def write_edges(self, edges, batch_size: int = int(1e6)) -&gt; bool:\n        \"\"\"Write edges to database.\n\n        Either takes an iterable of tuples (if given, translates to\n        ``BioCypherEdge`` objects) or an iterable of ``BioCypherEdge`` objects.\n\n        Args:\n        ----\n            edges (iterable): An iterable of edges to write to the database.\n\n        Returns:\n        -------\n            bool: True if successful.\n\n        \"\"\"\n        return self._add_edges(edges, batch_size=batch_size)\n\n    def add(self, entities) -&gt; None:\n        \"\"\"Add entities to the in-memory database.\n\n        Accepts an iterable of tuples (if given, translates to\n        ``BioCypherNode`` or ``BioCypherEdge`` objects) or an iterable of\n        ``BioCypherNode`` or ``BioCypherEdge`` objects.\n\n        Args:\n        ----\n            entities (iterable): An iterable of entities to add to the database.\n                Can be 3-tuples (nodes) or 5-tuples (edges); also accepts\n                4-tuples for edges (deprecated).\n\n        Returns:\n        -------\n            None\n\n        \"\"\"\n        return self._add_nodes(entities)\n\n    def merge_nodes(self, nodes) -&gt; bool:\n        \"\"\"Merge nodes into database.\n\n        Either takes an iterable of tuples (if given, translates to\n        ``BioCypherNode`` objects) or an iterable of ``BioCypherNode`` objects.\n\n        Args:\n        ----\n            nodes (iterable): An iterable of nodes to merge into the database.\n\n        Returns:\n        -------\n            bool: True if successful.\n\n        \"\"\"\n        return self._add_nodes(nodes)\n\n    def merge_edges(self, edges) -&gt; bool:\n        \"\"\"Merge edges into database.\n\n        Either takes an iterable of tuples (if given, translates to\n        ``BioCypherEdge`` objects) or an iterable of ``BioCypherEdge`` objects.\n\n        Args:\n        ----\n            edges (iterable): An iterable of edges to merge into the database.\n\n        Returns:\n        -------\n            bool: True if successful.\n\n        \"\"\"\n        return self._add_edges(edges)\n\n    def get_kg(self):\n        \"\"\"Get the in-memory KG instance.\n\n        Depending on the specified `dbms` this could either be a list of Pandas\n        dataframes or a NetworkX DiGraph.\n        \"\"\"\n        if not self._is_online_and_in_memory():\n            msg = (f\"Getting the in-memory KG is only available in online mode for {IN_MEMORY_DBMS}.\",)\n            raise ValueError(msg)\n        if not self._in_memory_kg:\n            msg = \"No in-memory KG instance found. Please call `add()` first.\"\n            raise ValueError(msg)\n\n        if not self._in_memory_kg:\n            self._initialize_in_memory_kg()\n        return self._in_memory_kg.get_kg()\n\n    # DOWNLOAD AND CACHE MANAGEMENT METHODS ###\n\n    def _get_downloader(self, cache_dir: str | None = None):\n        \"\"\"Create downloader if not exists.\"\"\"\n        if not self._downloader:\n            self._downloader = Downloader(self._cache_directory)\n\n    def download(self, *resources) -&gt; None:\n        \"\"\"Download or load from cache the resources given by the adapter.\n\n        Args:\n        ----\n            resources (iterable): An iterable of resources to download or load\n                from cache.\n\n        Returns:\n        -------\n            None\n\n        \"\"\"\n        self._get_downloader()\n        return self._downloader.download(*resources)\n\n    # OVERVIEW AND CONVENIENCE METHODS ###\n\n    def log_missing_input_labels(self) -&gt; dict[str, list[str]] | None:\n        \"\"\"Log missing input labels.\n\n        Get the set of input labels encountered without an entry in the\n        `schema_config.yaml` and print them to the logger.\n\n        Returns\n        -------\n            Optional[Dict[str, List[str]]]: A dictionary of Biolink types\n            encountered without an entry in the `schema_config.yaml` file.\n\n        \"\"\"\n        mt = self._translator.get_missing_biolink_types()\n\n        if mt:\n            msg = (\n                \"Input entities not accounted for due to them not being \"\n                f\"present in the schema configuration file {self._schema_config_path} \"\n                \"(this is not necessarily a problem, if you did not intend \"\n                \"to include them in the database; see the log for details): \\n\"\n            )\n            for k, v in mt.items():\n                msg += f\"    {k}: {v} \\n\"\n\n            logger.info(msg)\n            return mt\n\n        else:\n            logger.info(\"No missing labels in input.\")\n            return None\n\n    def log_duplicates(self) -&gt; None:\n        \"\"\"Log duplicate nodes and edges.\n\n        Get the set of duplicate nodes and edges encountered and print them to\n        the logger.\n        \"\"\"\n        dn = self._deduplicator.get_duplicate_nodes()\n\n        if dn:\n            ntypes = dn[0]\n            nids = dn[1]\n\n            msg = \"Duplicate node types encountered (IDs in log): \\n\"\n            for typ in ntypes:\n                msg += f\"    {typ}\\n\"\n\n            logger.info(msg)\n\n            idmsg = \"Duplicate node IDs encountered: \\n\"\n            for _id in nids:\n                idmsg += f\"    {_id}\\n\"\n\n            logger.debug(idmsg)\n\n        else:\n            logger.info(\"No duplicate nodes in input.\")\n\n        de = self._deduplicator.get_duplicate_edges()\n\n        if de:\n            etypes = de[0]\n            eids = de[1]\n\n            msg = \"Duplicate edge types encountered (IDs in log): \\n\"\n            for typ in etypes:\n                msg += f\"    {typ}\\n\"\n\n            logger.info(msg)\n\n            idmsg = \"Duplicate edge IDs encountered: \\n\"\n            for _id in eids:\n                idmsg += f\"    {_id}\\n\"\n\n            logger.debug(idmsg)\n\n        else:\n            logger.info(\"No duplicate edges in input.\")\n\n    def show_ontology_structure(self, **kwargs) -&gt; None:\n        \"\"\"Show the ontology structure using treelib or write to GRAPHML file.\n\n        Args:\n        ----\n            to_disk (str): If specified, the ontology structure will be saved\n                to disk as a GRAPHML file, to be opened in your favourite\n                graph visualisation tool.\n\n            full (bool): If True, the full ontology structure will be shown,\n                including all nodes and edges. If False, only the nodes and\n                edges that are relevant to the extended schema will be shown.\n\n        \"\"\"\n        if not self._ontology:\n            self._get_ontology()\n\n        return self._ontology.show_ontology_structure(**kwargs)\n\n    def write_import_call(self) -&gt; str:\n        \"\"\"Write a shell script to import the database.\n\n        Shell script is written depending on the chosen DBMS.\n\n        Returns\n        -------\n            str: path toward the file holding the import call.\n\n        \"\"\"\n        if not self._offline:\n            msg = \"Cannot write import call in online mode.\"\n            raise NotImplementedError(msg)\n\n        return self._writer.write_import_call()\n\n    def write_schema_info(self, as_node: bool = False) -&gt; None:\n        \"\"\"Write an extended schema info to file or node.\n\n        Creates a YAML file or KG node that extends the `schema_config.yaml`\n        with run-time information of the built KG. For instance, include\n        information on whether something present in the actual knowledge graph,\n        whether it is a relationship (which is important in the case of\n        representing relationships as nodes) and the actual sources and\n        targets of edges. Since this file can be used in place of the original\n        `schema_config.yaml` file, it indicates that it is the extended schema\n        by setting `is_schema_info` to `true`.\n\n        We start by using the `extended_schema` dictionary from the ontology\n        class instance, which contains all expanded entities and relationships.\n        The information of whether something is a relationship can be gathered\n        from the deduplicator instance, which keeps track of all entities that\n        have been seen.\n\n        Args:\n        ----\n            as_node (bool): If True, the schema info is written as a KG node.\n                If False, the schema info is written to a YAML file.\n\n        \"\"\"\n        if (not self._offline) and self._dbms not in IN_MEMORY_DBMS:\n            msg = \"Cannot write schema info in online mode.\"\n            raise NotImplementedError(msg)\n\n        ontology = self._get_ontology()\n        schema = ontology.mapping.extended_schema.copy()\n        schema[\"is_schema_info\"] = True\n\n        deduplicator = self._get_deduplicator()\n        for node in deduplicator.entity_types:\n            if node in schema:\n                schema[node][\"present_in_knowledge_graph\"] = True\n                schema[node][\"is_relationship\"] = False\n            else:\n                logger.info(\n                    f\"Node {node} not present in extended schema. Skipping schema info.\",\n                )\n\n        # find 'label_as_edge' cases in schema entries\n        changed_labels = {}\n        for k, v in schema.items():\n            if not isinstance(v, dict):\n                continue\n            if \"label_as_edge\" in v:\n                if v[\"label_as_edge\"] in deduplicator.seen_relationships:\n                    changed_labels[v[\"label_as_edge\"]] = k\n\n        for edge in deduplicator.seen_relationships:\n            if edge in changed_labels:\n                edge = changed_labels[edge]\n            if edge in schema:\n                schema[edge][\"present_in_knowledge_graph\"] = True\n                schema[edge][\"is_relationship\"] = True\n                # TODO information about source and target nodes\n            else:\n                logger.info(\n                    f\"Edge {edge} not present in extended schema. Skipping schema info.\",\n                )\n\n        # write to output directory as YAML file\n        path = os.path.join(self._output_directory, \"schema_info.yaml\")\n        with open(path, \"w\") as f:\n            f.write(yaml.dump(schema))\n\n        if as_node:\n            # write as node\n            node = BioCypherNode(\n                node_id=\"schema_info\",\n                node_label=\"schema_info\",\n                properties={\"schema_info\": json.dumps(schema)},\n            )\n            self.write_nodes([node], force=True)\n\n            # override import call with added schema info node\n            self.write_import_call()\n\n        return schema\n\n    # TRANSLATION METHODS ###\n\n    def translate_term(self, term: str) -&gt; str:\n        \"\"\"Translate a term to its BioCypher equivalent.\n\n        Args:\n        ----\n            term (str): The term to translate.\n\n        Returns:\n        -------\n            str: The BioCypher equivalent of the term.\n\n        \"\"\"\n        # instantiate adapter if not exists\n        self.start_ontology()\n\n        return self._translator.translate_term(term)\n\n    def summary(self) -&gt; None:\n        \"\"\"Call convenience and reporting methods.\n\n        Shows ontology structure and logs duplicates and missing input types.\n        \"\"\"\n        self.show_ontology_structure()\n        self.log_duplicates()\n        self.log_missing_input_labels()\n\n    def reverse_translate_term(self, term: str) -&gt; str:\n        \"\"\"Reverse translate a term from its BioCypher equivalent.\n\n        Args:\n        ----\n            term (str): The BioCypher term to reverse translate.\n\n        Returns:\n        -------\n            str: The original term.\n\n        \"\"\"\n        # instantiate adapter if not exists\n        self.start_ontology()\n\n        return self._translator.reverse_translate_term(term)\n\n    def translate_query(self, query: str) -&gt; str:\n        \"\"\"Translate a query to its BioCypher equivalent.\n\n        Args:\n        ----\n            query (str): The query to translate.\n\n        Returns:\n        -------\n            str: The BioCypher equivalent of the query.\n\n        \"\"\"\n        # instantiate adapter if not exists\n        self.start_ontology()\n\n        return self._translator.translate(query)\n\n    def reverse_translate_query(self, query: str) -&gt; str:\n        \"\"\"Reverse translate a query from its BioCypher equivalent.\n\n        Args:\n        ----\n            query (str): The BioCypher query to reverse translate.\n\n        Returns:\n        -------\n            str: The original query.\n\n        \"\"\"\n        # instantiate adapter if not exists\n        self.start_ontology()\n\n        return self._translator.reverse_translate(query)\n</code></pre>"},{"location":"reference/source/biocypher/#biocypher.BioCypher.add","title":"<code>add(entities)</code>","text":"<p>Add entities to the in-memory database.</p> <p>Accepts an iterable of tuples (if given, translates to <code>BioCypherNode</code> or <code>BioCypherEdge</code> objects) or an iterable of <code>BioCypherNode</code> or <code>BioCypherEdge</code> objects.</p> <pre><code>entities (iterable): An iterable of entities to add to the database.\n    Can be 3-tuples (nodes) or 5-tuples (edges); also accepts\n    4-tuples for edges (deprecated).\n</code></pre> <pre><code>None\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def add(self, entities) -&gt; None:\n    \"\"\"Add entities to the in-memory database.\n\n    Accepts an iterable of tuples (if given, translates to\n    ``BioCypherNode`` or ``BioCypherEdge`` objects) or an iterable of\n    ``BioCypherNode`` or ``BioCypherEdge`` objects.\n\n    Args:\n    ----\n        entities (iterable): An iterable of entities to add to the database.\n            Can be 3-tuples (nodes) or 5-tuples (edges); also accepts\n            4-tuples for edges (deprecated).\n\n    Returns:\n    -------\n        None\n\n    \"\"\"\n    return self._add_nodes(entities)\n</code></pre>"},{"location":"reference/source/biocypher/#biocypher.BioCypher.add_edges","title":"<code>add_edges(edges)</code>","text":"<p>Add new edges to the internal representation.</p> <p>Initially, receive edges data from adaptor and create internal representation for edges.</p> <pre><code> edges(iterable): An iterable of edges.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def add_edges(self, edges) -&gt; None:\n    \"\"\"Add new edges to the internal representation.\n\n    Initially, receive edges data from adaptor and create internal\n    representation for edges.\n\n    Args:\n    ----\n         edges(iterable): An iterable of edges.\n\n    \"\"\"\n    if isinstance(edges, list):\n        self._edges = list(itertools.chain(self._edges, edges))\n    else:\n        self._edges = itertools.chain(self._edges, edges)\n</code></pre>"},{"location":"reference/source/biocypher/#biocypher.BioCypher.add_nodes","title":"<code>add_nodes(nodes)</code>","text":"<p>Add new nodes to the internal representation.</p> <p>Initially, receive nodes data from adaptor and create internal representation for nodes.</p> <pre><code>nodes(iterable): An iterable of nodes\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def add_nodes(self, nodes) -&gt; None:\n    \"\"\"Add new nodes to the internal representation.\n\n    Initially, receive nodes data from adaptor and create internal\n    representation for nodes.\n\n    Args:\n    ----\n        nodes(iterable): An iterable of nodes\n\n    \"\"\"\n    if isinstance(nodes, list):\n        self._nodes = list(itertools.chain(self._nodes, nodes))\n    else:\n        self._nodes = itertools.chain(self._nodes, nodes)\n</code></pre>"},{"location":"reference/source/biocypher/#biocypher.BioCypher.download","title":"<code>download(*resources)</code>","text":"<p>Download or load from cache the resources given by the adapter.</p> <pre><code>resources (iterable): An iterable of resources to download or load\n    from cache.\n</code></pre> <pre><code>None\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def download(self, *resources) -&gt; None:\n    \"\"\"Download or load from cache the resources given by the adapter.\n\n    Args:\n    ----\n        resources (iterable): An iterable of resources to download or load\n            from cache.\n\n    Returns:\n    -------\n        None\n\n    \"\"\"\n    self._get_downloader()\n    return self._downloader.download(*resources)\n</code></pre>"},{"location":"reference/source/biocypher/#biocypher.BioCypher.get_kg","title":"<code>get_kg()</code>","text":"<p>Get the in-memory KG instance.</p> <p>Depending on the specified <code>dbms</code> this could either be a list of Pandas dataframes or a NetworkX DiGraph.</p> Source code in <code>biocypher/_core.py</code> <pre><code>def get_kg(self):\n    \"\"\"Get the in-memory KG instance.\n\n    Depending on the specified `dbms` this could either be a list of Pandas\n    dataframes or a NetworkX DiGraph.\n    \"\"\"\n    if not self._is_online_and_in_memory():\n        msg = (f\"Getting the in-memory KG is only available in online mode for {IN_MEMORY_DBMS}.\",)\n        raise ValueError(msg)\n    if not self._in_memory_kg:\n        msg = \"No in-memory KG instance found. Please call `add()` first.\"\n        raise ValueError(msg)\n\n    if not self._in_memory_kg:\n        self._initialize_in_memory_kg()\n    return self._in_memory_kg.get_kg()\n</code></pre>"},{"location":"reference/source/biocypher/#biocypher.BioCypher.log_duplicates","title":"<code>log_duplicates()</code>","text":"<p>Log duplicate nodes and edges.</p> <p>Get the set of duplicate nodes and edges encountered and print them to the logger.</p> Source code in <code>biocypher/_core.py</code> <pre><code>def log_duplicates(self) -&gt; None:\n    \"\"\"Log duplicate nodes and edges.\n\n    Get the set of duplicate nodes and edges encountered and print them to\n    the logger.\n    \"\"\"\n    dn = self._deduplicator.get_duplicate_nodes()\n\n    if dn:\n        ntypes = dn[0]\n        nids = dn[1]\n\n        msg = \"Duplicate node types encountered (IDs in log): \\n\"\n        for typ in ntypes:\n            msg += f\"    {typ}\\n\"\n\n        logger.info(msg)\n\n        idmsg = \"Duplicate node IDs encountered: \\n\"\n        for _id in nids:\n            idmsg += f\"    {_id}\\n\"\n\n        logger.debug(idmsg)\n\n    else:\n        logger.info(\"No duplicate nodes in input.\")\n\n    de = self._deduplicator.get_duplicate_edges()\n\n    if de:\n        etypes = de[0]\n        eids = de[1]\n\n        msg = \"Duplicate edge types encountered (IDs in log): \\n\"\n        for typ in etypes:\n            msg += f\"    {typ}\\n\"\n\n        logger.info(msg)\n\n        idmsg = \"Duplicate edge IDs encountered: \\n\"\n        for _id in eids:\n            idmsg += f\"    {_id}\\n\"\n\n        logger.debug(idmsg)\n\n    else:\n        logger.info(\"No duplicate edges in input.\")\n</code></pre>"},{"location":"reference/source/biocypher/#biocypher.BioCypher.log_missing_input_labels","title":"<code>log_missing_input_labels()</code>","text":"<p>Log missing input labels.</p> <p>Get the set of input labels encountered without an entry in the <code>schema_config.yaml</code> and print them to the logger.</p>"},{"location":"reference/source/biocypher/#biocypher.BioCypher.log_missing_input_labels--returns","title":"Returns","text":"<pre><code>Optional[Dict[str, List[str]]]: A dictionary of Biolink types\nencountered without an entry in the `schema_config.yaml` file.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def log_missing_input_labels(self) -&gt; dict[str, list[str]] | None:\n    \"\"\"Log missing input labels.\n\n    Get the set of input labels encountered without an entry in the\n    `schema_config.yaml` and print them to the logger.\n\n    Returns\n    -------\n        Optional[Dict[str, List[str]]]: A dictionary of Biolink types\n        encountered without an entry in the `schema_config.yaml` file.\n\n    \"\"\"\n    mt = self._translator.get_missing_biolink_types()\n\n    if mt:\n        msg = (\n            \"Input entities not accounted for due to them not being \"\n            f\"present in the schema configuration file {self._schema_config_path} \"\n            \"(this is not necessarily a problem, if you did not intend \"\n            \"to include them in the database; see the log for details): \\n\"\n        )\n        for k, v in mt.items():\n            msg += f\"    {k}: {v} \\n\"\n\n        logger.info(msg)\n        return mt\n\n    else:\n        logger.info(\"No missing labels in input.\")\n        return None\n</code></pre>"},{"location":"reference/source/biocypher/#biocypher.BioCypher.merge_edges","title":"<code>merge_edges(edges)</code>","text":"<p>Merge edges into database.</p> <p>Either takes an iterable of tuples (if given, translates to <code>BioCypherEdge</code> objects) or an iterable of <code>BioCypherEdge</code> objects.</p> <pre><code>edges (iterable): An iterable of edges to merge into the database.\n</code></pre> <pre><code>bool: True if successful.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def merge_edges(self, edges) -&gt; bool:\n    \"\"\"Merge edges into database.\n\n    Either takes an iterable of tuples (if given, translates to\n    ``BioCypherEdge`` objects) or an iterable of ``BioCypherEdge`` objects.\n\n    Args:\n    ----\n        edges (iterable): An iterable of edges to merge into the database.\n\n    Returns:\n    -------\n        bool: True if successful.\n\n    \"\"\"\n    return self._add_edges(edges)\n</code></pre>"},{"location":"reference/source/biocypher/#biocypher.BioCypher.merge_nodes","title":"<code>merge_nodes(nodes)</code>","text":"<p>Merge nodes into database.</p> <p>Either takes an iterable of tuples (if given, translates to <code>BioCypherNode</code> objects) or an iterable of <code>BioCypherNode</code> objects.</p> <pre><code>nodes (iterable): An iterable of nodes to merge into the database.\n</code></pre> <pre><code>bool: True if successful.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def merge_nodes(self, nodes) -&gt; bool:\n    \"\"\"Merge nodes into database.\n\n    Either takes an iterable of tuples (if given, translates to\n    ``BioCypherNode`` objects) or an iterable of ``BioCypherNode`` objects.\n\n    Args:\n    ----\n        nodes (iterable): An iterable of nodes to merge into the database.\n\n    Returns:\n    -------\n        bool: True if successful.\n\n    \"\"\"\n    return self._add_nodes(nodes)\n</code></pre>"},{"location":"reference/source/biocypher/#biocypher.BioCypher.reverse_translate_query","title":"<code>reverse_translate_query(query)</code>","text":"<p>Reverse translate a query from its BioCypher equivalent.</p> <pre><code>query (str): The BioCypher query to reverse translate.\n</code></pre> <pre><code>str: The original query.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def reverse_translate_query(self, query: str) -&gt; str:\n    \"\"\"Reverse translate a query from its BioCypher equivalent.\n\n    Args:\n    ----\n        query (str): The BioCypher query to reverse translate.\n\n    Returns:\n    -------\n        str: The original query.\n\n    \"\"\"\n    # instantiate adapter if not exists\n    self.start_ontology()\n\n    return self._translator.reverse_translate(query)\n</code></pre>"},{"location":"reference/source/biocypher/#biocypher.BioCypher.reverse_translate_term","title":"<code>reverse_translate_term(term)</code>","text":"<p>Reverse translate a term from its BioCypher equivalent.</p> <pre><code>term (str): The BioCypher term to reverse translate.\n</code></pre> <pre><code>str: The original term.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def reverse_translate_term(self, term: str) -&gt; str:\n    \"\"\"Reverse translate a term from its BioCypher equivalent.\n\n    Args:\n    ----\n        term (str): The BioCypher term to reverse translate.\n\n    Returns:\n    -------\n        str: The original term.\n\n    \"\"\"\n    # instantiate adapter if not exists\n    self.start_ontology()\n\n    return self._translator.reverse_translate_term(term)\n</code></pre>"},{"location":"reference/source/biocypher/#biocypher.BioCypher.show_ontology_structure","title":"<code>show_ontology_structure(**kwargs)</code>","text":"<p>Show the ontology structure using treelib or write to GRAPHML file.</p> <pre><code>to_disk (str): If specified, the ontology structure will be saved\n    to disk as a GRAPHML file, to be opened in your favourite\n    graph visualisation tool.\n\nfull (bool): If True, the full ontology structure will be shown,\n    including all nodes and edges. If False, only the nodes and\n    edges that are relevant to the extended schema will be shown.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def show_ontology_structure(self, **kwargs) -&gt; None:\n    \"\"\"Show the ontology structure using treelib or write to GRAPHML file.\n\n    Args:\n    ----\n        to_disk (str): If specified, the ontology structure will be saved\n            to disk as a GRAPHML file, to be opened in your favourite\n            graph visualisation tool.\n\n        full (bool): If True, the full ontology structure will be shown,\n            including all nodes and edges. If False, only the nodes and\n            edges that are relevant to the extended schema will be shown.\n\n    \"\"\"\n    if not self._ontology:\n        self._get_ontology()\n\n    return self._ontology.show_ontology_structure(**kwargs)\n</code></pre>"},{"location":"reference/source/biocypher/#biocypher.BioCypher.summary","title":"<code>summary()</code>","text":"<p>Call convenience and reporting methods.</p> <p>Shows ontology structure and logs duplicates and missing input types.</p> Source code in <code>biocypher/_core.py</code> <pre><code>def summary(self) -&gt; None:\n    \"\"\"Call convenience and reporting methods.\n\n    Shows ontology structure and logs duplicates and missing input types.\n    \"\"\"\n    self.show_ontology_structure()\n    self.log_duplicates()\n    self.log_missing_input_labels()\n</code></pre>"},{"location":"reference/source/biocypher/#biocypher.BioCypher.to_df","title":"<code>to_df()</code>","text":"<p>Create DataFrame using internal representation.</p> <p>TODO: to_df implies data frame, should be specifically that use case</p> Source code in <code>biocypher/_core.py</code> <pre><code>def to_df(self):\n    \"\"\"Create DataFrame using internal representation.\n\n    TODO: to_df implies data frame, should be specifically that use case\n    \"\"\"\n    return self._to_KG()\n</code></pre>"},{"location":"reference/source/biocypher/#biocypher.BioCypher.to_networkx","title":"<code>to_networkx()</code>","text":"<p>Create networkx using internal representation.</p> Source code in <code>biocypher/_core.py</code> <pre><code>def to_networkx(self):\n    \"\"\"Create networkx using internal representation.\"\"\"\n    return self._to_KG()\n</code></pre>"},{"location":"reference/source/biocypher/#biocypher.BioCypher.translate_query","title":"<code>translate_query(query)</code>","text":"<p>Translate a query to its BioCypher equivalent.</p> <pre><code>query (str): The query to translate.\n</code></pre> <pre><code>str: The BioCypher equivalent of the query.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def translate_query(self, query: str) -&gt; str:\n    \"\"\"Translate a query to its BioCypher equivalent.\n\n    Args:\n    ----\n        query (str): The query to translate.\n\n    Returns:\n    -------\n        str: The BioCypher equivalent of the query.\n\n    \"\"\"\n    # instantiate adapter if not exists\n    self.start_ontology()\n\n    return self._translator.translate(query)\n</code></pre>"},{"location":"reference/source/biocypher/#biocypher.BioCypher.translate_term","title":"<code>translate_term(term)</code>","text":"<p>Translate a term to its BioCypher equivalent.</p> <pre><code>term (str): The term to translate.\n</code></pre> <pre><code>str: The BioCypher equivalent of the term.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def translate_term(self, term: str) -&gt; str:\n    \"\"\"Translate a term to its BioCypher equivalent.\n\n    Args:\n    ----\n        term (str): The term to translate.\n\n    Returns:\n    -------\n        str: The BioCypher equivalent of the term.\n\n    \"\"\"\n    # instantiate adapter if not exists\n    self.start_ontology()\n\n    return self._translator.translate_term(term)\n</code></pre>"},{"location":"reference/source/biocypher/#biocypher.BioCypher.write_edges","title":"<code>write_edges(edges, batch_size=int(1000000.0))</code>","text":"<p>Write edges to database.</p> <p>Either takes an iterable of tuples (if given, translates to <code>BioCypherEdge</code> objects) or an iterable of <code>BioCypherEdge</code> objects.</p> <pre><code>edges (iterable): An iterable of edges to write to the database.\n</code></pre> <pre><code>bool: True if successful.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def write_edges(self, edges, batch_size: int = int(1e6)) -&gt; bool:\n    \"\"\"Write edges to database.\n\n    Either takes an iterable of tuples (if given, translates to\n    ``BioCypherEdge`` objects) or an iterable of ``BioCypherEdge`` objects.\n\n    Args:\n    ----\n        edges (iterable): An iterable of edges to write to the database.\n\n    Returns:\n    -------\n        bool: True if successful.\n\n    \"\"\"\n    return self._add_edges(edges, batch_size=batch_size)\n</code></pre>"},{"location":"reference/source/biocypher/#biocypher.BioCypher.write_import_call","title":"<code>write_import_call()</code>","text":"<p>Write a shell script to import the database.</p> <p>Shell script is written depending on the chosen DBMS.</p>"},{"location":"reference/source/biocypher/#biocypher.BioCypher.write_import_call--returns","title":"Returns","text":"<pre><code>str: path toward the file holding the import call.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def write_import_call(self) -&gt; str:\n    \"\"\"Write a shell script to import the database.\n\n    Shell script is written depending on the chosen DBMS.\n\n    Returns\n    -------\n        str: path toward the file holding the import call.\n\n    \"\"\"\n    if not self._offline:\n        msg = \"Cannot write import call in online mode.\"\n        raise NotImplementedError(msg)\n\n    return self._writer.write_import_call()\n</code></pre>"},{"location":"reference/source/biocypher/#biocypher.BioCypher.write_nodes","title":"<code>write_nodes(nodes, batch_size=int(1000000.0), force=False)</code>","text":"<p>Write nodes to database.</p> <p>Either takes an iterable of tuples (if given, translates to <code>BioCypherNode</code> objects) or an iterable of <code>BioCypherNode</code> objects.</p> <pre><code>nodes (iterable): An iterable of nodes to write to the database.\nbatch_size (int): The batch size to use when writing to disk.\nforce (bool): Whether to force writing to the output directory even\n    if the node type is not present in the schema config file.\n</code></pre> <pre><code>bool: True if successful.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def write_nodes(\n    self,\n    nodes,\n    batch_size: int = int(1e6),\n    force: bool = False,\n) -&gt; bool:\n    \"\"\"Write nodes to database.\n\n    Either takes an iterable of tuples (if given, translates to\n    ``BioCypherNode`` objects) or an iterable of ``BioCypherNode`` objects.\n\n    Args:\n    ----\n        nodes (iterable): An iterable of nodes to write to the database.\n        batch_size (int): The batch size to use when writing to disk.\n        force (bool): Whether to force writing to the output directory even\n            if the node type is not present in the schema config file.\n\n    Returns:\n    -------\n        bool: True if successful.\n\n    \"\"\"\n    return self._add_nodes(nodes, batch_size=batch_size, force=force)\n</code></pre>"},{"location":"reference/source/biocypher/#biocypher.BioCypher.write_schema_info","title":"<code>write_schema_info(as_node=False)</code>","text":"<p>Write an extended schema info to file or node.</p> <p>Creates a YAML file or KG node that extends the <code>schema_config.yaml</code> with run-time information of the built KG. For instance, include information on whether something present in the actual knowledge graph, whether it is a relationship (which is important in the case of representing relationships as nodes) and the actual sources and targets of edges. Since this file can be used in place of the original <code>schema_config.yaml</code> file, it indicates that it is the extended schema by setting <code>is_schema_info</code> to <code>true</code>.</p> <p>We start by using the <code>extended_schema</code> dictionary from the ontology class instance, which contains all expanded entities and relationships. The information of whether something is a relationship can be gathered from the deduplicator instance, which keeps track of all entities that have been seen.</p> <pre><code>as_node (bool): If True, the schema info is written as a KG node.\n    If False, the schema info is written to a YAML file.\n</code></pre> Source code in <code>biocypher/_core.py</code> <pre><code>def write_schema_info(self, as_node: bool = False) -&gt; None:\n    \"\"\"Write an extended schema info to file or node.\n\n    Creates a YAML file or KG node that extends the `schema_config.yaml`\n    with run-time information of the built KG. For instance, include\n    information on whether something present in the actual knowledge graph,\n    whether it is a relationship (which is important in the case of\n    representing relationships as nodes) and the actual sources and\n    targets of edges. Since this file can be used in place of the original\n    `schema_config.yaml` file, it indicates that it is the extended schema\n    by setting `is_schema_info` to `true`.\n\n    We start by using the `extended_schema` dictionary from the ontology\n    class instance, which contains all expanded entities and relationships.\n    The information of whether something is a relationship can be gathered\n    from the deduplicator instance, which keeps track of all entities that\n    have been seen.\n\n    Args:\n    ----\n        as_node (bool): If True, the schema info is written as a KG node.\n            If False, the schema info is written to a YAML file.\n\n    \"\"\"\n    if (not self._offline) and self._dbms not in IN_MEMORY_DBMS:\n        msg = \"Cannot write schema info in online mode.\"\n        raise NotImplementedError(msg)\n\n    ontology = self._get_ontology()\n    schema = ontology.mapping.extended_schema.copy()\n    schema[\"is_schema_info\"] = True\n\n    deduplicator = self._get_deduplicator()\n    for node in deduplicator.entity_types:\n        if node in schema:\n            schema[node][\"present_in_knowledge_graph\"] = True\n            schema[node][\"is_relationship\"] = False\n        else:\n            logger.info(\n                f\"Node {node} not present in extended schema. Skipping schema info.\",\n            )\n\n    # find 'label_as_edge' cases in schema entries\n    changed_labels = {}\n    for k, v in schema.items():\n        if not isinstance(v, dict):\n            continue\n        if \"label_as_edge\" in v:\n            if v[\"label_as_edge\"] in deduplicator.seen_relationships:\n                changed_labels[v[\"label_as_edge\"]] = k\n\n    for edge in deduplicator.seen_relationships:\n        if edge in changed_labels:\n            edge = changed_labels[edge]\n        if edge in schema:\n            schema[edge][\"present_in_knowledge_graph\"] = True\n            schema[edge][\"is_relationship\"] = True\n            # TODO information about source and target nodes\n        else:\n            logger.info(\n                f\"Edge {edge} not present in extended schema. Skipping schema info.\",\n            )\n\n    # write to output directory as YAML file\n    path = os.path.join(self._output_directory, \"schema_info.yaml\")\n    with open(path, \"w\") as f:\n        f.write(yaml.dump(schema))\n\n    if as_node:\n        # write as node\n        node = BioCypherNode(\n            node_id=\"schema_info\",\n            node_label=\"schema_info\",\n            properties={\"schema_info\": json.dumps(schema)},\n        )\n        self.write_nodes([node], force=True)\n\n        # override import call with added schema info node\n        self.write_import_call()\n\n    return schema\n</code></pre>"},{"location":"reference/source/download-cache/","title":"Download and Cache","text":""},{"location":"reference/source/download-cache/#resource-base-class","title":"Resource Base Class","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>biocypher/_get.py</code> <pre><code>class Resource(ABC):\n    def __init__(\n        self,\n        name: str,\n        url_s: str | list[str],\n        lifetime: int = 0,\n    ):\n        \"\"\"Initialize a Resource.\n\n        A Resource is a file, a list of files, an API request, or a list of API\n        requests, any of which can be downloaded from the given URL(s) and\n        cached locally. This class implements checks of the minimum requirements\n        for a resource, to be implemented by a biocypher adapter.\n\n        Args:\n        ----\n            name (str): The name of the resource.\n\n            url_s (str | list[str]): The URL or URLs of the resource.\n\n            lifetime (int): The lifetime of the resource in days. If 0, the\n                resource is considered to be permanent.\n\n        \"\"\"\n        self.name = name\n        self.url_s = url_s\n        self.lifetime = lifetime\n</code></pre>"},{"location":"reference/source/download-cache/#biocypher._get.Resource.__init__","title":"<code>__init__(name, url_s, lifetime=0)</code>","text":"<p>Initialize a Resource.</p> <p>A Resource is a file, a list of files, an API request, or a list of API requests, any of which can be downloaded from the given URL(s) and cached locally. This class implements checks of the minimum requirements for a resource, to be implemented by a biocypher adapter.</p> <pre><code>name (str): The name of the resource.\n\nurl_s (str | list[str]): The URL or URLs of the resource.\n\nlifetime (int): The lifetime of the resource in days. If 0, the\n    resource is considered to be permanent.\n</code></pre> Source code in <code>biocypher/_get.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    url_s: str | list[str],\n    lifetime: int = 0,\n):\n    \"\"\"Initialize a Resource.\n\n    A Resource is a file, a list of files, an API request, or a list of API\n    requests, any of which can be downloaded from the given URL(s) and\n    cached locally. This class implements checks of the minimum requirements\n    for a resource, to be implemented by a biocypher adapter.\n\n    Args:\n    ----\n        name (str): The name of the resource.\n\n        url_s (str | list[str]): The URL or URLs of the resource.\n\n        lifetime (int): The lifetime of the resource in days. If 0, the\n            resource is considered to be permanent.\n\n    \"\"\"\n    self.name = name\n    self.url_s = url_s\n    self.lifetime = lifetime\n</code></pre>"},{"location":"reference/source/download-cache/#api-request","title":"API Request","text":"<p>               Bases: <code>Resource</code></p> Source code in <code>biocypher/_get.py</code> <pre><code>class APIRequest(Resource):\n    def __init__(self, name: str, url_s: str | list[str], lifetime: int = 0):\n        \"\"\"Initialize an APIRequest object.\n\n        Represents basic information for an API Request.\n\n        Args:\n        ----\n            name(str): The name of the API Request.\n\n            url_s(str|list): The URL of the API endpoint.\n\n            lifetime(int): The lifetime of the API Request in days. If 0, the\n                API Request is cached indefinitely.\n\n        \"\"\"\n        super().__init__(name, url_s, lifetime)\n</code></pre>"},{"location":"reference/source/download-cache/#biocypher._get.APIRequest.__init__","title":"<code>__init__(name, url_s, lifetime=0)</code>","text":"<p>Initialize an APIRequest object.</p> <p>Represents basic information for an API Request.</p> <pre><code>name(str): The name of the API Request.\n\nurl_s(str|list): The URL of the API endpoint.\n\nlifetime(int): The lifetime of the API Request in days. If 0, the\n    API Request is cached indefinitely.\n</code></pre> Source code in <code>biocypher/_get.py</code> <pre><code>def __init__(self, name: str, url_s: str | list[str], lifetime: int = 0):\n    \"\"\"Initialize an APIRequest object.\n\n    Represents basic information for an API Request.\n\n    Args:\n    ----\n        name(str): The name of the API Request.\n\n        url_s(str|list): The URL of the API endpoint.\n\n        lifetime(int): The lifetime of the API Request in days. If 0, the\n            API Request is cached indefinitely.\n\n    \"\"\"\n    super().__init__(name, url_s, lifetime)\n</code></pre>"},{"location":"reference/source/download-cache/#file-download","title":"File Download","text":"<p>               Bases: <code>Resource</code></p> Source code in <code>biocypher/_get.py</code> <pre><code>class FileDownload(Resource):\n    def __init__(\n        self,\n        name: str,\n        url_s: str | list[str],\n        lifetime: int = 0,\n        is_dir: bool = False,\n    ):\n        \"\"\"Initialize a FileDownload object.\n\n        Represents basic information for a File Download.\n\n        Args:\n        ----\n            name(str): The name of the File Download.\n\n            url_s(str|list[str]): The URL(s) of the File Download.\n\n            lifetime(int): The lifetime of the File Download in days. If 0, the\n                File Download is cached indefinitely.\n\n            is_dir (bool): Whether the URL points to a directory or not.\n\n        \"\"\"\n        super().__init__(name, url_s, lifetime)\n        self.is_dir = is_dir\n</code></pre>"},{"location":"reference/source/download-cache/#biocypher._get.FileDownload.__init__","title":"<code>__init__(name, url_s, lifetime=0, is_dir=False)</code>","text":"<p>Initialize a FileDownload object.</p> <p>Represents basic information for a File Download.</p> <pre><code>name(str): The name of the File Download.\n\nurl_s(str|list[str]): The URL(s) of the File Download.\n\nlifetime(int): The lifetime of the File Download in days. If 0, the\n    File Download is cached indefinitely.\n\nis_dir (bool): Whether the URL points to a directory or not.\n</code></pre> Source code in <code>biocypher/_get.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    url_s: str | list[str],\n    lifetime: int = 0,\n    is_dir: bool = False,\n):\n    \"\"\"Initialize a FileDownload object.\n\n    Represents basic information for a File Download.\n\n    Args:\n    ----\n        name(str): The name of the File Download.\n\n        url_s(str|list[str]): The URL(s) of the File Download.\n\n        lifetime(int): The lifetime of the File Download in days. If 0, the\n            File Download is cached indefinitely.\n\n        is_dir (bool): Whether the URL points to a directory or not.\n\n    \"\"\"\n    super().__init__(name, url_s, lifetime)\n    self.is_dir = is_dir\n</code></pre>"},{"location":"reference/source/download-cache/#downloader","title":"Downloader","text":"Source code in <code>biocypher/_get.py</code> <pre><code>class Downloader:\n    def __init__(self, cache_dir: Optional[str] = None) -&gt; None:\n        \"\"\"Initialize the Downloader.\n\n        The Downloader is a class that manages resources that can be downloaded\n        and cached locally. It manages the lifetime of downloaded resources by\n        keeping a JSON record of the download date of each resource.\n\n        Args:\n        ----\n            cache_dir (str): The directory where the resources are cached. If\n                not given, a temporary directory is created.\n\n        \"\"\"\n        self.cache_dir = cache_dir or TemporaryDirectory().name\n        self.cache_file = os.path.join(self.cache_dir, \"cache.json\")\n        self.cache_dict = self._load_cache_dict()\n\n    def download(self, *resources: Resource):\n        \"\"\"Download one or multiple resources.\n\n        Load from cache if the resource is already downloaded and the cache is\n        not expired.\n\n        Args:\n        ----\n            resources (Resource): The resource(s) to download or load from\n                cache.\n\n        Returns:\n        -------\n            list[str]: The path or paths to the resource(s) that were downloaded\n                or loaded from cache.\n\n        \"\"\"\n        paths = []\n        for resource in resources:\n            paths.append(self._download_or_cache(resource))\n\n        # flatten list if it is nested\n        if is_nested(paths):\n            paths = [path for sublist in paths for path in sublist]\n\n        return paths\n\n    def _download_or_cache(self, resource: Resource, cache: bool = True):\n        \"\"\"Download a resource if it is not cached or exceeded its lifetime.\n\n        Args:\n        ----\n            resource (Resource): The resource to download.\n\n        Returns:\n        -------\n            list[str]: The path or paths to the downloaded resource(s).\n\n        \"\"\"\n        expired = self._is_cache_expired(resource)\n\n        if expired or not cache:\n            self._delete_expired_cache(resource)\n            if isinstance(resource, FileDownload):\n                logger.info(f\"Asking for download of resource {resource.name}.\")\n                paths = self._download_files(cache, resource)\n            elif isinstance(resource, APIRequest):\n                logger.info(f\"Asking for download of api request {resource.name}.\")\n                paths = self._download_api_request(resource)\n            else:\n                raise TypeError(f\"Unknown resource type: {type(resource)}\")\n        else:\n            paths = self.get_cached_version(resource)\n        self._update_cache_record(resource)\n        return paths\n\n    def _is_cache_expired(self, resource: Resource) -&gt; bool:\n        \"\"\"Check if resource or API request cache is expired.\n\n        Args:\n        ----\n            resource (Resource): The resource to download.\n\n        Returns:\n        -------\n            bool: cache is expired or not.\n\n        \"\"\"\n        cache_record = self._get_cache_record(resource)\n        if cache_record:\n            download_time = datetime.strptime(cache_record.get(\"date_downloaded\"), \"%Y-%m-%d %H:%M:%S.%f\")\n            lifetime = timedelta(days=resource.lifetime)\n            expired = download_time + lifetime &lt; datetime.now()\n        else:\n            expired = True\n        return expired\n\n    def _delete_expired_cache(self, resource: Resource):\n        cache_resource_path = self.cache_dir + \"/\" + resource.name\n        if os.path.exists(cache_resource_path) and os.path.isdir(cache_resource_path):\n            shutil.rmtree(cache_resource_path)\n\n    def _download_files(self, cache, file_download: FileDownload) -&gt; list[str]:\n        \"\"\"Download a resource given it is a file or a directory.\n\n        Upon downloading, return the path(s).\n\n        Args:\n        ----\n            cache (bool): Whether to cache the resource or not.\n\n            file_download (FileDownload): The resource to download.\n\n        Returns:\n        -------\n            list[str]: The path or paths to the downloaded resource(s).\n\n        \"\"\"\n        if file_download.is_dir:\n            files = self._get_files(file_download)\n            file_download.url_s = [file_download.url_s + \"/\" + file for file in files]\n            file_download.is_dir = False\n            paths = self._download_or_cache(file_download, cache)\n        elif isinstance(file_download.url_s, list):\n            paths = []\n            for url in file_download.url_s:\n                fname = self._trim_filename(url)\n                path = self._retrieve(\n                    url=url,\n                    fname=fname,\n                    path=os.path.join(self.cache_dir, file_download.name),\n                )\n                paths.append(path)\n        else:\n            paths = []\n            fname = self._trim_filename(file_download.url_s)\n            results = self._retrieve(\n                url=file_download.url_s,\n                fname=fname,\n                path=os.path.join(self.cache_dir, file_download.name),\n            )\n            if isinstance(results, list):\n                paths.extend(results)\n            else:\n                paths.append(results)\n\n        # sometimes a compressed file contains multiple files\n        # TODO ask for a list of files in the archive to be used from the\n        # adapter\n        return paths\n\n    def _download_api_request(self, api_request: APIRequest) -&gt; list[str]:\n        \"\"\"Download an API request and return the path.\n\n        Args:\n        ----\n            api_request(APIRequest): The API request result that is being\n                cached.\n\n        Returns:\n        -------\n            list[str]: The path to the cached API request.\n\n        \"\"\"\n        urls = api_request.url_s if isinstance(api_request.url_s, list) else [api_request.url_s]\n        paths = []\n        for url in urls:\n            fname = self._trim_filename(url)\n            logger.info(f\"Asking for caching API of {api_request.name} {fname}.\")\n            response = requests.get(url=url)\n\n            if response.status_code != 200:\n                response.raise_for_status()\n            response_data = response.json()\n            api_path = os.path.join(self.cache_dir, api_request.name, f\"{fname}.json\")\n\n            os.makedirs(os.path.dirname(api_path), exist_ok=True)\n            with open(api_path, \"w\") as f:\n                json.dump(response_data, f)\n                logger.info(f\"Caching API request to {api_path}.\")\n            paths.append(api_path)\n        return paths\n\n    def get_cached_version(self, resource: Resource) -&gt; list[str]:\n        \"\"\"Get the cached version of a resource.\n\n        Args:\n        ----\n            resource(Resource): The resource to get the cached version of.\n\n        Returns:\n        -------\n            list[str]: The paths to the cached resource(s).\n\n        \"\"\"\n        cached_location = os.path.join(self.cache_dir, resource.name)\n        logger.info(f\"Use cached version from {cached_location}.\")\n        paths = []\n        for file in os.listdir(cached_location):\n            paths.append(os.path.join(cached_location, file))\n        return paths\n\n    def _retrieve(\n        self,\n        url: str,\n        fname: str,\n        path: str,\n        known_hash: str = None,\n    ) -&gt; str:\n        \"\"\"Retrieve a file from a URL using Pooch.\n\n        Infer type of file from extension and use appropriate processor.\n\n        Args:\n        ----\n            url (str): The URL to retrieve the file from.\n\n            fname (str): The name of the file.\n\n            path (str): The path to the file.\n\n            known_hash (str): The known hash of the file.\n\n        Returns:\n        -------\n            str: The path to the file.\n\n        \"\"\"\n        if fname.endswith(\".zip\"):\n            return pooch.retrieve(\n                url=url,\n                known_hash=known_hash,\n                fname=fname,\n                path=path,\n                processor=pooch.Unzip(),\n                progressbar=True,\n            )\n\n        elif fname.endswith(\".tar.gz\"):\n            return pooch.retrieve(\n                url=url,\n                known_hash=known_hash,\n                fname=fname,\n                path=path,\n                processor=pooch.Untar(),\n                progressbar=True,\n            )\n\n        elif fname.endswith(\".gz\"):\n            return pooch.retrieve(\n                url=url,\n                known_hash=known_hash,\n                fname=fname,\n                path=path,\n                processor=pooch.Decompress(),\n                progressbar=True,\n            )\n\n        else:\n            return pooch.retrieve(\n                url=url,\n                known_hash=known_hash,\n                fname=fname,\n                path=path,\n                progressbar=True,\n            )\n\n    def _get_files(self, file_download: FileDownload) -&gt; list[str]:\n        \"\"\"Get the files contained in a directory file.\n\n        Args:\n        ----\n            file_download (FileDownload): The directory file.\n\n        Returns:\n        -------\n            list[str]: The files contained in the directory.\n\n        \"\"\"\n        if file_download.url_s.startswith(\"ftp://\"):\n            # remove protocol\n            url = file_download.url_s[6:]\n            # get base url\n            url = url[: url.find(\"/\")]\n            # get directory (remove initial slash as well)\n            dir = file_download.url_s[7 + len(url) :]\n            # get files\n            ftp = ftplib.FTP(url)\n            ftp.login()\n            ftp.cwd(dir)\n            files = ftp.nlst()\n            ftp.quit()\n        else:\n            msg = \"Only FTP directories are supported at the moment.\"\n            logger.error(msg)\n            raise NotImplementedError(msg)\n\n        return files\n\n    def _load_cache_dict(self) -&gt; dict:\n        \"\"\"Load the cache dictionary from the cache file.\n\n        Create an empty cache file if it does not exist.\n\n        Args:\n        ----\n            None.\n\n        Returns:\n        -------\n            dict: The cache dictionary.\n\n        \"\"\"\n        if not os.path.exists(self.cache_dir):\n            logger.info(f\"Creating cache directory {self.cache_dir}.\")\n            os.makedirs(self.cache_dir)\n\n        if not os.path.exists(self.cache_file):\n            logger.info(f\"Creating cache file {self.cache_file}.\")\n            with open(self.cache_file, \"w\") as f:\n                json.dump({}, f)\n\n        with open(self.cache_file) as f:\n            logger.info(f\"Loading cache file {self.cache_file}.\")\n            return json.load(f)\n\n    def _get_cache_record(self, resource: Resource) -&gt; dict:\n        \"\"\"Get the cache record of a resource.\n\n        Args:\n        ----\n            resource (Resource): The resource to get the cache record of.\n\n        Returns:\n        -------\n            dict: The cache record of the resource.\n\n        \"\"\"\n        return self.cache_dict.get(resource.name, {})\n\n    def _update_cache_record(self, resource: Resource) -&gt; None:\n        \"\"\"Update the cache record of a resource.\n\n        Args:\n        ----\n            resource (Resource): The resource to update the cache record of.\n\n        \"\"\"\n        cache_record = {}\n        cache_record[\"url\"] = to_list(resource.url_s)\n        cache_record[\"date_downloaded\"] = str(datetime.now())\n        cache_record[\"lifetime\"] = resource.lifetime\n        self.cache_dict[resource.name] = cache_record\n        with open(self.cache_file, \"w\") as f:\n            json.dump(self.cache_dict, f, default=str)\n\n    def _trim_filename(self, url: str, max_length: int = 150) -&gt; str:\n        \"\"\"Create a trimmed filename from a URL.\n\n        If the URL exceeds max_length, create a hash of the filename.\n\n        Args:\n        ----\n            url (str): The URL to generate a filename from\n            max_length (int): Maximum filename length (default: 150)\n\n        Returns:\n        -------\n            str: A valid filename derived from the URL, trimmed if necessary\n\n        \"\"\"\n        # Extract the filename from the URL\n        fname = url[url.rfind(\"/\") + 1 :]\n\n        # Remove query parameters if present\n        if \"?\" in fname:\n            fname = fname.split(\"?\")[0]\n\n        if len(fname) &gt; max_length:\n            import hashlib\n\n            fname_trimmed = hashlib.md5(fname.encode()).hexdigest()\n        else:\n            fname_trimmed = fname\n\n        return fname_trimmed\n</code></pre>"},{"location":"reference/source/download-cache/#biocypher._get.Downloader.__init__","title":"<code>__init__(cache_dir=None)</code>","text":"<p>Initialize the Downloader.</p> <p>The Downloader is a class that manages resources that can be downloaded and cached locally. It manages the lifetime of downloaded resources by keeping a JSON record of the download date of each resource.</p> <pre><code>cache_dir (str): The directory where the resources are cached. If\n    not given, a temporary directory is created.\n</code></pre> Source code in <code>biocypher/_get.py</code> <pre><code>def __init__(self, cache_dir: Optional[str] = None) -&gt; None:\n    \"\"\"Initialize the Downloader.\n\n    The Downloader is a class that manages resources that can be downloaded\n    and cached locally. It manages the lifetime of downloaded resources by\n    keeping a JSON record of the download date of each resource.\n\n    Args:\n    ----\n        cache_dir (str): The directory where the resources are cached. If\n            not given, a temporary directory is created.\n\n    \"\"\"\n    self.cache_dir = cache_dir or TemporaryDirectory().name\n    self.cache_file = os.path.join(self.cache_dir, \"cache.json\")\n    self.cache_dict = self._load_cache_dict()\n</code></pre>"},{"location":"reference/source/download-cache/#biocypher._get.Downloader.download","title":"<code>download(*resources)</code>","text":"<p>Download one or multiple resources.</p> <p>Load from cache if the resource is already downloaded and the cache is not expired.</p> <pre><code>resources (Resource): The resource(s) to download or load from\n    cache.\n</code></pre> <pre><code>list[str]: The path or paths to the resource(s) that were downloaded\n    or loaded from cache.\n</code></pre> Source code in <code>biocypher/_get.py</code> <pre><code>def download(self, *resources: Resource):\n    \"\"\"Download one or multiple resources.\n\n    Load from cache if the resource is already downloaded and the cache is\n    not expired.\n\n    Args:\n    ----\n        resources (Resource): The resource(s) to download or load from\n            cache.\n\n    Returns:\n    -------\n        list[str]: The path or paths to the resource(s) that were downloaded\n            or loaded from cache.\n\n    \"\"\"\n    paths = []\n    for resource in resources:\n        paths.append(self._download_or_cache(resource))\n\n    # flatten list if it is nested\n    if is_nested(paths):\n        paths = [path for sublist in paths for path in sublist]\n\n    return paths\n</code></pre>"},{"location":"reference/source/download-cache/#biocypher._get.Downloader.get_cached_version","title":"<code>get_cached_version(resource)</code>","text":"<p>Get the cached version of a resource.</p> <pre><code>resource(Resource): The resource to get the cached version of.\n</code></pre> <pre><code>list[str]: The paths to the cached resource(s).\n</code></pre> Source code in <code>biocypher/_get.py</code> <pre><code>def get_cached_version(self, resource: Resource) -&gt; list[str]:\n    \"\"\"Get the cached version of a resource.\n\n    Args:\n    ----\n        resource(Resource): The resource to get the cached version of.\n\n    Returns:\n    -------\n        list[str]: The paths to the cached resource(s).\n\n    \"\"\"\n    cached_location = os.path.join(self.cache_dir, resource.name)\n    logger.info(f\"Use cached version from {cached_location}.\")\n    paths = []\n    for file in os.listdir(cached_location):\n        paths.append(os.path.join(cached_location, file))\n    return paths\n</code></pre>"},{"location":"reference/source/graph-handling/","title":"Graph Handling","text":""},{"location":"reference/source/graph-handling/#biocypher-node","title":"BioCypher Node","text":"<p>Handoff class to represent biomedical entities as Neo4j nodes.</p> <p>Has id, label, property dict; id and label (in the Neo4j sense of a label, ie, the entity descriptor after the colon, such as \":Protein\") are non-optional and called node_id and node_label to avoid confusion with \"label\" properties. Node labels are written in PascalCase and as nouns, as per Neo4j consensus.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>string</code> <p>consensus \"best\" id for biological entity</p> required <code>node_label</code> <code>string</code> <p>primary type of entity, capitalised</p> required <code>**properties</code> <code>kwargs</code> <p>collection of all other properties to be passed to neo4j for the respective node (dict)</p> <code>dict()</code> Todo <ul> <li>check and correct small inconsistencies such as capitalisation     of ID names (\"uniprot\" vs \"UniProt\")</li> <li>check for correct ID patterns (eg \"ENSG\" + string of numbers,     uniprot length)</li> <li>ID conversion using pypath translation facilities for now</li> </ul> Source code in <code>biocypher/_create.py</code> <pre><code>@dataclass(frozen=True)\nclass BioCypherNode:\n    \"\"\"\n    Handoff class to represent biomedical entities as Neo4j nodes.\n\n    Has id, label, property dict; id and label (in the Neo4j sense of a\n    label, ie, the entity descriptor after the colon, such as\n    \":Protein\") are non-optional and called node_id and node_label to\n    avoid confusion with \"label\" properties. Node labels are written in\n    PascalCase and as nouns, as per Neo4j consensus.\n\n    Args:\n        node_id (string): consensus \"best\" id for biological entity\n        node_label (string): primary type of entity, capitalised\n        **properties (kwargs): collection of all other properties to be\n            passed to neo4j for the respective node (dict)\n\n    Todo:\n        - check and correct small inconsistencies such as capitalisation\n            of ID names (\"uniprot\" vs \"UniProt\")\n        - check for correct ID patterns (eg \"ENSG\" + string of numbers,\n            uniprot length)\n        - ID conversion using pypath translation facilities for now\n    \"\"\"\n\n    node_id: str\n    node_label: str\n    preferred_id: str = \"id\"\n    properties: dict = field(default_factory=dict)\n\n    def __post_init__(self):\n        \"\"\"\n        Add id field to properties.\n\n        Check for reserved keywords.\n\n        Replace unwanted characters in properties.\n        \"\"\"\n        self.properties[\"id\"] = self.node_id\n        self.properties[\"preferred_id\"] = self.preferred_id or None\n        # TODO actually make None possible here; as is, \"id\" is the default in\n        # the dataclass as well as in the configuration file\n\n        if \":TYPE\" in self.properties.keys():\n            logger.warning(\n                \"Keyword ':TYPE' is reserved for Neo4j. Removing from properties.\",\n                # \"Renaming to 'type'.\"\n            )\n            # self.properties[\"type\"] = self.properties[\":TYPE\"]\n            del self.properties[\":TYPE\"]\n\n        for k, v in self.properties.items():\n            if isinstance(v, str):\n                self.properties[k] = (\n                    v.replace(\n                        os.linesep,\n                        \" \",\n                    )\n                    .replace(\n                        \"\\n\",\n                        \" \",\n                    )\n                    .replace(\n                        \"\\r\",\n                        \" \",\n                    )\n                )\n\n            elif isinstance(v, list):\n                self.properties[k] = [\n                    val.replace(\n                        os.linesep,\n                        \" \",\n                    )\n                    .replace(\n                        \"\\n\",\n                        \" \",\n                    )\n                    .replace(\"\\r\", \" \")\n                    for val in v\n                ]\n\n    def get_id(self) -&gt; str:\n        \"\"\"\n        Returns primary node identifier.\n\n        Returns:\n            str: node_id\n        \"\"\"\n        return self.node_id\n\n    def get_label(self) -&gt; str:\n        \"\"\"\n        Returns primary node label.\n\n        Returns:\n            str: node_label\n        \"\"\"\n        return self.node_label\n\n    def get_type(self) -&gt; str:\n        \"\"\"\n        Returns primary node label.\n\n        Returns:\n            str: node_label\n        \"\"\"\n        return self.node_label\n\n    def get_preferred_id(self) -&gt; str:\n        \"\"\"\n        Returns preferred id.\n\n        Returns:\n            str: preferred_id\n        \"\"\"\n        return self.preferred_id\n\n    def get_properties(self) -&gt; dict:\n        \"\"\"\n        Returns all other node properties apart from primary id and\n        label as key-value pairs.\n\n        Returns:\n            dict: properties\n        \"\"\"\n        return self.properties\n\n    def get_dict(self) -&gt; dict:\n        \"\"\"\n        Return dict of id, labels, and properties.\n\n        Returns:\n            dict: node_id and node_label as top-level key-value pairs,\n            properties as second-level dict.\n        \"\"\"\n        return {\n            \"node_id\": self.node_id,\n            \"node_label\": self.node_label,\n            \"properties\": self.properties,\n        }\n</code></pre>"},{"location":"reference/source/graph-handling/#biocypher._create.BioCypherNode.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Add id field to properties.</p> <p>Check for reserved keywords.</p> <p>Replace unwanted characters in properties.</p> Source code in <code>biocypher/_create.py</code> <pre><code>def __post_init__(self):\n    \"\"\"\n    Add id field to properties.\n\n    Check for reserved keywords.\n\n    Replace unwanted characters in properties.\n    \"\"\"\n    self.properties[\"id\"] = self.node_id\n    self.properties[\"preferred_id\"] = self.preferred_id or None\n    # TODO actually make None possible here; as is, \"id\" is the default in\n    # the dataclass as well as in the configuration file\n\n    if \":TYPE\" in self.properties.keys():\n        logger.warning(\n            \"Keyword ':TYPE' is reserved for Neo4j. Removing from properties.\",\n            # \"Renaming to 'type'.\"\n        )\n        # self.properties[\"type\"] = self.properties[\":TYPE\"]\n        del self.properties[\":TYPE\"]\n\n    for k, v in self.properties.items():\n        if isinstance(v, str):\n            self.properties[k] = (\n                v.replace(\n                    os.linesep,\n                    \" \",\n                )\n                .replace(\n                    \"\\n\",\n                    \" \",\n                )\n                .replace(\n                    \"\\r\",\n                    \" \",\n                )\n            )\n\n        elif isinstance(v, list):\n            self.properties[k] = [\n                val.replace(\n                    os.linesep,\n                    \" \",\n                )\n                .replace(\n                    \"\\n\",\n                    \" \",\n                )\n                .replace(\"\\r\", \" \")\n                for val in v\n            ]\n</code></pre>"},{"location":"reference/source/graph-handling/#biocypher._create.BioCypherNode.get_dict","title":"<code>get_dict()</code>","text":"<p>Return dict of id, labels, and properties.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>node_id and node_label as top-level key-value pairs,</p> <code>dict</code> <p>properties as second-level dict.</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_dict(self) -&gt; dict:\n    \"\"\"\n    Return dict of id, labels, and properties.\n\n    Returns:\n        dict: node_id and node_label as top-level key-value pairs,\n        properties as second-level dict.\n    \"\"\"\n    return {\n        \"node_id\": self.node_id,\n        \"node_label\": self.node_label,\n        \"properties\": self.properties,\n    }\n</code></pre>"},{"location":"reference/source/graph-handling/#biocypher._create.BioCypherNode.get_id","title":"<code>get_id()</code>","text":"<p>Returns primary node identifier.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>node_id</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_id(self) -&gt; str:\n    \"\"\"\n    Returns primary node identifier.\n\n    Returns:\n        str: node_id\n    \"\"\"\n    return self.node_id\n</code></pre>"},{"location":"reference/source/graph-handling/#biocypher._create.BioCypherNode.get_label","title":"<code>get_label()</code>","text":"<p>Returns primary node label.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>node_label</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_label(self) -&gt; str:\n    \"\"\"\n    Returns primary node label.\n\n    Returns:\n        str: node_label\n    \"\"\"\n    return self.node_label\n</code></pre>"},{"location":"reference/source/graph-handling/#biocypher._create.BioCypherNode.get_preferred_id","title":"<code>get_preferred_id()</code>","text":"<p>Returns preferred id.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>preferred_id</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_preferred_id(self) -&gt; str:\n    \"\"\"\n    Returns preferred id.\n\n    Returns:\n        str: preferred_id\n    \"\"\"\n    return self.preferred_id\n</code></pre>"},{"location":"reference/source/graph-handling/#biocypher._create.BioCypherNode.get_properties","title":"<code>get_properties()</code>","text":"<p>Returns all other node properties apart from primary id and label as key-value pairs.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>properties</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_properties(self) -&gt; dict:\n    \"\"\"\n    Returns all other node properties apart from primary id and\n    label as key-value pairs.\n\n    Returns:\n        dict: properties\n    \"\"\"\n    return self.properties\n</code></pre>"},{"location":"reference/source/graph-handling/#biocypher._create.BioCypherNode.get_type","title":"<code>get_type()</code>","text":"<p>Returns primary node label.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>node_label</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_type(self) -&gt; str:\n    \"\"\"\n    Returns primary node label.\n\n    Returns:\n        str: node_label\n    \"\"\"\n    return self.node_label\n</code></pre>"},{"location":"reference/source/graph-handling/#biocypher-edge","title":"BioCypher Edge","text":"<p>Handoff class to represent biomedical relationships in Neo4j.</p> <p>Has source and target ids, label, property dict; ids and label (in the Neo4j sense of a label, ie, the entity descriptor after the colon, such as \":TARGETS\") are non-optional and called source_id, target_id, and relationship_label to avoid confusion with properties called \"label\", which usually denotes the human-readable form. Relationship labels are written in UPPERCASE and as verbs, as per Neo4j consensus.</p> <p>Args:</p> <pre><code>source_id (string): consensus \"best\" id for biological entity\n\ntarget_id (string): consensus \"best\" id for biological entity\n\nrelationship_label (string): type of interaction, UPPERCASE\n\nproperties (dict): collection of all other properties of the\nrespective edge\n</code></pre> Source code in <code>biocypher/_create.py</code> <pre><code>@dataclass(frozen=True)\nclass BioCypherEdge:\n    \"\"\"\n    Handoff class to represent biomedical relationships in Neo4j.\n\n    Has source and target ids, label, property dict; ids and label (in\n    the Neo4j sense of a label, ie, the entity descriptor after the\n    colon, such as \":TARGETS\") are non-optional and called source_id,\n    target_id, and relationship_label to avoid confusion with properties\n    called \"label\", which usually denotes the human-readable form.\n    Relationship labels are written in UPPERCASE and as verbs, as per\n    Neo4j consensus.\n\n    Args:\n\n        source_id (string): consensus \"best\" id for biological entity\n\n        target_id (string): consensus \"best\" id for biological entity\n\n        relationship_label (string): type of interaction, UPPERCASE\n\n        properties (dict): collection of all other properties of the\n        respective edge\n\n    \"\"\"\n\n    source_id: str\n    target_id: str\n    relationship_label: str\n    relationship_id: str = None\n    properties: dict = field(default_factory=dict)\n\n    def __post_init__(self):\n        \"\"\"\n        Check for reserved keywords.\n        \"\"\"\n\n        if \":TYPE\" in self.properties.keys():\n            logger.debug(\n                \"Keyword ':TYPE' is reserved for Neo4j. Removing from properties.\",\n                # \"Renaming to 'type'.\"\n            )\n            # self.properties[\"type\"] = self.properties[\":TYPE\"]\n            del self.properties[\":TYPE\"]\n        elif \"id\" in self.properties.keys():\n            logger.debug(\n                \"Keyword 'id' is reserved for Neo4j. Removing from properties.\",\n                # \"Renaming to 'type'.\"\n            )\n            # self.properties[\"type\"] = self.properties[\":TYPE\"]\n            del self.properties[\"id\"]\n        elif \"_ID\" in self.properties.keys():\n            logger.debug(\n                \"Keyword '_ID' is reserved for Postgres. Removing from properties.\",\n                # \"Renaming to 'type'.\"\n            )\n            # self.properties[\"type\"] = self.properties[\":TYPE\"]\n            del self.properties[\"_ID\"]\n\n    def get_id(self) -&gt; Union[str, None]:\n        \"\"\"\n        Returns primary node identifier or None.\n\n        Returns:\n            str: node_id\n        \"\"\"\n\n        return self.relationship_id\n\n    def get_source_id(self) -&gt; str:\n        \"\"\"\n        Returns primary node identifier of relationship source.\n\n        Returns:\n            str: source_id\n        \"\"\"\n        return self.source_id\n\n    def get_target_id(self) -&gt; str:\n        \"\"\"\n        Returns primary node identifier of relationship target.\n\n        Returns:\n            str: target_id\n        \"\"\"\n        return self.target_id\n\n    def get_label(self) -&gt; str:\n        \"\"\"\n        Returns relationship label.\n\n        Returns:\n            str: relationship_label\n        \"\"\"\n        return self.relationship_label\n\n    def get_type(self) -&gt; str:\n        \"\"\"\n        Returns relationship label.\n\n        Returns:\n            str: relationship_label\n        \"\"\"\n        return self.relationship_label\n\n    def get_properties(self) -&gt; dict:\n        \"\"\"\n        Returns all other relationship properties apart from primary ids\n        and label as key-value pairs.\n\n        Returns:\n            dict: properties\n        \"\"\"\n        return self.properties\n\n    def get_dict(self) -&gt; dict:\n        \"\"\"\n        Return dict of ids, label, and properties.\n\n        Returns:\n            dict: source_id, target_id and relationship_label as\n                top-level key-value pairs, properties as second-level\n                dict.\n        \"\"\"\n        return {\n            \"relationship_id\": self.relationship_id or None,\n            \"source_id\": self.source_id,\n            \"target_id\": self.target_id,\n            \"relationship_label\": self.relationship_label,\n            \"properties\": self.properties,\n        }\n</code></pre>"},{"location":"reference/source/graph-handling/#biocypher._create.BioCypherEdge.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Check for reserved keywords.</p> Source code in <code>biocypher/_create.py</code> <pre><code>def __post_init__(self):\n    \"\"\"\n    Check for reserved keywords.\n    \"\"\"\n\n    if \":TYPE\" in self.properties.keys():\n        logger.debug(\n            \"Keyword ':TYPE' is reserved for Neo4j. Removing from properties.\",\n            # \"Renaming to 'type'.\"\n        )\n        # self.properties[\"type\"] = self.properties[\":TYPE\"]\n        del self.properties[\":TYPE\"]\n    elif \"id\" in self.properties.keys():\n        logger.debug(\n            \"Keyword 'id' is reserved for Neo4j. Removing from properties.\",\n            # \"Renaming to 'type'.\"\n        )\n        # self.properties[\"type\"] = self.properties[\":TYPE\"]\n        del self.properties[\"id\"]\n    elif \"_ID\" in self.properties.keys():\n        logger.debug(\n            \"Keyword '_ID' is reserved for Postgres. Removing from properties.\",\n            # \"Renaming to 'type'.\"\n        )\n        # self.properties[\"type\"] = self.properties[\":TYPE\"]\n        del self.properties[\"_ID\"]\n</code></pre>"},{"location":"reference/source/graph-handling/#biocypher._create.BioCypherEdge.get_dict","title":"<code>get_dict()</code>","text":"<p>Return dict of ids, label, and properties.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>source_id, target_id and relationship_label as top-level key-value pairs, properties as second-level dict.</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_dict(self) -&gt; dict:\n    \"\"\"\n    Return dict of ids, label, and properties.\n\n    Returns:\n        dict: source_id, target_id and relationship_label as\n            top-level key-value pairs, properties as second-level\n            dict.\n    \"\"\"\n    return {\n        \"relationship_id\": self.relationship_id or None,\n        \"source_id\": self.source_id,\n        \"target_id\": self.target_id,\n        \"relationship_label\": self.relationship_label,\n        \"properties\": self.properties,\n    }\n</code></pre>"},{"location":"reference/source/graph-handling/#biocypher._create.BioCypherEdge.get_id","title":"<code>get_id()</code>","text":"<p>Returns primary node identifier or None.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>Union[str, None]</code> <p>node_id</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_id(self) -&gt; Union[str, None]:\n    \"\"\"\n    Returns primary node identifier or None.\n\n    Returns:\n        str: node_id\n    \"\"\"\n\n    return self.relationship_id\n</code></pre>"},{"location":"reference/source/graph-handling/#biocypher._create.BioCypherEdge.get_label","title":"<code>get_label()</code>","text":"<p>Returns relationship label.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>relationship_label</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_label(self) -&gt; str:\n    \"\"\"\n    Returns relationship label.\n\n    Returns:\n        str: relationship_label\n    \"\"\"\n    return self.relationship_label\n</code></pre>"},{"location":"reference/source/graph-handling/#biocypher._create.BioCypherEdge.get_properties","title":"<code>get_properties()</code>","text":"<p>Returns all other relationship properties apart from primary ids and label as key-value pairs.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>properties</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_properties(self) -&gt; dict:\n    \"\"\"\n    Returns all other relationship properties apart from primary ids\n    and label as key-value pairs.\n\n    Returns:\n        dict: properties\n    \"\"\"\n    return self.properties\n</code></pre>"},{"location":"reference/source/graph-handling/#biocypher._create.BioCypherEdge.get_source_id","title":"<code>get_source_id()</code>","text":"<p>Returns primary node identifier of relationship source.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>source_id</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_source_id(self) -&gt; str:\n    \"\"\"\n    Returns primary node identifier of relationship source.\n\n    Returns:\n        str: source_id\n    \"\"\"\n    return self.source_id\n</code></pre>"},{"location":"reference/source/graph-handling/#biocypher._create.BioCypherEdge.get_target_id","title":"<code>get_target_id()</code>","text":"<p>Returns primary node identifier of relationship target.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>target_id</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_target_id(self) -&gt; str:\n    \"\"\"\n    Returns primary node identifier of relationship target.\n\n    Returns:\n        str: target_id\n    \"\"\"\n    return self.target_id\n</code></pre>"},{"location":"reference/source/graph-handling/#biocypher._create.BioCypherEdge.get_type","title":"<code>get_type()</code>","text":"<p>Returns relationship label.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>relationship_label</p> Source code in <code>biocypher/_create.py</code> <pre><code>def get_type(self) -&gt; str:\n    \"\"\"\n    Returns relationship label.\n\n    Returns:\n        str: relationship_label\n    \"\"\"\n    return self.relationship_label\n</code></pre>"},{"location":"reference/source/graph-handling/#biocypher-relasnode","title":"BioCypher RelAsNode","text":"<p>Class to represent relationships as nodes (with in- and outgoing edges) as a triplet of a BioCypherNode and two BioCypherEdges. Main usage in type checking (instances where the receiving function needs to check whether it receives a relationship as a single edge or as a triplet).</p> <p>Args:</p> <pre><code>node (BioCypherNode): node representing the relationship\n\nsource_edge (BioCypherEdge): edge representing the source of the\n    relationship\n\ntarget_edge (BioCypherEdge): edge representing the target of the\n    relationship\n</code></pre> Source code in <code>biocypher/_create.py</code> <pre><code>@dataclass(frozen=True)\nclass BioCypherRelAsNode:\n    \"\"\"\n    Class to represent relationships as nodes (with in- and outgoing\n    edges) as a triplet of a BioCypherNode and two BioCypherEdges. Main\n    usage in type checking (instances where the receiving function needs\n    to check whether it receives a relationship as a single edge or as\n    a triplet).\n\n    Args:\n\n        node (BioCypherNode): node representing the relationship\n\n        source_edge (BioCypherEdge): edge representing the source of the\n            relationship\n\n        target_edge (BioCypherEdge): edge representing the target of the\n            relationship\n\n    \"\"\"\n\n    node: BioCypherNode\n    source_edge: BioCypherEdge\n    target_edge: BioCypherEdge\n\n    def __post_init__(self):\n        if not isinstance(self.node, BioCypherNode):\n            raise TypeError(\n                f\"BioCypherRelAsNode.node must be a BioCypherNode, \" f\"not {type(self.node)}.\",\n            )\n\n        if not isinstance(self.source_edge, BioCypherEdge):\n            raise TypeError(\n                f\"BioCypherRelAsNode.source_edge must be a BioCypherEdge, \" f\"not {type(self.source_edge)}.\",\n            )\n\n        if not isinstance(self.target_edge, BioCypherEdge):\n            raise TypeError(\n                f\"BioCypherRelAsNode.target_edge must be a BioCypherEdge, \" f\"not {type(self.target_edge)}.\",\n            )\n\n    def get_node(self) -&gt; BioCypherNode:\n        return self.node\n\n    def get_source_edge(self) -&gt; BioCypherEdge:\n        return self.source_edge\n\n    def get_target_edge(self) -&gt; BioCypherEdge:\n        return self.target_edge\n</code></pre>"},{"location":"reference/source/logging/","title":"Logging","text":"<p>Access the module logger, create a new one if does not exist yet.</p> <p>Method providing central logger instance to main module. Is called only from main submodule, :mod:<code>biocypher.driver</code>. In child modules, the standard Python logging facility is called (using <code>logging.getLogger(__name__)</code>), automatically inheriting the handlers from the central logger.</p> <p>The file handler creates a log file named after the current date and time. Levels to output to file and console can be set here.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the logger instance.</p> <code>'biocypher'</code> <p>Returns:</p> Type Description <code>Logger</code> <p>An instance of the Python mod:<code>logging.Logger</code>.</p> Source code in <code>biocypher/_logger.py</code> <pre><code>def get_logger(name: str = \"biocypher\") -&gt; logging.Logger:\n    \"\"\"\n    Access the module logger, create a new one if does not exist yet.\n\n    Method providing central logger instance to main module. Is called\n    only from main submodule, :mod:`biocypher.driver`. In child modules,\n    the standard Python logging facility is called\n    (using ``logging.getLogger(__name__)``), automatically inheriting\n    the handlers from the central logger.\n\n    The file handler creates a log file named after the current date and\n    time. Levels to output to file and console can be set here.\n\n    Args:\n        name:\n            Name of the logger instance.\n\n    Returns:\n        An instance of the Python :py:mod:`logging.Logger`.\n    \"\"\"\n\n    if not logging.getLogger(name).hasHandlers():\n        # create logger\n        logger = logging.getLogger(name)\n        logger.setLevel(logging.DEBUG)\n        logger.propagate = True\n\n        # formatting\n        file_formatter = logging.Formatter(\n            \"%(asctime)s\\t%(levelname)s\\tmodule:%(module)s\\n%(message)s\",\n        )\n        stdout_formatter = logging.Formatter(\"%(levelname)s -- %(message)s\")\n\n        # file name and creation\n        now = datetime.now()\n        date_time = now.strftime(\"%Y%m%d-%H%M%S\")\n\n        log_to_disk = _config.config(\"biocypher\").get(\"log_to_disk\")\n\n        if log_to_disk:\n            logdir = _config.config(\"biocypher\").get(\"log_directory\") or \"biocypher-log\"\n            os.makedirs(logdir, exist_ok=True)\n            logfile = os.path.join(logdir, f\"biocypher-{date_time}.log\")\n\n            # file handler\n            file_handler = logging.FileHandler(logfile)\n\n            if _config.config(\"biocypher\").get(\"debug\"):\n                file_handler.setLevel(logging.DEBUG)\n            else:\n                file_handler.setLevel(logging.INFO)\n\n            file_handler.setFormatter(file_formatter)\n\n            logger.addHandler(file_handler)\n\n        # handlers\n        # stream handler\n        stdout_handler = logging.StreamHandler()\n        stdout_handler.setLevel(logging.INFO)\n        stdout_handler.setFormatter(stdout_formatter)\n\n        # add handlers\n        logger.addHandler(stdout_handler)\n\n        # startup message\n        logger.info(f\"This is BioCypher v{__version__}.\")\n        if log_to_disk:\n            logger.info(f\"Logging into `{logfile}`.\")\n        else:\n            logger.info(\"Logging into stdout.\")\n\n    return logging.getLogger(name)\n</code></pre>"},{"location":"reference/source/ontology/","title":"Ontology Handling","text":""},{"location":"reference/source/ontology/#ontology-base-class","title":"Ontology Base Class","text":"<p>A class that represents the ontological \"backbone\" of a KG.</p> <p>The ontology can be built from a single resource, or hybridised from a combination of resources, with one resource being the \"head\" ontology, while an arbitrary number of other resources can become \"tail\" ontologies at arbitrary fusion points inside the \"head\" ontology.</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>class Ontology:\n    \"\"\"A class that represents the ontological \"backbone\" of a KG.\n\n    The ontology can be built from a single resource, or hybridised from a\n    combination of resources, with one resource being the \"head\" ontology, while\n    an arbitrary number of other resources can become \"tail\" ontologies at\n    arbitrary fusion points inside the \"head\" ontology.\n    \"\"\"\n\n    def __init__(\n        self,\n        head_ontology: dict,\n        ontology_mapping: Optional[\"OntologyMapping\"] = None,\n        tail_ontologies: dict | None = None,\n    ):\n        \"\"\"Initialize the Ontology class.\n\n        Args:\n        ----\n            head_ontology (OntologyAdapter): The head ontology.\n\n            tail_ontologies (list): A list of OntologyAdapters that will be\n                added to the head ontology. Defaults to None.\n\n        \"\"\"\n        self._head_ontology_meta = head_ontology\n        self.mapping = ontology_mapping\n        self._tail_ontology_meta = tail_ontologies\n\n        self._tail_ontologies = None\n        self._nx_graph = None\n\n        # keep track of nodes that have been extended\n        self._extended_nodes = set()\n\n        self._main()\n\n    def _main(self) -&gt; None:\n        \"\"\"Instantiate the ontology.\n\n        Loads the ontologies, joins them, and returns the hybrid ontology.\n        Loads only the head ontology if nothing else is given. Adds user\n        extensions and properties from the mapping.\n        \"\"\"\n        self._load_ontologies()\n\n        if self._tail_ontologies:\n            for adapter in self._tail_ontologies.values():\n                head_join_node = self._get_head_join_node(adapter)\n                self._join_ontologies(adapter, head_join_node)\n        else:\n            self._nx_graph = self._head_ontology.get_nx_graph()\n\n        if self.mapping:\n            self._extend_ontology()\n\n            # experimental: add connections of disjoint classes to entity\n            # self._connect_biolink_classes()\n\n            self._add_properties()\n\n    def _load_ontologies(self) -&gt; None:\n        \"\"\"For each ontology, load the OntologyAdapter object.\n\n        Store it as an instance variable (head) or in an instance dictionary\n        (tail).\n        \"\"\"\n        logger.info(\"Loading ontologies...\")\n\n        self._head_ontology = OntologyAdapter(\n            ontology_file=self._head_ontology_meta[\"url\"],\n            root_label=self._head_ontology_meta[\"root_node\"],\n            ontology_file_format=self._head_ontology_meta.get(\"format\", None),\n            switch_label_and_id=self._head_ontology_meta.get(\"switch_label_and_id\", True),\n        )\n\n        if self._tail_ontology_meta:\n            self._tail_ontologies = {}\n            for key, value in self._tail_ontology_meta.items():\n                self._tail_ontologies[key] = OntologyAdapter(\n                    ontology_file=value[\"url\"],\n                    root_label=value[\"tail_join_node\"],\n                    head_join_node_label=value[\"head_join_node\"],\n                    ontology_file_format=value.get(\"format\", None),\n                    merge_nodes=value.get(\"merge_nodes\", True),\n                    switch_label_and_id=value.get(\"switch_label_and_id\", True),\n                )\n\n    def _get_head_join_node(self, adapter: OntologyAdapter) -&gt; str:\n        \"\"\"Try to find the head join node of the given ontology adapter.\n\n        Find the node in the head ontology that is the head join node. If the\n        join node is not found, the method will raise an error.\n\n        Args:\n        ----\n            adapter (OntologyAdapter): The ontology adapter of which to find the\n                join node in the head ontology.\n\n        Returns:\n        -------\n            str: The head join node in the head ontology.\n\n        Raises:\n        ------\n            ValueError: If the head join node is not found in the head ontology.\n\n        \"\"\"\n        head_join_node = None\n        user_defined_head_join_node_label = adapter.get_head_join_node()\n        head_join_node_label_in_bc_format = to_lower_sentence_case(user_defined_head_join_node_label.replace(\"_\", \" \"))\n\n        if self._head_ontology._switch_label_and_id:\n            head_join_node = head_join_node_label_in_bc_format\n        elif not self._head_ontology._switch_label_and_id:\n            for node_id, data in self._head_ontology.get_nx_graph().nodes(data=True):\n                if \"label\" in data and data[\"label\"] == head_join_node_label_in_bc_format:\n                    head_join_node = node_id\n                    break\n\n        if head_join_node not in self._head_ontology.get_nx_graph().nodes:\n            head_ontology = self._head_ontology._rdf_to_nx(\n                self._head_ontology.get_rdf_graph(),\n                self._head_ontology._root_label,\n                self._head_ontology._switch_label_and_id,\n                rename_nodes=False,\n            )\n            msg = (\n                f\"Head join node '{head_join_node}' not found in head ontology. \"\n                f\"The head ontology contains the following nodes: {head_ontology.nodes}.\"\n            )\n            logger.error(msg)\n            raise ValueError(msg)\n        return head_join_node\n\n    def _join_ontologies(self, adapter: OntologyAdapter, head_join_node) -&gt; None:\n        \"\"\"Join the present ontologies.\n\n        Join two ontologies by adding the tail ontology as a subgraph to the\n        head ontology at the specified join nodes.\n\n        Args:\n        ----\n            adapter (OntologyAdapter): The ontology adapter of the tail ontology\n                to be added to the head ontology.\n\n        \"\"\"\n        if not self._nx_graph:\n            self._nx_graph = self._head_ontology.get_nx_graph().copy()\n\n        tail_join_node = adapter.get_root_node()\n        tail_ontology = adapter.get_nx_graph()\n\n        # subtree of tail ontology at join node\n        tail_ontology_subtree = nx.dfs_tree(tail_ontology.reverse(), tail_join_node).reverse()\n\n        # transfer node attributes from tail ontology to subtree\n        for node in tail_ontology_subtree.nodes:\n            tail_ontology_subtree.nodes[node].update(tail_ontology.nodes[node])\n\n        # if merge_nodes is False, create parent of tail join node from head\n        # join node\n        if not adapter._merge_nodes:\n            # add head join node from head ontology to tail ontology subtree\n            # as parent of tail join node\n            tail_ontology_subtree.add_node(\n                head_join_node,\n                **self._head_ontology.get_nx_graph().nodes[head_join_node],\n            )\n            tail_ontology_subtree.add_edge(tail_join_node, head_join_node)\n\n        # else rename tail join node to match head join node if necessary\n        elif tail_join_node != head_join_node:\n            tail_ontology_subtree = nx.relabel_nodes(tail_ontology_subtree, {tail_join_node: head_join_node})\n\n        # combine head ontology and tail subtree\n        self._nx_graph = nx.compose(self._nx_graph, tail_ontology_subtree)\n\n    def _extend_ontology(self) -&gt; None:\n        \"\"\"Add the user extensions to the ontology.\n\n        Tries to find the parent in the ontology, adds it if necessary, and adds\n        the child and a directed edge from child to parent. Can handle multiple\n        parents.\n        \"\"\"\n        if not self._nx_graph:\n            self._nx_graph = self._head_ontology.get_nx_graph().copy()\n\n        for key, value in self.mapping.extended_schema.items():\n            # If this class is either a root or a synonym.\n            if not value.get(\"is_a\"):\n                # If it is a synonym.\n                if self._nx_graph.has_node(value.get(\"synonym_for\")):\n                    continue\n\n                # If this class is in the schema, but not in the loaded vocabulary.\n                if not self._nx_graph.has_node(key):\n                    msg = (\n                        f\"Node {key} not found in ontology, but also has no inheritance definition. Please check your \"\n                        \"schema for spelling errors, first letter not in lower case, use of underscores, a missing \"\n                        \"`is_a` definition (SubClassOf a root node), or missing labels in class or super-classes.\"\n                    )\n                    logger.error(msg)\n                    raise ValueError(msg)\n\n                # It is a root and it is in the loaded vocabulary.\n                continue\n\n            # It is not a root.\n            parents = to_list(value.get(\"is_a\"))\n            child = key\n\n            while parents:\n                parent = parents.pop(0)\n\n                if parent not in self._nx_graph.nodes:\n                    self._nx_graph.add_node(parent)\n                    self._nx_graph.nodes[parent][\"label\"] = sentencecase_to_pascalcase(parent)\n\n                    # mark parent as user extension\n                    self._nx_graph.nodes[parent][\"user_extension\"] = True\n                    self._extended_nodes.add(parent)\n\n                if child not in self._nx_graph.nodes:\n                    self._nx_graph.add_node(child)\n                    self._nx_graph.nodes[child][\"label\"] = sentencecase_to_pascalcase(child)\n\n                    # mark child as user extension\n                    self._nx_graph.nodes[child][\"user_extension\"] = True\n                    self._extended_nodes.add(child)\n\n                self._nx_graph.add_edge(child, parent)\n\n                child = parent\n\n    def _connect_biolink_classes(self) -&gt; None:\n        \"\"\"Experimental: Adds edges from disjoint classes to the entity node.\"\"\"\n        if not self._nx_graph:\n            self._nx_graph = self._head_ontology.get_nx_graph().copy()\n\n        if \"entity\" not in self._nx_graph.nodes:\n            return\n\n        # biolink classes that are disjoint from entity\n        disjoint_classes = [\n            \"frequency qualifier mixin\",\n            \"chemical entity to entity association mixin\",\n            \"ontology class\",\n            \"relationship quantifier\",\n            \"physical essence or occurrent\",\n            \"gene or gene product\",\n            \"subject of investigation\",\n        ]\n\n        for node in disjoint_classes:\n            if not self._nx_graph.nodes.get(node):\n                self._nx_graph.add_node(node)\n                self._nx_graph.nodes[node][\"label\"] = sentencecase_to_pascalcase(node)\n\n            self._nx_graph.add_edge(node, \"entity\")\n\n    def _add_properties(self) -&gt; None:\n        \"\"\"Add properties to the ontology.\n\n        For each entity in the mapping, update the ontology with the properties\n        specified in the mapping. Updates synonym information in the graph,\n        setting the synonym as the primary node label.\n        \"\"\"\n        for key, value in self.mapping.extended_schema.items():\n            if key in self._nx_graph.nodes:\n                self._nx_graph.nodes[key].update(value)\n\n            if value.get(\"synonym_for\"):\n                # change node label to synonym\n                if value[\"synonym_for\"] not in self._nx_graph.nodes:\n                    msg = f\"Node {value['synonym_for']} not found in ontology.\"\n                    logger.error(msg)\n                    raise ValueError(msg)\n\n                self._nx_graph = nx.relabel_nodes(self._nx_graph, {value[\"synonym_for\"]: key})\n\n    def get_ancestors(self, node_label: str) -&gt; list:\n        \"\"\"Get the ancestors of a node in the ontology.\n\n        Args:\n        ----\n            node_label (str): The label of the node in the ontology.\n\n        Returns:\n        -------\n            list: A list of the ancestors of the node.\n\n        \"\"\"\n        return nx.dfs_tree(self._nx_graph, node_label)\n\n    def show_ontology_structure(self, to_disk: str = None, full: bool = False):\n        \"\"\"Show the ontology structure using treelib or write to GRAPHML file.\n\n        Args:\n        ----\n            to_disk (str): If specified, the ontology structure will be saved\n                to disk as a GRAPHML file at the location (directory) specified\n                by the `to_disk` string, to be opened in your favourite graph\n                visualisation tool.\n\n            full (bool): If True, the full ontology structure will be shown,\n                including all nodes and edges. If False, only the nodes and\n                edges that are relevant to the extended schema will be shown.\n\n        \"\"\"\n        if not full and not self.mapping.extended_schema:\n            msg = (\n                \"You are attempting to visualise a subset of the loaded\"\n                \"ontology, but have not provided a schema configuration. \"\n                \"To display a partial ontology graph, please provide a schema \"\n                \"configuration file; to visualise the full graph, please use \"\n                \"the parameter `full = True`.\",\n            )\n            logger.error(msg)\n            raise ValueError(msg)\n\n        if not self._nx_graph:\n            msg = \"Ontology not loaded.\"\n            logger.error(msg)\n            raise ValueError(msg)\n\n        if not self._tail_ontologies:\n            msg = f\"Showing ontology structure based on {self._head_ontology._ontology_file}\"\n\n        else:\n            msg = f\"Showing ontology structure based on {len(self._tail_ontology_meta) + 1} ontologies: \"\n\n        logger.info(msg)\n\n        if not full:\n            # set of leaves and their intermediate parents up to the root\n            filter_nodes = set(self.mapping.extended_schema.keys())\n\n            for node in self.mapping.extended_schema.keys():\n                filter_nodes.update(self.get_ancestors(node).nodes)\n\n            # filter graph\n            G = self._nx_graph.subgraph(filter_nodes)\n\n        else:\n            G = self._nx_graph\n\n        if not to_disk:\n            # create tree\n            tree = create_tree_visualisation(G)\n\n            # add synonym information\n            for node in self.mapping.extended_schema:\n                if not isinstance(self.mapping.extended_schema[node], dict):\n                    continue\n                if self.mapping.extended_schema[node].get(\"synonym_for\"):\n                    tree.nodes[node].tag = f\"{node} = {self.mapping.extended_schema[node].get('synonym_for')}\"\n\n            logger.info(f\"\\n{tree}\")\n\n            return tree\n\n        else:\n            # convert lists/dicts to strings for vis only\n            for node in G.nodes:\n                # rename node and use former id as label\n                label = G.nodes[node].get(\"label\")\n\n                if not label:\n                    label = node\n\n                G = nx.relabel_nodes(G, {node: label})\n                G.nodes[label][\"label\"] = node\n\n                for attrib in G.nodes[label]:\n                    if type(G.nodes[label][attrib]) in [list, dict]:\n                        G.nodes[label][attrib] = str(G.nodes[label][attrib])\n\n            path = os.path.join(to_disk, \"ontology_structure.graphml\")\n\n            logger.info(f\"Writing ontology structure to {path}.\")\n\n            nx.write_graphml(G, path)\n\n            return True\n\n    def get_dict(self) -&gt; dict:\n        \"\"\"Return a dictionary representation of the ontology.\n\n        The dictionary is compatible with a BioCypher node for compatibility\n        with the Neo4j driver.\n        \"\"\"\n        d = {\n            \"node_id\": self._get_current_id(),\n            \"node_label\": \"BioCypher\",\n            \"properties\": {\n                \"schema\": \"self.ontology_mapping.extended_schema\",\n            },\n        }\n\n        return d\n\n    def _get_current_id(self):\n        \"\"\"Instantiate a version ID for the current session.\n\n        For now does simple versioning using datetime.\n\n        Can later implement incremental versioning, versioning from\n        config file, or manual specification via argument.\n        \"\"\"\n        now = datetime.now()\n        return now.strftime(\"v%Y%m%d-%H%M%S\")\n\n    def get_rdf_graph(self):\n        \"\"\"Return the merged RDF graph.\n\n        Return the merged graph of all loaded ontologies (head and tails).\n        \"\"\"\n        graph = self._head_ontology.get_rdf_graph()\n        if self._tail_ontologies:\n            for key, onto in self._tail_ontologies.items():\n                assert type(onto) == OntologyAdapter\n                # RDFlib uses the + operator for merging.\n                graph += onto.get_rdf_graph()\n        return graph\n</code></pre>"},{"location":"reference/source/ontology/#biocypher._ontology.Ontology.__init__","title":"<code>__init__(head_ontology, ontology_mapping=None, tail_ontologies=None)</code>","text":"<p>Initialize the Ontology class.</p> <pre><code>head_ontology (OntologyAdapter): The head ontology.\n\ntail_ontologies (list): A list of OntologyAdapters that will be\n    added to the head ontology. Defaults to None.\n</code></pre> Source code in <code>biocypher/_ontology.py</code> <pre><code>def __init__(\n    self,\n    head_ontology: dict,\n    ontology_mapping: Optional[\"OntologyMapping\"] = None,\n    tail_ontologies: dict | None = None,\n):\n    \"\"\"Initialize the Ontology class.\n\n    Args:\n    ----\n        head_ontology (OntologyAdapter): The head ontology.\n\n        tail_ontologies (list): A list of OntologyAdapters that will be\n            added to the head ontology. Defaults to None.\n\n    \"\"\"\n    self._head_ontology_meta = head_ontology\n    self.mapping = ontology_mapping\n    self._tail_ontology_meta = tail_ontologies\n\n    self._tail_ontologies = None\n    self._nx_graph = None\n\n    # keep track of nodes that have been extended\n    self._extended_nodes = set()\n\n    self._main()\n</code></pre>"},{"location":"reference/source/ontology/#biocypher._ontology.Ontology.get_ancestors","title":"<code>get_ancestors(node_label)</code>","text":"<p>Get the ancestors of a node in the ontology.</p> <pre><code>node_label (str): The label of the node in the ontology.\n</code></pre> <pre><code>list: A list of the ancestors of the node.\n</code></pre> Source code in <code>biocypher/_ontology.py</code> <pre><code>def get_ancestors(self, node_label: str) -&gt; list:\n    \"\"\"Get the ancestors of a node in the ontology.\n\n    Args:\n    ----\n        node_label (str): The label of the node in the ontology.\n\n    Returns:\n    -------\n        list: A list of the ancestors of the node.\n\n    \"\"\"\n    return nx.dfs_tree(self._nx_graph, node_label)\n</code></pre>"},{"location":"reference/source/ontology/#biocypher._ontology.Ontology.get_dict","title":"<code>get_dict()</code>","text":"<p>Return a dictionary representation of the ontology.</p> <p>The dictionary is compatible with a BioCypher node for compatibility with the Neo4j driver.</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>def get_dict(self) -&gt; dict:\n    \"\"\"Return a dictionary representation of the ontology.\n\n    The dictionary is compatible with a BioCypher node for compatibility\n    with the Neo4j driver.\n    \"\"\"\n    d = {\n        \"node_id\": self._get_current_id(),\n        \"node_label\": \"BioCypher\",\n        \"properties\": {\n            \"schema\": \"self.ontology_mapping.extended_schema\",\n        },\n    }\n\n    return d\n</code></pre>"},{"location":"reference/source/ontology/#biocypher._ontology.Ontology.get_rdf_graph","title":"<code>get_rdf_graph()</code>","text":"<p>Return the merged RDF graph.</p> <p>Return the merged graph of all loaded ontologies (head and tails).</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>def get_rdf_graph(self):\n    \"\"\"Return the merged RDF graph.\n\n    Return the merged graph of all loaded ontologies (head and tails).\n    \"\"\"\n    graph = self._head_ontology.get_rdf_graph()\n    if self._tail_ontologies:\n        for key, onto in self._tail_ontologies.items():\n            assert type(onto) == OntologyAdapter\n            # RDFlib uses the + operator for merging.\n            graph += onto.get_rdf_graph()\n    return graph\n</code></pre>"},{"location":"reference/source/ontology/#biocypher._ontology.Ontology.show_ontology_structure","title":"<code>show_ontology_structure(to_disk=None, full=False)</code>","text":"<p>Show the ontology structure using treelib or write to GRAPHML file.</p> <pre><code>to_disk (str): If specified, the ontology structure will be saved\n    to disk as a GRAPHML file at the location (directory) specified\n    by the `to_disk` string, to be opened in your favourite graph\n    visualisation tool.\n\nfull (bool): If True, the full ontology structure will be shown,\n    including all nodes and edges. If False, only the nodes and\n    edges that are relevant to the extended schema will be shown.\n</code></pre> Source code in <code>biocypher/_ontology.py</code> <pre><code>def show_ontology_structure(self, to_disk: str = None, full: bool = False):\n    \"\"\"Show the ontology structure using treelib or write to GRAPHML file.\n\n    Args:\n    ----\n        to_disk (str): If specified, the ontology structure will be saved\n            to disk as a GRAPHML file at the location (directory) specified\n            by the `to_disk` string, to be opened in your favourite graph\n            visualisation tool.\n\n        full (bool): If True, the full ontology structure will be shown,\n            including all nodes and edges. If False, only the nodes and\n            edges that are relevant to the extended schema will be shown.\n\n    \"\"\"\n    if not full and not self.mapping.extended_schema:\n        msg = (\n            \"You are attempting to visualise a subset of the loaded\"\n            \"ontology, but have not provided a schema configuration. \"\n            \"To display a partial ontology graph, please provide a schema \"\n            \"configuration file; to visualise the full graph, please use \"\n            \"the parameter `full = True`.\",\n        )\n        logger.error(msg)\n        raise ValueError(msg)\n\n    if not self._nx_graph:\n        msg = \"Ontology not loaded.\"\n        logger.error(msg)\n        raise ValueError(msg)\n\n    if not self._tail_ontologies:\n        msg = f\"Showing ontology structure based on {self._head_ontology._ontology_file}\"\n\n    else:\n        msg = f\"Showing ontology structure based on {len(self._tail_ontology_meta) + 1} ontologies: \"\n\n    logger.info(msg)\n\n    if not full:\n        # set of leaves and their intermediate parents up to the root\n        filter_nodes = set(self.mapping.extended_schema.keys())\n\n        for node in self.mapping.extended_schema.keys():\n            filter_nodes.update(self.get_ancestors(node).nodes)\n\n        # filter graph\n        G = self._nx_graph.subgraph(filter_nodes)\n\n    else:\n        G = self._nx_graph\n\n    if not to_disk:\n        # create tree\n        tree = create_tree_visualisation(G)\n\n        # add synonym information\n        for node in self.mapping.extended_schema:\n            if not isinstance(self.mapping.extended_schema[node], dict):\n                continue\n            if self.mapping.extended_schema[node].get(\"synonym_for\"):\n                tree.nodes[node].tag = f\"{node} = {self.mapping.extended_schema[node].get('synonym_for')}\"\n\n        logger.info(f\"\\n{tree}\")\n\n        return tree\n\n    else:\n        # convert lists/dicts to strings for vis only\n        for node in G.nodes:\n            # rename node and use former id as label\n            label = G.nodes[node].get(\"label\")\n\n            if not label:\n                label = node\n\n            G = nx.relabel_nodes(G, {node: label})\n            G.nodes[label][\"label\"] = node\n\n            for attrib in G.nodes[label]:\n                if type(G.nodes[label][attrib]) in [list, dict]:\n                    G.nodes[label][attrib] = str(G.nodes[label][attrib])\n\n        path = os.path.join(to_disk, \"ontology_structure.graphml\")\n\n        logger.info(f\"Writing ontology structure to {path}.\")\n\n        nx.write_graphml(G, path)\n\n        return True\n</code></pre>"},{"location":"reference/source/ontology/#ontology-adapter","title":"Ontology Adapter","text":"<p>Class that represents an ontology to be used in the Biocypher framework.</p> <p>Can read from a variety of formats, including OWL, OBO, and RDF/XML. The ontology is represented by a networkx.DiGraph object; an RDFlib graph is also kept. By default, the DiGraph reverses the label and identifier of the nodes, such that the node name in the graph is the human-readable label. The edges are oriented from child to parent. Labels are formatted in lower sentence case and underscores are replaced by spaces. Identifiers are taken as defined and the prefixes are removed by default.</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>class OntologyAdapter:\n    \"\"\"Class that represents an ontology to be used in the Biocypher framework.\n\n    Can read from a variety of formats, including OWL, OBO, and RDF/XML. The\n    ontology is represented by a networkx.DiGraph object; an RDFlib graph is\n    also kept. By default, the DiGraph reverses the label and identifier of the\n    nodes, such that the node name in the graph is the human-readable label. The\n    edges are oriented from child to parent. Labels are formatted in lower\n    sentence case and underscores are replaced by spaces. Identifiers are taken\n    as defined and the prefixes are removed by default.\n    \"\"\"\n\n    def __init__(\n        self,\n        ontology_file: str,\n        root_label: str,\n        ontology_file_format: str | None = None,\n        head_join_node_label: str | None = None,\n        merge_nodes: bool | None = True,\n        switch_label_and_id: bool = True,\n        remove_prefixes: bool = True,\n    ):\n        \"\"\"Initialize the OntologyAdapter class.\n\n        Args:\n        ----\n            ontology_file (str): Path to the ontology file. Can be local or\n                remote.\n\n            root_label (str): The label of the root node in the ontology. In\n                case of a tail ontology, this is the tail join node.\n\n            ontology_file_format (str): The format of the ontology file (e.g. \"application/rdf+xml\")\n                If format is not passed, it is determined automatically.\n\n            head_join_node_label (str): Optional variable to store the label of the\n                node in the head ontology that should be used to join to the\n                root node of the tail ontology. Defaults to None.\n\n            merge_nodes (bool): If True, head and tail join nodes will be\n                merged, using the label of the head join node. If False, the\n                tail join node will be attached as a child of the head join\n                node.\n\n            switch_label_and_id (bool): If True, the node names in the graph will be\n                the human-readable labels. If False, the node names will be the\n                identifiers. Defaults to True.\n\n            remove_prefixes (bool): If True, the prefixes of the identifiers will\n                be removed. Defaults to True.\n\n        \"\"\"\n        logger.info(f\"Instantiating OntologyAdapter class for {ontology_file}.\")\n\n        self._ontology_file = ontology_file\n        self._root_label = root_label\n        self._format = ontology_file_format\n        self._merge_nodes = merge_nodes\n        self._head_join_node = head_join_node_label\n        self._switch_label_and_id = switch_label_and_id\n        self._remove_prefixes = remove_prefixes\n\n        self._rdf_graph = self._load_rdf_graph(ontology_file)\n\n        self._nx_graph = self._rdf_to_nx(self._rdf_graph, root_label, switch_label_and_id)\n\n    def _rdf_to_nx(\n        self,\n        _rdf_graph: rdflib.Graph,\n        root_label: str,\n        switch_label_and_id: bool,\n        rename_nodes: bool = True,\n    ) -&gt; nx.DiGraph:\n        one_to_one_triples, one_to_many_dict = self._get_relevant_rdf_triples(_rdf_graph)\n        nx_graph = self._convert_to_nx(one_to_one_triples, one_to_many_dict)\n        nx_graph = self._add_labels_to_nodes(nx_graph, switch_label_and_id)\n        nx_graph = self._change_nodes_to_biocypher_format(nx_graph, switch_label_and_id, rename_nodes)\n        nx_graph = self._get_all_ancestors(nx_graph, root_label, switch_label_and_id, rename_nodes)\n        return nx.DiGraph(nx_graph)\n\n    def _get_relevant_rdf_triples(self, g: rdflib.Graph) -&gt; tuple:\n        one_to_one_inheritance_graph = self._get_one_to_one_inheritance_triples(g)\n        intersection = self._get_multiple_inheritance_dict(g)\n        return one_to_one_inheritance_graph, intersection\n\n    def _get_one_to_one_inheritance_triples(self, g: rdflib.Graph) -&gt; rdflib.Graph:\n        \"\"\"Get the one to one inheritance triples from the RDF graph.\n\n        Args:\n        ----\n            g (rdflib.Graph): The RDF graph\n\n        Returns:\n        -------\n            rdflib.Graph: The one to one inheritance graph\n\n        \"\"\"\n        one_to_one_inheritance_graph = Graph()\n        # for s, p, o in g.triples((None, rdflib.RDFS.subClassOf, None)):\n        for s, p, o in chain(\n            g.triples((None, rdflib.RDFS.subClassOf, None)),  # Node classes\n            g.triples((None, rdflib.RDF.type, rdflib.RDFS.Class)),  # Root classes\n            g.triples((None, rdflib.RDFS.subPropertyOf, None)),  # OWL \"edges\" classes\n            g.triples((None, rdflib.RDF.type, rdflib.OWL.ObjectProperty)),  # OWL \"edges\" root classes\n        ):\n            if self.has_label(s, g):\n                one_to_one_inheritance_graph.add((s, p, o))\n        return one_to_one_inheritance_graph\n\n    def _get_multiple_inheritance_dict(self, g: rdflib.Graph) -&gt; dict:\n        \"\"\"Get the multiple inheritance dictionary from the RDF graph.\n\n        Args:\n        ----\n            g (rdflib.Graph): The RDF graph\n\n        Returns:\n        -------\n            dict: The multiple inheritance dictionary\n\n        \"\"\"\n        multiple_inheritance = g.triples((None, rdflib.OWL.intersectionOf, None))\n        intersection = {}\n        for (\n            node,\n            has_multiple_parents,\n            first_node_of_intersection_list,\n        ) in multiple_inheritance:\n            parents = self._retrieve_rdf_linked_list(first_node_of_intersection_list)\n            child_name = None\n            for s_, _, _ in chain(\n                g.triples((None, rdflib.RDFS.subClassOf, node)),\n                g.triples((None, rdflib.RDFS.subPropertyOf, node)),\n            ):\n                child_name = s_\n\n            # Handle Snomed CT post coordinated expressions\n            if not child_name:\n                for s_, _, _ in g.triples((None, rdflib.OWL.equivalentClass, node)):\n                    child_name = s_\n\n            if child_name:\n                intersection[node] = {\n                    \"child_name\": child_name,\n                    \"parent_node_names\": parents,\n                }\n        return intersection\n\n    def has_label(self, node: rdflib.URIRef, g: rdflib.Graph) -&gt; bool:\n        \"\"\"Check if the node has a label in the graph.\n\n        Args:\n        ----\n            node (rdflib.URIRef): The node to check\n            g (rdflib.Graph): The graph to check in\n        Returns:\n            bool: True if the node has a label, False otherwise\n\n        \"\"\"\n        return (node, rdflib.RDFS.label, None) in g\n\n    def _retrieve_rdf_linked_list(self, subject: rdflib.URIRef) -&gt; list:\n        \"\"\"Recursively retrieve a linked list from RDF.\n\n        Example RDF list with the items [item1, item2]:\n        list_node - first -&gt; item1\n        list_node - rest -&gt; list_node2\n        list_node2 - first -&gt; item2\n        list_node2 - rest -&gt; nil\n\n        Args:\n        ----\n            subject (rdflib.URIRef): One list_node of the RDF list\n\n        Returns:\n        -------\n            list: The items of the RDF list\n\n        \"\"\"\n        g = self._rdf_graph\n        rdf_list = []\n        for s, p, o in g.triples((subject, rdflib.RDF.first, None)):\n            rdf_list.append(o)\n        for s, p, o in g.triples((subject, rdflib.RDF.rest, None)):\n            if o != rdflib.RDF.nil:\n                rdf_list.extend(self._retrieve_rdf_linked_list(o))\n        return rdf_list\n\n    def _convert_to_nx(self, one_to_one: rdflib.Graph, one_to_many: dict) -&gt; nx.DiGraph:\n        \"\"\"Convert the one to one and one to many inheritance graphs to networkx.\n\n        Args:\n        ----\n            one_to_one (rdflib.Graph): The one to one inheritance graph\n            one_to_many (dict): The one to many inheritance dictionary\n\n        Returns:\n        -------\n            nx.DiGraph: The networkx graph\n\n        \"\"\"\n        nx_graph = rdflib_to_networkx_digraph(one_to_one, edge_attrs=lambda s, p, o: {}, calc_weights=False)\n        for key, value in one_to_many.items():\n            nx_graph.add_edges_from([(value[\"child_name\"], parent) for parent in value[\"parent_node_names\"]])\n            if key in nx_graph.nodes:\n                nx_graph.remove_node(key)\n        return nx_graph\n\n    def _add_labels_to_nodes(self, nx_graph: nx.DiGraph, switch_label_and_id: bool) -&gt; nx.DiGraph:\n        \"\"\"Add labels to the nodes in the networkx graph.\n\n        Args:\n        ----\n            nx_graph (nx.DiGraph): The networkx graph\n            switch_label_and_id (bool): If True, id and label are switched\n\n        Returns:\n        -------\n            nx.DiGraph: The networkx graph with labels\n\n        \"\"\"\n        for node in list(nx_graph.nodes):\n            nx_id, nx_label = self._get_nx_id_and_label(node, switch_label_and_id)\n            if nx_id == \"none\":\n                # remove node if it has no id\n                nx_graph.remove_node(node)\n                continue\n\n            nx_graph.nodes[node][\"label\"] = nx_label\n        return nx_graph\n\n    def _change_nodes_to_biocypher_format(\n        self,\n        nx_graph: nx.DiGraph,\n        switch_label_and_id: bool,\n        rename_nodes: bool = True,\n    ) -&gt; nx.DiGraph:\n        \"\"\"Change the nodes in the networkx graph to BioCypher format.\n\n        This involves:\n            - removing the prefix of the identifier\n            - switching the id and label if requested\n            - adapting the labels (replace _ with space and convert to lower\n                sentence case)\n        Args:\n        ----\n            nx_graph (nx.DiGraph): The networkx graph\n            switch_label_and_id (bool): If True, id and label are switched\n            rename_nodes (bool): If True, the nodes are renamed\n\n        Returns:\n        -------\n            nx.DiGraph: The networkx ontology graph in BioCypher format\n\n        \"\"\"\n        mapping = {\n            node: self._get_nx_id_and_label(node, switch_label_and_id, rename_nodes)[0] for node in nx_graph.nodes\n        }\n        renamed = nx.relabel_nodes(nx_graph, mapping, copy=False)\n        return renamed\n\n    def _get_all_ancestors(\n        self,\n        renamed: nx.DiGraph,\n        root_label: str,\n        switch_label_and_id: bool,\n        rename_nodes: bool = True,\n    ) -&gt; nx.DiGraph:\n        \"\"\"Get all ancestors of the root node in the networkx graph.\n\n        Args:\n        ----\n            renamed (nx.DiGraph): The renamed networkx graph\n            root_label (str): The label of the root node in the ontology\n            switch_label_and_id (bool): If True, id and label are switched\n            rename_nodes (bool): If True, the nodes are renamed\n\n        Returns:\n        -------\n            nx.DiGraph: The filtered networkx graph\n\n        \"\"\"\n        root = self._get_nx_id_and_label(\n            self._find_root_label(self._rdf_graph, root_label),\n            switch_label_and_id,\n            rename_nodes,\n        )[0]\n        ancestors = nx.ancestors(renamed, root)\n        ancestors.add(root)\n        filtered_graph = renamed.subgraph(ancestors)\n        return filtered_graph\n\n    def _get_nx_id_and_label(self, node, switch_id_and_label: bool, rename_nodes: bool = True) -&gt; tuple[str, str]:\n        \"\"\"Rename node id and label for nx graph.\n\n        Args:\n        ----\n            node (str): The node to rename\n            switch_id_and_label (bool): If True, switch id and label\n\n        Returns:\n        -------\n            tuple[str, str]: The renamed node id and label\n\n        \"\"\"\n        node_id_str = self._remove_prefix(str(node))\n        node_label_str = str(self._rdf_graph.value(node, rdflib.RDFS.label))\n        if rename_nodes:\n            node_label_str = node_label_str.replace(\"_\", \" \")\n            node_label_str = to_lower_sentence_case(node_label_str)\n        nx_id = node_label_str if switch_id_and_label else node_id_str\n        nx_label = node_id_str if switch_id_and_label else node_label_str\n        return nx_id, nx_label\n\n    def _find_root_label(self, g, root_label):\n        # Loop through all labels in the ontology\n        for label_subject, _, label_in_ontology in g.triples((None, rdflib.RDFS.label, None)):\n            # If the label is the root label, set the root node to the label's subject\n            if str(label_in_ontology) == root_label:\n                root = label_subject\n                break\n        else:\n            labels_in_ontology = []\n            for label_subject, _, label_in_ontology in g.triples((None, rdflib.RDFS.label, None)):\n                labels_in_ontology.append(str(label_in_ontology))\n            msg = (\n                f\"Could not find root node with label '{root_label}'. \"\n                f\"The ontology contains the following labels: {labels_in_ontology}\"\n            )\n            logger.error(msg)\n            raise ValueError(msg)\n        return root\n\n    def _remove_prefix(self, uri: str) -&gt; str:\n        \"\"\"Remove the prefix of a URI.\n\n        URIs can contain either \"#\" or \"/\" as a separator between the prefix\n        and the local name. The prefix is everything before the last separator.\n\n        Args:\n        ----\n            uri (str): The URI to remove the prefix from\n\n        Returns:\n        -------\n            str: The URI without the prefix\n\n        \"\"\"\n        if self._remove_prefixes:\n            return uri.rsplit(\"#\", 1)[-1].rsplit(\"/\", 1)[-1]\n        else:\n            return uri\n\n    def _load_rdf_graph(self, ontology_file):\n        \"\"\"Load the ontology into an RDFlib graph.\n\n        The ontology file can be in OWL, OBO, or RDF/XML format.\n\n        Args:\n        ----\n            ontology_file (str): The path to the ontology file\n\n        Returns:\n        -------\n            rdflib.Graph: The RDFlib graph\n\n        \"\"\"\n        g = rdflib.Graph()\n        g.parse(ontology_file, format=self._get_format(ontology_file))\n        return g\n\n    def _get_format(self, ontology_file):\n        \"\"\"Get the format of the ontology file.\"\"\"\n        if self._format:\n            if self._format == \"owl\":\n                return \"application/rdf+xml\"\n            elif self._format == \"obo\":\n                raise NotImplementedError(\"OBO format not yet supported\")\n            elif self._format == \"rdf\":\n                return \"application/rdf+xml\"\n            elif self._format == \"ttl\":\n                return self._format\n            else:\n                msg = f\"Could not determine format of ontology file {ontology_file}\"\n                logger.error(msg)\n                raise ValueError(msg)\n\n        if ontology_file.endswith(\".owl\"):\n            return \"application/rdf+xml\"\n        elif ontology_file.endswith(\".obo\"):\n            msg = \"OBO format not yet supported\"\n            logger.error(msg)\n            raise NotImplementedError(msg)\n        elif ontology_file.endswith(\".rdf\"):\n            return \"application/rdf+xml\"\n        elif ontology_file.endswith(\".ttl\"):\n            return \"ttl\"\n        else:\n            msg = f\"Could not determine format of ontology file {ontology_file}\"\n            logger.error(msg)\n            raise ValueError(msg)\n\n    def get_nx_graph(self):\n        \"\"\"Get the networkx graph representing the ontology.\"\"\"\n        return self._nx_graph\n\n    def get_rdf_graph(self):\n        \"\"\"Get the RDFlib graph representing the ontology.\"\"\"\n        return self._rdf_graph\n\n    def get_root_node(self):\n        \"\"\"Get root node in the ontology.\n\n        Returns\n        -------\n            root_node: If _switch_label_and_id is True, the root node label is\n                returned, otherwise the root node id is returned.\n\n        \"\"\"\n        root_node = None\n        root_label = self._root_label.replace(\"_\", \" \")\n\n        if self._switch_label_and_id:\n            root_node = to_lower_sentence_case(root_label)\n        elif not self._switch_label_and_id:\n            for node, data in self.get_nx_graph().nodes(data=True):\n                if \"label\" in data and data[\"label\"] == to_lower_sentence_case(root_label):\n                    root_node = node\n                    break\n\n        return root_node\n\n    def get_ancestors(self, node_label):\n        \"\"\"Get the ancestors of a node in the ontology.\"\"\"\n        return nx.dfs_preorder_nodes(self._nx_graph, node_label)\n\n    def get_head_join_node(self):\n        \"\"\"Get the head join node of the ontology.\"\"\"\n        return self._head_join_node\n</code></pre>"},{"location":"reference/source/ontology/#biocypher._ontology.OntologyAdapter.__init__","title":"<code>__init__(ontology_file, root_label, ontology_file_format=None, head_join_node_label=None, merge_nodes=True, switch_label_and_id=True, remove_prefixes=True)</code>","text":"<p>Initialize the OntologyAdapter class.</p> <pre><code>ontology_file (str): Path to the ontology file. Can be local or\n    remote.\n\nroot_label (str): The label of the root node in the ontology. In\n    case of a tail ontology, this is the tail join node.\n\nontology_file_format (str): The format of the ontology file (e.g. \"application/rdf+xml\")\n    If format is not passed, it is determined automatically.\n\nhead_join_node_label (str): Optional variable to store the label of the\n    node in the head ontology that should be used to join to the\n    root node of the tail ontology. Defaults to None.\n\nmerge_nodes (bool): If True, head and tail join nodes will be\n    merged, using the label of the head join node. If False, the\n    tail join node will be attached as a child of the head join\n    node.\n\nswitch_label_and_id (bool): If True, the node names in the graph will be\n    the human-readable labels. If False, the node names will be the\n    identifiers. Defaults to True.\n\nremove_prefixes (bool): If True, the prefixes of the identifiers will\n    be removed. Defaults to True.\n</code></pre> Source code in <code>biocypher/_ontology.py</code> <pre><code>def __init__(\n    self,\n    ontology_file: str,\n    root_label: str,\n    ontology_file_format: str | None = None,\n    head_join_node_label: str | None = None,\n    merge_nodes: bool | None = True,\n    switch_label_and_id: bool = True,\n    remove_prefixes: bool = True,\n):\n    \"\"\"Initialize the OntologyAdapter class.\n\n    Args:\n    ----\n        ontology_file (str): Path to the ontology file. Can be local or\n            remote.\n\n        root_label (str): The label of the root node in the ontology. In\n            case of a tail ontology, this is the tail join node.\n\n        ontology_file_format (str): The format of the ontology file (e.g. \"application/rdf+xml\")\n            If format is not passed, it is determined automatically.\n\n        head_join_node_label (str): Optional variable to store the label of the\n            node in the head ontology that should be used to join to the\n            root node of the tail ontology. Defaults to None.\n\n        merge_nodes (bool): If True, head and tail join nodes will be\n            merged, using the label of the head join node. If False, the\n            tail join node will be attached as a child of the head join\n            node.\n\n        switch_label_and_id (bool): If True, the node names in the graph will be\n            the human-readable labels. If False, the node names will be the\n            identifiers. Defaults to True.\n\n        remove_prefixes (bool): If True, the prefixes of the identifiers will\n            be removed. Defaults to True.\n\n    \"\"\"\n    logger.info(f\"Instantiating OntologyAdapter class for {ontology_file}.\")\n\n    self._ontology_file = ontology_file\n    self._root_label = root_label\n    self._format = ontology_file_format\n    self._merge_nodes = merge_nodes\n    self._head_join_node = head_join_node_label\n    self._switch_label_and_id = switch_label_and_id\n    self._remove_prefixes = remove_prefixes\n\n    self._rdf_graph = self._load_rdf_graph(ontology_file)\n\n    self._nx_graph = self._rdf_to_nx(self._rdf_graph, root_label, switch_label_and_id)\n</code></pre>"},{"location":"reference/source/ontology/#biocypher._ontology.OntologyAdapter.get_ancestors","title":"<code>get_ancestors(node_label)</code>","text":"<p>Get the ancestors of a node in the ontology.</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>def get_ancestors(self, node_label):\n    \"\"\"Get the ancestors of a node in the ontology.\"\"\"\n    return nx.dfs_preorder_nodes(self._nx_graph, node_label)\n</code></pre>"},{"location":"reference/source/ontology/#biocypher._ontology.OntologyAdapter.get_head_join_node","title":"<code>get_head_join_node()</code>","text":"<p>Get the head join node of the ontology.</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>def get_head_join_node(self):\n    \"\"\"Get the head join node of the ontology.\"\"\"\n    return self._head_join_node\n</code></pre>"},{"location":"reference/source/ontology/#biocypher._ontology.OntologyAdapter.get_nx_graph","title":"<code>get_nx_graph()</code>","text":"<p>Get the networkx graph representing the ontology.</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>def get_nx_graph(self):\n    \"\"\"Get the networkx graph representing the ontology.\"\"\"\n    return self._nx_graph\n</code></pre>"},{"location":"reference/source/ontology/#biocypher._ontology.OntologyAdapter.get_rdf_graph","title":"<code>get_rdf_graph()</code>","text":"<p>Get the RDFlib graph representing the ontology.</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>def get_rdf_graph(self):\n    \"\"\"Get the RDFlib graph representing the ontology.\"\"\"\n    return self._rdf_graph\n</code></pre>"},{"location":"reference/source/ontology/#biocypher._ontology.OntologyAdapter.get_root_node","title":"<code>get_root_node()</code>","text":"<p>Get root node in the ontology.</p>"},{"location":"reference/source/ontology/#biocypher._ontology.OntologyAdapter.get_root_node--returns","title":"Returns","text":"<pre><code>root_node: If _switch_label_and_id is True, the root node label is\n    returned, otherwise the root node id is returned.\n</code></pre> Source code in <code>biocypher/_ontology.py</code> <pre><code>def get_root_node(self):\n    \"\"\"Get root node in the ontology.\n\n    Returns\n    -------\n        root_node: If _switch_label_and_id is True, the root node label is\n            returned, otherwise the root node id is returned.\n\n    \"\"\"\n    root_node = None\n    root_label = self._root_label.replace(\"_\", \" \")\n\n    if self._switch_label_and_id:\n        root_node = to_lower_sentence_case(root_label)\n    elif not self._switch_label_and_id:\n        for node, data in self.get_nx_graph().nodes(data=True):\n            if \"label\" in data and data[\"label\"] == to_lower_sentence_case(root_label):\n                root_node = node\n                break\n\n    return root_node\n</code></pre>"},{"location":"reference/source/ontology/#biocypher._ontology.OntologyAdapter.has_label","title":"<code>has_label(node, g)</code>","text":"<p>Check if the node has a label in the graph.</p> <pre><code>node (rdflib.URIRef): The node to check\ng (rdflib.Graph): The graph to check in\n</code></pre> <p>Returns:     bool: True if the node has a label, False otherwise</p> Source code in <code>biocypher/_ontology.py</code> <pre><code>def has_label(self, node: rdflib.URIRef, g: rdflib.Graph) -&gt; bool:\n    \"\"\"Check if the node has a label in the graph.\n\n    Args:\n    ----\n        node (rdflib.URIRef): The node to check\n        g (rdflib.Graph): The graph to check in\n    Returns:\n        bool: True if the node has a label, False otherwise\n\n    \"\"\"\n    return (node, rdflib.RDFS.label, None) in g\n</code></pre>"},{"location":"reference/source/ontology/#mapping-of-data-inputs-to-kg-ontology","title":"Mapping of data inputs to KG ontology","text":"<p>Class to store the ontology mapping and extensions.</p> Source code in <code>biocypher/_mapping.py</code> <pre><code>class OntologyMapping:\n    \"\"\"\n    Class to store the ontology mapping and extensions.\n    \"\"\"\n\n    def __init__(self, config_file: str = None):\n        self.schema = self._read_config(config_file)\n\n        self.extended_schema = self._extend_schema()\n\n    def _read_config(self, config_file: str = None):\n        \"\"\"\n        Read the configuration file and store the ontology mapping and extensions.\n        \"\"\"\n        if config_file is None:\n            schema_config = {}\n\n        # load yaml file from web\n        elif config_file.startswith(\"http\"):\n            with urlopen(config_file) as f:\n                schema_config = yaml.safe_load(f)\n\n        # get graph state from config (assume file is local)\n        else:\n            with open(config_file, \"r\") as f:\n                schema_config = yaml.safe_load(f)\n\n        return schema_config\n\n    def _extend_schema(self, d: Optional[dict] = None) -&gt; dict:\n        \"\"\"\n        Get leaves of the tree hierarchy from the data structure dict\n        contained in the `schema_config.yaml`. Creates virtual leaves\n        (as children) from entries that provide more than one preferred\n        id type (and corresponding inputs).\n\n        Args:\n            d:\n                Data structure dict from yaml file.\n\n        \"\"\"\n\n        d = d or self.schema\n\n        extended_schema = dict()\n\n        # first pass: get parent leaves with direct representation in ontology\n        for k, v in d.items():\n            # k is not an entity\n            if \"represented_as\" not in v:\n                continue\n\n            # preferred_id optional: if not provided, use `id`\n            if not v.get(\"preferred_id\"):\n                v[\"preferred_id\"] = \"id\"\n\n            # k is an entity that is present in the ontology\n            if \"is_a\" not in v:\n                extended_schema[k] = v\n\n        # second pass: \"vertical\" inheritance\n        d = self._vertical_property_inheritance(d)\n        for k, v in d.items():\n            if \"is_a\" in v:\n                # prevent loops\n                if k == v[\"is_a\"]:\n                    logger.warning(\n                        f\"Loop detected in ontology mapping: {k} -&gt; {v}. \"\n                        \"Removing item. Please fix the inheritance if you want \"\n                        \"to use this item.\"\n                    )\n                    continue\n\n                extended_schema[k] = v\n\n        # \"horizontal\" inheritance: create siblings for multiple identifiers or\n        # sources -&gt; virtual leaves or implicit children\n        mi_leaves = {}\n        ms_leaves = {}\n        for k, v in d.items():\n            # k is not an entity\n            if \"represented_as\" not in v:\n                continue\n\n            if isinstance(v.get(\"preferred_id\"), list):\n                mi_leaves = self._horizontal_inheritance_pid(k, v)\n                extended_schema.update(mi_leaves)\n\n            elif isinstance(v.get(\"source\"), list):\n                ms_leaves = self._horizontal_inheritance_source(k, v)\n                extended_schema.update(ms_leaves)\n\n        return extended_schema\n\n    def _vertical_property_inheritance(self, d):\n        \"\"\"\n        Inherit properties from parents to children and update `d` accordingly.\n        \"\"\"\n        for k, v in d.items():\n            # k is not an entity\n            if \"represented_as\" not in v:\n                continue\n\n            # k is an entity that is present in the ontology\n            if \"is_a\" not in v:\n                continue\n\n            # \"vertical\" inheritance: inherit properties from parent\n            if v.get(\"inherit_properties\", False):\n                # get direct ancestor\n                if isinstance(v[\"is_a\"], list):\n                    parent = v[\"is_a\"][0]\n                else:\n                    parent = v[\"is_a\"]\n\n                # ensure child has properties and exclude_properties\n                if \"properties\" not in v:\n                    v[\"properties\"] = {}\n                if \"exclude_properties\" not in v:\n                    v[\"exclude_properties\"] = {}\n\n                # update properties of child\n                parent_props = self.schema[parent].get(\"properties\", {})\n                if parent_props:\n                    v[\"properties\"].update(parent_props)\n\n                parent_excl_props = self.schema[parent].get(\"exclude_properties\", {})\n                if parent_excl_props:\n                    v[\"exclude_properties\"].update(parent_excl_props)\n\n                # update schema (d)\n                d[k] = v\n\n        return d\n\n    def _horizontal_inheritance_pid(self, key, value):\n        \"\"\"\n        Create virtual leaves for multiple preferred id types or sources.\n\n        If we create virtual leaves, input_label/label_in_input always has to be\n        a list.\n        \"\"\"\n\n        leaves = {}\n\n        preferred_id = value[\"preferred_id\"]\n        input_label = value.get(\"input_label\") or value[\"label_in_input\"]\n        represented_as = value[\"represented_as\"]\n\n        # adjust lengths\n        max_l = max(\n            [\n                len(_misc.to_list(preferred_id)),\n                len(_misc.to_list(input_label)),\n                len(_misc.to_list(represented_as)),\n            ],\n        )\n\n        # adjust pid length if necessary\n        if isinstance(preferred_id, str):\n            pids = [preferred_id] * max_l\n        else:\n            pids = preferred_id\n\n        # adjust rep length if necessary\n        if isinstance(represented_as, str):\n            reps = [represented_as] * max_l\n        else:\n            reps = represented_as\n\n        for pid, lab, rep in zip(pids, input_label, reps):\n            skey = pid + \".\" + key\n            svalue = {\n                \"preferred_id\": pid,\n                \"input_label\": lab,\n                \"represented_as\": rep,\n                # mark as virtual\n                \"virtual\": True,\n            }\n\n            # inherit is_a if exists\n            if \"is_a\" in value.keys():\n                # treat as multiple inheritance\n                if isinstance(value[\"is_a\"], list):\n                    v = list(value[\"is_a\"])\n                    v.insert(0, key)\n                    svalue[\"is_a\"] = v\n\n                else:\n                    svalue[\"is_a\"] = [key, value[\"is_a\"]]\n\n            else:\n                # set parent as is_a\n                svalue[\"is_a\"] = key\n\n            # inherit everything except core attributes\n            for k, v in value.items():\n                if k not in [\n                    \"is_a\",\n                    \"preferred_id\",\n                    \"input_label\",\n                    \"label_in_input\",\n                    \"represented_as\",\n                ]:\n                    svalue[k] = v\n\n            leaves[skey] = svalue\n\n        return leaves\n\n    def _horizontal_inheritance_source(self, key, value):\n        \"\"\"\n        Create virtual leaves for multiple sources.\n\n        If we create virtual leaves, input_label/label_in_input always has to be\n        a list.\n        \"\"\"\n\n        leaves = {}\n\n        source = value[\"source\"]\n        input_label = value.get(\"input_label\") or value[\"label_in_input\"]\n        represented_as = value[\"represented_as\"]\n\n        # adjust lengths\n        src_l = len(source)\n\n        # adjust label length if necessary\n        if isinstance(input_label, str):\n            labels = [input_label] * src_l\n        else:\n            labels = input_label\n\n        # adjust rep length if necessary\n        if isinstance(represented_as, str):\n            reps = [represented_as] * src_l\n        else:\n            reps = represented_as\n\n        for src, lab, rep in zip(source, labels, reps):\n            skey = src + \".\" + key\n            svalue = {\n                \"source\": src,\n                \"input_label\": lab,\n                \"represented_as\": rep,\n                # mark as virtual\n                \"virtual\": True,\n            }\n\n            # inherit is_a if exists\n            if \"is_a\" in value.keys():\n                # treat as multiple inheritance\n                if isinstance(value[\"is_a\"], list):\n                    v = list(value[\"is_a\"])\n                    v.insert(0, key)\n                    svalue[\"is_a\"] = v\n\n                else:\n                    svalue[\"is_a\"] = [key, value[\"is_a\"]]\n\n            else:\n                # set parent as is_a\n                svalue[\"is_a\"] = key\n\n            # inherit everything except core attributes\n            for k, v in value.items():\n                if k not in [\n                    \"is_a\",\n                    \"source\",\n                    \"input_label\",\n                    \"label_in_input\",\n                    \"represented_as\",\n                ]:\n                    svalue[k] = v\n\n            leaves[skey] = svalue\n\n        return leaves\n</code></pre>"},{"location":"reference/source/output-driver/","title":"Output Driver","text":""},{"location":"reference/source/output-driver/#connector-retrieval","title":"Connector Retrieval","text":"<p>Return the connector class.</p>"},{"location":"reference/source/output-driver/#biocypher.output.connect._get_connector.get_connector--returns","title":"Returns","text":"<pre><code>class: the connector class\n</code></pre>"},{"location":"reference/source/output-driver/#biocypher.output.connect._get_connector.get_connector--raises","title":"Raises","text":"<pre><code>NotImplementedError: if the DBMS is not supported\n</code></pre> Source code in <code>biocypher/output/connect/_get_connector.py</code> <pre><code>def get_connector(\n    dbms: str,\n    translator: Translator,\n) -&gt; _Neo4jDriver:\n    \"\"\"Return the connector class.\n\n    Returns\n    -------\n        class: the connector class\n\n    Raises\n    ------\n        NotImplementedError: if the DBMS is not supported\n\n    \"\"\"\n    dbms_config = _config(dbms)\n\n    if dbms == \"neo4j\":\n        return _Neo4jDriver(\n            database_name=dbms_config[\"database_name\"],\n            wipe=dbms_config[\"wipe\"],\n            uri=dbms_config[\"uri\"],\n            user=dbms_config[\"user\"],\n            password=dbms_config[\"password\"],\n            multi_db=dbms_config[\"multi_db\"],\n            translator=translator,\n        )\n\n    msg = f\"Online mode is not supported for the DBMS {dbms}.\"\n    logger.error(msg)\n    raise NotImplementedError(msg)\n</code></pre>"},{"location":"reference/source/output-driver/#neo4j-driver","title":"Neo4j Driver","text":"<p>Manages a BioCypher connection to a Neo4j database using the <code>neo4j_utils.Driver</code> class.</p> <p>Args:</p> <pre><code>database_name (str): The name of the database to connect to.\n\nwipe (bool): Whether to wipe the database before importing.\n\nuri (str): The URI of the database.\n\nuser (str): The username to use for authentication.\n\npassword (str): The password to use for authentication.\n\nmulti_db (bool): Whether to use multi-database mode.\n\nfetch_size (int): The number of records to fetch at a time.\n\nincrement_version (bool): Whether to increment the version number.\n\ntranslator (Translator): The translator to use for mapping.\n</code></pre> Source code in <code>biocypher/output/connect/_neo4j_driver.py</code> <pre><code>class _Neo4jDriver:\n    \"\"\"\n    Manages a BioCypher connection to a Neo4j database using the\n    ``neo4j_utils.Driver`` class.\n\n    Args:\n\n        database_name (str): The name of the database to connect to.\n\n        wipe (bool): Whether to wipe the database before importing.\n\n        uri (str): The URI of the database.\n\n        user (str): The username to use for authentication.\n\n        password (str): The password to use for authentication.\n\n        multi_db (bool): Whether to use multi-database mode.\n\n        fetch_size (int): The number of records to fetch at a time.\n\n        increment_version (bool): Whether to increment the version number.\n\n        translator (Translator): The translator to use for mapping.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        database_name: str,\n        uri: str,\n        user: str,\n        password: str,\n        multi_db: bool,\n        translator: Translator,\n        wipe: bool = False,\n        fetch_size: int = 1000,\n        increment_version: bool = True,\n    ):\n        self.translator = translator\n\n        self._driver = neo4j_utils.Driver(\n            db_name=database_name,\n            db_uri=uri,\n            db_user=user,\n            db_passwd=password,\n            fetch_size=fetch_size,\n            wipe=wipe,\n            multi_db=multi_db,\n            raise_errors=True,\n        )\n\n        # check for biocypher config in connected graph\n\n        if wipe:\n            self.init_db()\n\n        if increment_version:\n            # set new current version node\n            self._update_meta_graph()\n\n    def _update_meta_graph(self):\n        logger.info(\"Updating Neo4j meta graph.\")\n\n        # find current version node\n        db_version = self._driver.query(\n            \"MATCH (v:BioCypher) WHERE NOT (v)-[:PRECEDES]-&gt;() RETURN v\",\n        )\n        # add version node\n        self.add_biocypher_nodes(self.translator.ontology)\n\n        # connect version node to previous\n        if db_version[0]:\n            previous = db_version[0][0]\n            previous_id = previous[\"v\"][\"id\"]\n            e_meta = BioCypherEdge(\n                previous_id,\n                self.translator.ontology.get_dict().get(\"node_id\"),\n                \"PRECEDES\",\n            )\n            self.add_biocypher_edges(e_meta)\n\n    def init_db(self):\n        \"\"\"\n        Used to initialise a property graph database by setting up new\n        constraints. Wipe has been performed by the ``neo4j_utils.Driver``\n        class` already.\n\n        Todo:\n            - set up constraint creation interactively depending on the\n                need of the database\n        \"\"\"\n\n        logger.info(\"Initialising database.\")\n        self._create_constraints()\n\n    def _create_constraints(self):\n        \"\"\"\n        Creates constraints on node types in the graph. Used for\n        initial setup.\n\n        Grabs leaves of the ``schema_config.yaml`` file and creates\n        constraints on the id of all entities represented as nodes.\n        \"\"\"\n\n        logger.info(\"Creating constraints for node types in config.\")\n\n        major_neo4j_version = int(self._get_neo4j_version().split(\".\")[0])\n        # get structure\n        for leaf in self.translator.ontology.mapping.extended_schema.items():\n            label = _misc.sentencecase_to_pascalcase(leaf[0], sep=r\"\\s\\.\")\n            if leaf[1][\"represented_as\"] == \"node\":\n                if major_neo4j_version &gt;= 5:\n                    s = f\"CREATE CONSTRAINT `{label}_id` \" f\"IF NOT EXISTS FOR (n:`{label}`) \" \"REQUIRE n.id IS UNIQUE\"\n                    self._driver.query(s)\n                else:\n                    s = f\"CREATE CONSTRAINT `{label}_id` \" f\"IF NOT EXISTS ON (n:`{label}`) \" \"ASSERT n.id IS UNIQUE\"\n                    self._driver.query(s)\n\n    def _get_neo4j_version(self):\n        \"\"\"Get neo4j version.\"\"\"\n        try:\n            neo4j_version = self._driver.query(\n                \"\"\"\n                    CALL dbms.components()\n                    YIELD name, versions, edition\n                    UNWIND versions AS version\n                    RETURN version AS version\n                \"\"\",\n            )[0][0][\"version\"]\n            return neo4j_version\n        except Exception as e:\n            logger.warning(f\"Error detecting Neo4j version: {e} use default version 4.0.0.\")\n            return \"4.0.0\"\n\n    def add_nodes(self, id_type_tuples: Iterable[tuple]) -&gt; tuple:\n        \"\"\"\n        Generic node adder method to add any kind of input to the graph via the\n        :class:`biocypher.create.BioCypherNode` class. Employs translation\n        functionality and calls the :meth:`add_biocypher_nodes()` method.\n\n        Args:\n            id_type_tuples (iterable of 3-tuple): for each node to add to\n                the biocypher graph, a 3-tuple with the following layout:\n                first, the (unique if constrained) ID of the node; second, the\n                type of the node, capitalised or PascalCase and in noun form\n                (Neo4j primary label, eg `:Protein`); and third, a dictionary\n                of arbitrary properties the node should possess (can be empty).\n\n        Returns:\n            2-tuple: the query result of :meth:`add_biocypher_nodes()`\n                - first entry: data\n                - second entry: Neo4j summary.\n        \"\"\"\n\n        bn = self.translator.translate_nodes(id_type_tuples)\n        return self.add_biocypher_nodes(bn)\n\n    def add_edges(self, id_src_tar_type_tuples: Iterable[tuple]) -&gt; tuple:\n        \"\"\"\n        Generic edge adder method to add any kind of input to the graph\n        via the :class:`biocypher.create.BioCypherEdge` class. Employs\n        translation functionality and calls the\n        :meth:`add_biocypher_edges()` method.\n\n        Args:\n\n            id_src_tar_type_tuples (iterable of 5-tuple):\n\n                for each edge to add to the biocypher graph, a 5-tuple\n                with the following layout: first, the optional unique ID\n                of the interaction. This can be `None` if there is no\n                systematic identifier (which for many interactions is\n                the case). Second and third, the (unique if constrained)\n                IDs of the source and target nodes of the relationship;\n                fourth, the type of the relationship; and fifth, a\n                dictionary of arbitrary properties the edge should\n                possess (can be empty).\n\n        Returns:\n\n            2-tuple: the query result of :meth:`add_biocypher_edges()`\n\n                - first entry: data\n                - second entry: Neo4j summary.\n        \"\"\"\n\n        bn = self.translator.translate_edges(id_src_tar_type_tuples)\n        return self.add_biocypher_edges(bn)\n\n    def add_biocypher_nodes(\n        self,\n        nodes: Iterable[BioCypherNode],\n        explain: bool = False,\n        profile: bool = False,\n    ) -&gt; bool:\n        \"\"\"\n        Accepts a node type handoff class\n        (:class:`biocypher.create.BioCypherNode`) with id,\n        label, and a dict of properties (passing on the type of\n        property, ie, ``int``, ``str``, ...).\n\n        The dict retrieved by the\n        :meth:`biocypher.create.BioCypherNode.get_dict()` method is\n        passed into Neo4j as a map of maps, explicitly encoding node id\n        and label, and adding all other properties from the 'properties'\n        key of the dict. The merge is performed via APOC, matching only\n        on node id to prevent duplicates. The same properties are set on\n        match and on create, irrespective of the actual event.\n\n        Args:\n            nodes:\n                An iterable of :class:`biocypher.create.BioCypherNode` objects.\n            explain:\n                Call ``EXPLAIN`` on the CYPHER query.\n            profile:\n                Do profiling on the CYPHER query.\n\n        Returns:\n            True for success, False otherwise.\n        \"\"\"\n\n        try:\n            nodes = _misc.to_list(nodes)\n\n            entities = [node.get_dict() for node in nodes]\n\n        except AttributeError:\n            msg = \"Nodes must have a `get_dict` method.\"\n            logger.error(msg)\n\n            raise ValueError(msg)\n\n        logger.info(f\"Merging {len(entities)} nodes.\")\n\n        entity_query = (\n            \"UNWIND $entities AS ent \"\n            \"CALL apoc.merge.node([ent.node_label], \"\n            \"{id: ent.node_id}, ent.properties, ent.properties) \"\n            \"YIELD node \"\n            \"RETURN node\"\n        )\n\n        method = \"explain\" if explain else \"profile\" if profile else \"query\"\n\n        result = getattr(self._driver, method)(\n            entity_query,\n            parameters={\n                \"entities\": entities,\n            },\n        )\n\n        logger.info(\"Finished merging nodes.\")\n\n        return result\n\n    def add_biocypher_edges(\n        self,\n        edges: Iterable[BioCypherEdge],\n        explain: bool = False,\n        profile: bool = False,\n    ) -&gt; bool:\n        \"\"\"\n        Accepts an edge type handoff class\n        (:class:`biocypher.create.BioCypherEdge`) with source\n        and target ids, label, and a dict of properties (passing on the\n        type of property, ie, int, string ...).\n\n        The individual edge is either passed as a singleton, in the case\n        of representation as an edge in the graph, or as a 4-tuple, in\n        the case of representation as a node (with two edges connecting\n        to interaction partners).\n\n        The dict retrieved by the\n        :meth:`biocypher.create.BioCypherEdge.get_dict()` method is\n        passed into Neo4j as a map of maps, explicitly encoding source\n        and target ids and the relationship label, and adding all edge\n        properties from the 'properties' key of the dict. The merge is\n        performed via APOC, matching only on source and target id to\n        prevent duplicates. The same properties are set on match and on\n        create, irrespective of the actual event.\n\n        Args:\n            edges:\n                An iterable of :class:`biocypher.create.BioCypherEdge` objects.\n            explain:\n                Call ``EXPLAIN`` on the CYPHER query.\n            profile:\n                Do profiling on the CYPHER query.\n\n        Returns:\n            `True` for success, `False` otherwise.\n        \"\"\"\n\n        edges = _misc.ensure_iterable(edges)\n        edges = itertools.chain(*(_misc.ensure_iterable(i) for i in edges))\n\n        nodes = []\n        rels = []\n\n        try:\n            for e in edges:\n                if hasattr(e, \"get_node\"):\n                    nodes.append(e.get_node())\n                    rels.append(e.get_source_edge().get_dict())\n                    rels.append(e.get_target_edge().get_dict())\n\n                else:\n                    rels.append(e.get_dict())\n\n        except AttributeError:\n            msg = \"Edges and nodes must have a `get_dict` method.\"\n            logger.error(msg)\n\n            raise ValueError(msg)\n\n        self.add_biocypher_nodes(nodes)\n        logger.info(f\"Merging {len(rels)} edges.\")\n\n        # cypher query\n\n        # merging only on the ids of the entities, passing the\n        # properties on match and on create;\n        # TODO add node labels?\n        node_query = \"UNWIND $rels AS r \" \"MERGE (src {id: r.source_id}) \" \"MERGE (tar {id: r.target_id}) \"\n\n        self._driver.query(node_query, parameters={\"rels\": rels})\n\n        edge_query = (\n            \"UNWIND $rels AS r \"\n            \"MATCH (src {id: r.source_id}) \"\n            \"MATCH (tar {id: r.target_id}) \"\n            \"WITH src, tar, r \"\n            \"CALL apoc.merge.relationship\"\n            \"(src, r.relationship_label, NULL, \"\n            \"r.properties, tar, r.properties) \"\n            \"YIELD rel \"\n            \"RETURN rel\"\n        )\n\n        method = \"explain\" if explain else \"profile\" if profile else \"query\"\n\n        result = getattr(self._driver, method)(edge_query, parameters={\"rels\": rels})\n\n        logger.info(\"Finished merging edges.\")\n\n        return result\n</code></pre>"},{"location":"reference/source/output-driver/#biocypher.output.connect._neo4j_driver._Neo4jDriver.add_biocypher_edges","title":"<code>add_biocypher_edges(edges, explain=False, profile=False)</code>","text":"<p>Accepts an edge type handoff class (:class:<code>biocypher.create.BioCypherEdge</code>) with source and target ids, label, and a dict of properties (passing on the type of property, ie, int, string ...).</p> <p>The individual edge is either passed as a singleton, in the case of representation as an edge in the graph, or as a 4-tuple, in the case of representation as a node (with two edges connecting to interaction partners).</p> <p>The dict retrieved by the :meth:<code>biocypher.create.BioCypherEdge.get_dict()</code> method is passed into Neo4j as a map of maps, explicitly encoding source and target ids and the relationship label, and adding all edge properties from the 'properties' key of the dict. The merge is performed via APOC, matching only on source and target id to prevent duplicates. The same properties are set on match and on create, irrespective of the actual event.</p> <p>Parameters:</p> Name Type Description Default <code>edges</code> <code>Iterable[BioCypherEdge]</code> <p>An iterable of :class:<code>biocypher.create.BioCypherEdge</code> objects.</p> required <code>explain</code> <code>bool</code> <p>Call <code>EXPLAIN</code> on the CYPHER query.</p> <code>False</code> <code>profile</code> <code>bool</code> <p>Do profiling on the CYPHER query.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> for success, <code>False</code> otherwise.</p> Source code in <code>biocypher/output/connect/_neo4j_driver.py</code> <pre><code>def add_biocypher_edges(\n    self,\n    edges: Iterable[BioCypherEdge],\n    explain: bool = False,\n    profile: bool = False,\n) -&gt; bool:\n    \"\"\"\n    Accepts an edge type handoff class\n    (:class:`biocypher.create.BioCypherEdge`) with source\n    and target ids, label, and a dict of properties (passing on the\n    type of property, ie, int, string ...).\n\n    The individual edge is either passed as a singleton, in the case\n    of representation as an edge in the graph, or as a 4-tuple, in\n    the case of representation as a node (with two edges connecting\n    to interaction partners).\n\n    The dict retrieved by the\n    :meth:`biocypher.create.BioCypherEdge.get_dict()` method is\n    passed into Neo4j as a map of maps, explicitly encoding source\n    and target ids and the relationship label, and adding all edge\n    properties from the 'properties' key of the dict. The merge is\n    performed via APOC, matching only on source and target id to\n    prevent duplicates. The same properties are set on match and on\n    create, irrespective of the actual event.\n\n    Args:\n        edges:\n            An iterable of :class:`biocypher.create.BioCypherEdge` objects.\n        explain:\n            Call ``EXPLAIN`` on the CYPHER query.\n        profile:\n            Do profiling on the CYPHER query.\n\n    Returns:\n        `True` for success, `False` otherwise.\n    \"\"\"\n\n    edges = _misc.ensure_iterable(edges)\n    edges = itertools.chain(*(_misc.ensure_iterable(i) for i in edges))\n\n    nodes = []\n    rels = []\n\n    try:\n        for e in edges:\n            if hasattr(e, \"get_node\"):\n                nodes.append(e.get_node())\n                rels.append(e.get_source_edge().get_dict())\n                rels.append(e.get_target_edge().get_dict())\n\n            else:\n                rels.append(e.get_dict())\n\n    except AttributeError:\n        msg = \"Edges and nodes must have a `get_dict` method.\"\n        logger.error(msg)\n\n        raise ValueError(msg)\n\n    self.add_biocypher_nodes(nodes)\n    logger.info(f\"Merging {len(rels)} edges.\")\n\n    # cypher query\n\n    # merging only on the ids of the entities, passing the\n    # properties on match and on create;\n    # TODO add node labels?\n    node_query = \"UNWIND $rels AS r \" \"MERGE (src {id: r.source_id}) \" \"MERGE (tar {id: r.target_id}) \"\n\n    self._driver.query(node_query, parameters={\"rels\": rels})\n\n    edge_query = (\n        \"UNWIND $rels AS r \"\n        \"MATCH (src {id: r.source_id}) \"\n        \"MATCH (tar {id: r.target_id}) \"\n        \"WITH src, tar, r \"\n        \"CALL apoc.merge.relationship\"\n        \"(src, r.relationship_label, NULL, \"\n        \"r.properties, tar, r.properties) \"\n        \"YIELD rel \"\n        \"RETURN rel\"\n    )\n\n    method = \"explain\" if explain else \"profile\" if profile else \"query\"\n\n    result = getattr(self._driver, method)(edge_query, parameters={\"rels\": rels})\n\n    logger.info(\"Finished merging edges.\")\n\n    return result\n</code></pre>"},{"location":"reference/source/output-driver/#biocypher.output.connect._neo4j_driver._Neo4jDriver.add_biocypher_nodes","title":"<code>add_biocypher_nodes(nodes, explain=False, profile=False)</code>","text":"<p>Accepts a node type handoff class (:class:<code>biocypher.create.BioCypherNode</code>) with id, label, and a dict of properties (passing on the type of property, ie, <code>int</code>, <code>str</code>, ...).</p> <p>The dict retrieved by the :meth:<code>biocypher.create.BioCypherNode.get_dict()</code> method is passed into Neo4j as a map of maps, explicitly encoding node id and label, and adding all other properties from the 'properties' key of the dict. The merge is performed via APOC, matching only on node id to prevent duplicates. The same properties are set on match and on create, irrespective of the actual event.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>Iterable[BioCypherNode]</code> <p>An iterable of :class:<code>biocypher.create.BioCypherNode</code> objects.</p> required <code>explain</code> <code>bool</code> <p>Call <code>EXPLAIN</code> on the CYPHER query.</p> <code>False</code> <code>profile</code> <code>bool</code> <p>Do profiling on the CYPHER query.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True for success, False otherwise.</p> Source code in <code>biocypher/output/connect/_neo4j_driver.py</code> <pre><code>def add_biocypher_nodes(\n    self,\n    nodes: Iterable[BioCypherNode],\n    explain: bool = False,\n    profile: bool = False,\n) -&gt; bool:\n    \"\"\"\n    Accepts a node type handoff class\n    (:class:`biocypher.create.BioCypherNode`) with id,\n    label, and a dict of properties (passing on the type of\n    property, ie, ``int``, ``str``, ...).\n\n    The dict retrieved by the\n    :meth:`biocypher.create.BioCypherNode.get_dict()` method is\n    passed into Neo4j as a map of maps, explicitly encoding node id\n    and label, and adding all other properties from the 'properties'\n    key of the dict. The merge is performed via APOC, matching only\n    on node id to prevent duplicates. The same properties are set on\n    match and on create, irrespective of the actual event.\n\n    Args:\n        nodes:\n            An iterable of :class:`biocypher.create.BioCypherNode` objects.\n        explain:\n            Call ``EXPLAIN`` on the CYPHER query.\n        profile:\n            Do profiling on the CYPHER query.\n\n    Returns:\n        True for success, False otherwise.\n    \"\"\"\n\n    try:\n        nodes = _misc.to_list(nodes)\n\n        entities = [node.get_dict() for node in nodes]\n\n    except AttributeError:\n        msg = \"Nodes must have a `get_dict` method.\"\n        logger.error(msg)\n\n        raise ValueError(msg)\n\n    logger.info(f\"Merging {len(entities)} nodes.\")\n\n    entity_query = (\n        \"UNWIND $entities AS ent \"\n        \"CALL apoc.merge.node([ent.node_label], \"\n        \"{id: ent.node_id}, ent.properties, ent.properties) \"\n        \"YIELD node \"\n        \"RETURN node\"\n    )\n\n    method = \"explain\" if explain else \"profile\" if profile else \"query\"\n\n    result = getattr(self._driver, method)(\n        entity_query,\n        parameters={\n            \"entities\": entities,\n        },\n    )\n\n    logger.info(\"Finished merging nodes.\")\n\n    return result\n</code></pre>"},{"location":"reference/source/output-driver/#biocypher.output.connect._neo4j_driver._Neo4jDriver.add_edges","title":"<code>add_edges(id_src_tar_type_tuples)</code>","text":"<p>Generic edge adder method to add any kind of input to the graph via the :class:<code>biocypher.create.BioCypherEdge</code> class. Employs translation functionality and calls the :meth:<code>add_biocypher_edges()</code> method.</p> <p>Args:</p> <pre><code>id_src_tar_type_tuples (iterable of 5-tuple):\n\n    for each edge to add to the biocypher graph, a 5-tuple\n    with the following layout: first, the optional unique ID\n    of the interaction. This can be `None` if there is no\n    systematic identifier (which for many interactions is\n    the case). Second and third, the (unique if constrained)\n    IDs of the source and target nodes of the relationship;\n    fourth, the type of the relationship; and fifth, a\n    dictionary of arbitrary properties the edge should\n    possess (can be empty).\n</code></pre> <p>Returns:</p> <pre><code>2-tuple: the query result of :meth:`add_biocypher_edges()`\n\n    - first entry: data\n    - second entry: Neo4j summary.\n</code></pre> Source code in <code>biocypher/output/connect/_neo4j_driver.py</code> <pre><code>def add_edges(self, id_src_tar_type_tuples: Iterable[tuple]) -&gt; tuple:\n    \"\"\"\n    Generic edge adder method to add any kind of input to the graph\n    via the :class:`biocypher.create.BioCypherEdge` class. Employs\n    translation functionality and calls the\n    :meth:`add_biocypher_edges()` method.\n\n    Args:\n\n        id_src_tar_type_tuples (iterable of 5-tuple):\n\n            for each edge to add to the biocypher graph, a 5-tuple\n            with the following layout: first, the optional unique ID\n            of the interaction. This can be `None` if there is no\n            systematic identifier (which for many interactions is\n            the case). Second and third, the (unique if constrained)\n            IDs of the source and target nodes of the relationship;\n            fourth, the type of the relationship; and fifth, a\n            dictionary of arbitrary properties the edge should\n            possess (can be empty).\n\n    Returns:\n\n        2-tuple: the query result of :meth:`add_biocypher_edges()`\n\n            - first entry: data\n            - second entry: Neo4j summary.\n    \"\"\"\n\n    bn = self.translator.translate_edges(id_src_tar_type_tuples)\n    return self.add_biocypher_edges(bn)\n</code></pre>"},{"location":"reference/source/output-driver/#biocypher.output.connect._neo4j_driver._Neo4jDriver.add_nodes","title":"<code>add_nodes(id_type_tuples)</code>","text":"<p>Generic node adder method to add any kind of input to the graph via the :class:<code>biocypher.create.BioCypherNode</code> class. Employs translation functionality and calls the :meth:<code>add_biocypher_nodes()</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>id_type_tuples</code> <code>iterable of 3-tuple</code> <p>for each node to add to the biocypher graph, a 3-tuple with the following layout: first, the (unique if constrained) ID of the node; second, the type of the node, capitalised or PascalCase and in noun form (Neo4j primary label, eg <code>:Protein</code>); and third, a dictionary of arbitrary properties the node should possess (can be empty).</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>2-tuple: the query result of :meth:<code>add_biocypher_nodes()</code> - first entry: data - second entry: Neo4j summary.</p> Source code in <code>biocypher/output/connect/_neo4j_driver.py</code> <pre><code>def add_nodes(self, id_type_tuples: Iterable[tuple]) -&gt; tuple:\n    \"\"\"\n    Generic node adder method to add any kind of input to the graph via the\n    :class:`biocypher.create.BioCypherNode` class. Employs translation\n    functionality and calls the :meth:`add_biocypher_nodes()` method.\n\n    Args:\n        id_type_tuples (iterable of 3-tuple): for each node to add to\n            the biocypher graph, a 3-tuple with the following layout:\n            first, the (unique if constrained) ID of the node; second, the\n            type of the node, capitalised or PascalCase and in noun form\n            (Neo4j primary label, eg `:Protein`); and third, a dictionary\n            of arbitrary properties the node should possess (can be empty).\n\n    Returns:\n        2-tuple: the query result of :meth:`add_biocypher_nodes()`\n            - first entry: data\n            - second entry: Neo4j summary.\n    \"\"\"\n\n    bn = self.translator.translate_nodes(id_type_tuples)\n    return self.add_biocypher_nodes(bn)\n</code></pre>"},{"location":"reference/source/output-driver/#biocypher.output.connect._neo4j_driver._Neo4jDriver.init_db","title":"<code>init_db()</code>","text":"<p>Used to initialise a property graph database by setting up new constraints. Wipe has been performed by the <code>neo4j_utils.Driver</code> class` already.</p> Todo <ul> <li>set up constraint creation interactively depending on the     need of the database</li> </ul> Source code in <code>biocypher/output/connect/_neo4j_driver.py</code> <pre><code>def init_db(self):\n    \"\"\"\n    Used to initialise a property graph database by setting up new\n    constraints. Wipe has been performed by the ``neo4j_utils.Driver``\n    class` already.\n\n    Todo:\n        - set up constraint creation interactively depending on the\n            need of the database\n    \"\"\"\n\n    logger.info(\"Initialising database.\")\n    self._create_constraints()\n</code></pre>"},{"location":"reference/source/output-in-memory/","title":"Output In-Memory","text":""},{"location":"reference/source/output-in-memory/#in-memory-knowledge-graph-retrieval","title":"In-Memory Knowledge Graph Retrieval","text":"<p>Return the in-memory KG class.</p>"},{"location":"reference/source/output-in-memory/#biocypher.output.in_memory._get_in_memory_kg.get_in_memory_kg--returns","title":"Returns","text":"<pre><code>_InMemoryKG: the in-memory KG class\n</code></pre> Source code in <code>biocypher/output/in_memory/_get_in_memory_kg.py</code> <pre><code>def get_in_memory_kg(\n    dbms: str,\n    deduplicator: Deduplicator,\n) -&gt; _InMemoryKG:\n    \"\"\"Return the in-memory KG class.\n\n    Returns\n    -------\n        _InMemoryKG: the in-memory KG class\n\n    \"\"\"\n    if dbms in [\"csv\", \"pandas\", \"tabular\"]:\n        return PandasKG(deduplicator)\n    if dbms == \"networkx\":\n        return NetworkxKG(deduplicator)\n    elif dbms == \"airr\":\n        return AirrKG(deduplicator)\n    else:\n        msg = f\"Getting the in memory BioCypher KG is not supported for the DBMS {dbms}. Supported: {IN_MEMORY_DBMS}.\"\n        logger.error(msg)\n        raise NotImplementedError(msg)\n</code></pre>"},{"location":"reference/source/output-in-memory/#in-memory-knowledge-graph-base-class","title":"In-Memory Knowledge Graph Base Class","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class for handling the in-memory Knowledge Graph instance. Specifics of the different in-memory implementations (e.g. csv, networkx) are implemented in the child classes. Any concrete in-memory implementation needs to implement at least: - add_nodes - add_edges - get_kg - _separate_entity_types</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>InMemoryKG implementation must override 'add_nodes'</p> <code>NotImplementedError</code> <p>InMemoryKG implementation must override 'add_edges'</p> <code>NotImplementedError</code> <p>InMemoryKG implementation must override 'get_kg'</p> Source code in <code>biocypher/output/in_memory/_in_memory_kg.py</code> <pre><code>class _InMemoryKG(ABC):\n    \"\"\"Abstract class for handling the in-memory Knowledge Graph instance.\n    Specifics of the different in-memory implementations (e.g. csv, networkx)\n    are implemented in the child classes. Any concrete in-memory implementation\n    needs to implement at least:\n    - add_nodes\n    - add_edges\n    - get_kg\n    - _separate_entity_types\n\n    Raises:\n        NotImplementedError: InMemoryKG implementation must override 'add_nodes'\n        NotImplementedError: InMemoryKG implementation must override 'add_edges'\n        NotImplementedError: InMemoryKG implementation must override 'get_kg'\n    \"\"\"\n\n    @abstractmethod\n    def add_nodes(self, nodes):\n        \"\"\"Add nodes to the in-memory knowledge graph.\n\n        Args:\n            nodes (Iterable[BioCypherNode]): Iterable of BioCypherNode objects.\n        \"\"\"\n        raise NotImplementedError(\"InMemoryKG implementation must override 'add_nodes'\")\n\n    @abstractmethod\n    def add_edges(self, edges):\n        \"\"\"Add edges to the in-memory knowledge graph.\n\n        Args:\n            edges (Iterable[BioCypherEdge]): Iterable of BioCypherEdge objects.\n        \"\"\"\n        raise NotImplementedError(\"InMemoryKG implementation must override 'add_edges'\")\n\n    @abstractmethod\n    def get_kg(self):\n        \"\"\"Return the in-memory knowledge graph.\"\"\"\n        raise NotImplementedError(\"InMemoryKG implementation must override 'get_kg'\")\n\n    def _separate_entity_types(self, entities):\n        \"\"\"\n        Given mixed iterable of BioCypher objects, separate them into lists by\n        type. Also deduplicates using the `Deduplicator` instance.\n        \"\"\"\n        lists = {}\n        for entity in entities:\n            if (\n                not isinstance(entity, BioCypherNode)\n                and not isinstance(entity, BioCypherEdge)\n                and not isinstance(entity, BioCypherRelAsNode)\n            ):\n                raise TypeError(\n                    \"Expected a BioCypherNode / BioCypherEdge / \" f\"BioCypherRelAsNode, got {type(entity)}.\"\n                )\n\n            if isinstance(entity, BioCypherNode):\n                seen = self.deduplicator.node_seen(entity)\n            elif isinstance(entity, BioCypherEdge):\n                seen = self.deduplicator.edge_seen(entity)\n            elif isinstance(entity, BioCypherRelAsNode):\n                seen = self.deduplicator.rel_as_node_seen(entity)\n\n            if seen:\n                continue\n\n            if isinstance(entity, BioCypherRelAsNode):\n                node = entity.get_node()\n                source_edge = entity.get_source_edge()\n                target_edge = entity.get_target_edge()\n\n                _type = node.get_type()\n                if _type not in lists:\n                    lists[_type] = []\n                lists[_type].append(node)\n\n                _source_type = source_edge.get_type()\n                if _source_type not in lists:\n                    lists[_source_type] = []\n                lists[_source_type].append(source_edge)\n\n                _target_type = target_edge.get_type()\n                if _target_type not in lists:\n                    lists[_target_type] = []\n                lists[_target_type].append(target_edge)\n                continue\n\n            _type = entity.get_type()\n            if _type not in lists:\n                lists[_type] = []\n            lists[_type].append(entity)\n\n        return lists\n</code></pre>"},{"location":"reference/source/output-in-memory/#biocypher.output.in_memory._in_memory_kg._InMemoryKG.add_edges","title":"<code>add_edges(edges)</code>  <code>abstractmethod</code>","text":"<p>Add edges to the in-memory knowledge graph.</p> <p>Parameters:</p> Name Type Description Default <code>edges</code> <code>Iterable[BioCypherEdge]</code> <p>Iterable of BioCypherEdge objects.</p> required Source code in <code>biocypher/output/in_memory/_in_memory_kg.py</code> <pre><code>@abstractmethod\ndef add_edges(self, edges):\n    \"\"\"Add edges to the in-memory knowledge graph.\n\n    Args:\n        edges (Iterable[BioCypherEdge]): Iterable of BioCypherEdge objects.\n    \"\"\"\n    raise NotImplementedError(\"InMemoryKG implementation must override 'add_edges'\")\n</code></pre>"},{"location":"reference/source/output-in-memory/#biocypher.output.in_memory._in_memory_kg._InMemoryKG.add_nodes","title":"<code>add_nodes(nodes)</code>  <code>abstractmethod</code>","text":"<p>Add nodes to the in-memory knowledge graph.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>Iterable[BioCypherNode]</code> <p>Iterable of BioCypherNode objects.</p> required Source code in <code>biocypher/output/in_memory/_in_memory_kg.py</code> <pre><code>@abstractmethod\ndef add_nodes(self, nodes):\n    \"\"\"Add nodes to the in-memory knowledge graph.\n\n    Args:\n        nodes (Iterable[BioCypherNode]): Iterable of BioCypherNode objects.\n    \"\"\"\n    raise NotImplementedError(\"InMemoryKG implementation must override 'add_nodes'\")\n</code></pre>"},{"location":"reference/source/output-in-memory/#biocypher.output.in_memory._in_memory_kg._InMemoryKG.get_kg","title":"<code>get_kg()</code>  <code>abstractmethod</code>","text":"<p>Return the in-memory knowledge graph.</p> Source code in <code>biocypher/output/in_memory/_in_memory_kg.py</code> <pre><code>@abstractmethod\ndef get_kg(self):\n    \"\"\"Return the in-memory knowledge graph.\"\"\"\n    raise NotImplementedError(\"InMemoryKG implementation must override 'get_kg'\")\n</code></pre>"},{"location":"reference/source/output-in-memory/#pandas-knowledge-graph","title":"Pandas Knowledge Graph","text":"<p>               Bases: <code>_InMemoryKG</code></p> Source code in <code>biocypher/output/in_memory/_pandas.py</code> <pre><code>class PandasKG(_InMemoryKG):\n    def __init__(self, deduplicator):\n        super().__init__()  # keeping in spite of ABC not having __init__\n        self.deduplicator = deduplicator\n\n        self.dfs = {}\n\n    def get_kg(self):\n        return self.dfs\n\n    def add_nodes(self, nodes):\n        self.add_tables(nodes)\n\n    def add_edges(self, edges):\n        self.add_tables(edges)\n\n    def add_tables(self, entities):\n        \"\"\"Add Pandas dataframes for each node and edge type in the input.\"\"\"\n        lists = self._separate_entity_types(entities)\n\n        for _type, _entities in lists.items():\n            self._add_entity_df(_type, _entities)\n\n    def _add_entity_df(self, _type, _entities):\n        df = pd.DataFrame(pd.json_normalize([node.get_dict() for node in _entities]))\n        # replace \"properties.\" with \"\" in column names\n        df.columns = [col.replace(\"properties.\", \"\") for col in df.columns]\n        if _type not in self.dfs:\n            self.dfs[_type] = df\n        else:\n            self.dfs[_type] = pd.concat([self.dfs[_type], df], ignore_index=True)\n        return self.dfs[_type]\n</code></pre>"},{"location":"reference/source/output-in-memory/#biocypher.output.in_memory._pandas.PandasKG.add_tables","title":"<code>add_tables(entities)</code>","text":"<p>Add Pandas dataframes for each node and edge type in the input.</p> Source code in <code>biocypher/output/in_memory/_pandas.py</code> <pre><code>def add_tables(self, entities):\n    \"\"\"Add Pandas dataframes for each node and edge type in the input.\"\"\"\n    lists = self._separate_entity_types(entities)\n\n    for _type, _entities in lists.items():\n        self._add_entity_df(_type, _entities)\n</code></pre>"},{"location":"reference/source/output-in-memory/#networkx-knowledge-graph","title":"NetworkX Knowledge Graph","text":"<p>               Bases: <code>_InMemoryKG</code></p> Source code in <code>biocypher/output/in_memory/_networkx.py</code> <pre><code>class NetworkxKG(_InMemoryKG):\n    def __init__(self, deduplicator):\n        super().__init__()  # keeping in spite of ABC not having __init__\n        self.deduplicator = deduplicator\n        self._pd = PandasKG(\n            deduplicator=self.deduplicator,\n        )\n        self.KG = None\n\n    def get_kg(self):\n        if not self.KG:\n            self.KG = self._create_networkx_kg()\n        return self.KG\n\n    def add_nodes(self, nodes):\n        self._pd.add_nodes(nodes)\n        return True\n\n    def add_edges(self, edges):\n        self._pd.add_edges(edges)\n        return True\n\n    def _create_networkx_kg(self) -&gt; nx.DiGraph:\n        self.KG = nx.DiGraph()\n        all_dfs = self._pd.dfs\n        node_dfs = [df for df in all_dfs.values() if df.columns.str.contains(\"node_id\").any()]\n        edge_dfs = [\n            df\n            for df in all_dfs.values()\n            if df.columns.str.contains(\"source_id\").any() and df.columns.str.contains(\"target_id\").any()\n        ]\n        for df in node_dfs:\n            nodes = df.set_index(\"node_id\").to_dict(orient=\"index\")\n            self.KG.add_nodes_from(nodes.items())\n        for df in edge_dfs:\n            edges = df.set_index([\"source_id\", \"target_id\"]).to_dict(orient=\"index\")\n            self.KG.add_edges_from(((source, target, attrs) for (source, target), attrs in edges.items()))\n        return self.KG\n</code></pre>"},{"location":"reference/source/output-write/","title":"Output Writing","text":""},{"location":"reference/source/output-write/#writer-retrieval","title":"Writer Retrieval","text":"<p>Return the writer class based on the selection in the config file.</p> <pre><code>dbms: the database management system; for options, see DBMS_TO_CLASS.\ntranslator: the Translator object.\ndeduplicator: the Deduplicator object.\noutput_directory: the directory to output.write the output files to.\nstrict_mode: whether to use strict mode.\n</code></pre> <pre><code>instance: an instance of the selected writer class.\n</code></pre> Source code in <code>biocypher/output/write/_get_writer.py</code> <pre><code>def get_writer(\n    dbms: str,\n    translator: \"Translator\",\n    deduplicator: \"Deduplicator\",\n    output_directory: str,\n    strict_mode: bool,\n) -&gt; _BatchWriter | None:\n    \"\"\"Return the writer class based on the selection in the config file.\n\n    Args:\n    ----\n        dbms: the database management system; for options, see DBMS_TO_CLASS.\n        translator: the Translator object.\n        deduplicator: the Deduplicator object.\n        output_directory: the directory to output.write the output files to.\n        strict_mode: whether to use strict mode.\n\n    Returns:\n    -------\n        instance: an instance of the selected writer class.\n\n    \"\"\"\n    dbms_config = _config(dbms) or {}\n\n    writer = DBMS_TO_CLASS[dbms]\n\n    if \"rdf_format\" in dbms_config:\n        logger.warning(\"The 'rdf_format' config option is deprecated, use 'file_format' instead.\")\n        if \"file_format\" not in dbms_config:\n            format = dbms_config[\"rdf_format\"]\n            logger.warning(f\"I will set 'file_format: {format}' for you.\")\n            dbms_config[\"file_format\"] = format\n            dbms_config.pop(\"rdf_format\")\n        logger.warning(\"NOTE: this warning will become an error in next versions.\")\n\n    if not writer:\n        msg = f\"Unknown dbms: {dbms}\"\n        raise ValueError(msg)\n\n    if writer is not None:\n        return writer(\n            translator=translator,\n            deduplicator=deduplicator,\n            delimiter=dbms_config.get(\"delimiter\"),\n            array_delimiter=dbms_config.get(\"array_delimiter\"),\n            quote=dbms_config.get(\"quote_character\"),\n            output_directory=output_directory,\n            db_name=dbms_config.get(\"database_name\"),\n            import_call_bin_prefix=dbms_config.get(\"import_call_bin_prefix\"),\n            import_call_file_prefix=dbms_config.get(\"import_call_file_prefix\"),\n            wipe=dbms_config.get(\"wipe\"),\n            strict_mode=strict_mode,\n            skip_bad_relationships=dbms_config.get(\"skip_bad_relationships\"),  # neo4j\n            skip_duplicate_nodes=dbms_config.get(\"skip_duplicate_nodes\"),  # neo4j\n            db_user=dbms_config.get(\"user\"),  # psql\n            db_password=dbms_config.get(\"password\"),  # psql\n            db_port=dbms_config.get(\"port\"),  # psql\n            file_format=dbms_config.get(\"file_format\"),  # rdf, owl\n            rdf_namespaces=dbms_config.get(\"rdf_namespaces\"),  # rdf, owl\n            edge_model=dbms_config.get(\"edge_model\"),  # owl\n        )\n    return None\n</code></pre>"},{"location":"reference/source/output-write/#writer-base-class","title":"Writer Base Class","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class for writing node and edge representations to disk. Specifics of the different writers (e.g. neo4j, postgresql, csv, etc.) are implemented in the child classes. Any concrete writer needs to implement at least: - _write_node_data - _write_edge_data - _construct_import_call - _get_import_script_name</p> <pre><code>translator (Translator): Instance of :py:class:`Translator` to enable translation of\n    nodes and manipulation of properties.\ndeduplicator (Deduplicator): Instance of :py:class:`Deduplicator` to enable deduplication\n    of nodes and edges.\noutput_directory (str, optional): Path for exporting CSV files. Defaults to None.\nstrict_mode (bool, optional): Whether to enforce source, version, and license properties. Defaults to False.\n</code></pre> <pre><code>NotImplementedError: Writer implementation must override '_write_node_data'\nNotImplementedError: Writer implementation must override '_write_edge_data'\nNotImplementedError: Writer implementation must override '_construct_import_call'\nNotImplementedError: Writer implementation must override '_get_import_script_name'\n</code></pre> Source code in <code>biocypher/output/write/_writer.py</code> <pre><code>class _Writer(ABC):\n    \"\"\"Abstract class for writing node and edge representations to disk.\n    Specifics of the different writers (e.g. neo4j, postgresql, csv, etc.)\n    are implemented in the child classes. Any concrete writer needs to\n    implement at least:\n    - _write_node_data\n    - _write_edge_data\n    - _construct_import_call\n    - _get_import_script_name\n\n    Args:\n    ----\n        translator (Translator): Instance of :py:class:`Translator` to enable translation of\n            nodes and manipulation of properties.\n        deduplicator (Deduplicator): Instance of :py:class:`Deduplicator` to enable deduplication\n            of nodes and edges.\n        output_directory (str, optional): Path for exporting CSV files. Defaults to None.\n        strict_mode (bool, optional): Whether to enforce source, version, and license properties. Defaults to False.\n\n    Raises:\n    ------\n        NotImplementedError: Writer implementation must override '_write_node_data'\n        NotImplementedError: Writer implementation must override '_write_edge_data'\n        NotImplementedError: Writer implementation must override '_construct_import_call'\n        NotImplementedError: Writer implementation must override '_get_import_script_name'\n\n    \"\"\"\n\n    def __init__(\n        self,\n        translator: Translator,\n        deduplicator: Deduplicator,\n        output_directory: str | None = None,\n        strict_mode: bool = False,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"Abstract class for writing node and edge representations to disk.\n\n        Args:\n        ----\n            translator (Translator): Instance of :py:class:`Translator` to enable translation of\n                nodes and manipulation of properties.\n            deduplicator (Deduplicator): Instance of :py:class:`Deduplicator` to enable deduplication\n                of nodes and edges.\n            output_directory (str, optional): Path for exporting CSV files. Defaults to None.\n            strict_mode (bool, optional): Whether to enforce source, version, and license properties. Defaults to False.\n\n        \"\"\"\n        self.translator = translator\n        self.deduplicator = deduplicator\n        self.strict_mode = strict_mode\n        self.output_directory = output_directory\n\n        if os.path.exists(self.output_directory):\n            if kwargs.get(\"write_to_file\", True):\n                logger.warning(\n                    f\"Output directory `{self.output_directory}` already exists. \"\n                    \"If this is not planned, file consistency may be compromised.\",\n                )\n        else:\n            logger.info(f\"Creating output directory `{self.output_directory}`.\")\n            os.makedirs(self.output_directory)\n\n    @abstractmethod\n    def _write_node_data(\n        self,\n        nodes: Iterable[BioCypherNode | BioCypherEdge | BioCypherRelAsNode],\n    ) -&gt; bool:\n        \"\"\"Implement how to output.write nodes to disk.\n\n        Args:\n        ----\n            nodes (Iterable): An iterable of BioCypherNode / BioCypherEdge / BioCypherRelAsNode objects.\n\n        Returns:\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        \"\"\"\n        raise NotImplementedError(\"Writer implementation must override 'write_nodes'\")\n\n    @abstractmethod\n    def _write_edge_data(\n        self,\n        edges: Iterable[BioCypherNode | BioCypherEdge | BioCypherRelAsNode],\n    ) -&gt; bool:\n        \"\"\"Implement how to output.write edges to disk.\n\n        Args:\n        ----\n            edges (Iterable): An iterable of BioCypherNode / BioCypherEdge / BioCypherRelAsNode objects.\n\n        Returns:\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        \"\"\"\n        raise NotImplementedError(\"Writer implementation must override 'write_edges'\")\n\n    @abstractmethod\n    def _construct_import_call(self) -&gt; str:\n        \"\"\"Function to construct the import call detailing folder and\n        individual node and edge headers and data files, as well as\n        delimiters and database name. Built after all data has been\n        processed to ensure that nodes are called before any edges.\n\n        Returns\n        -------\n            str: command for importing the output files into a DBMS.\n\n        \"\"\"\n        raise NotImplementedError(\"Writer implementation must override '_construct_import_call'\")\n\n    @abstractmethod\n    def _get_import_script_name(self) -&gt; str:\n        \"\"\"Returns the name of the import script.\n\n        Returns\n        -------\n            str: The name of the import script (ending in .sh)\n\n        \"\"\"\n        raise NotImplementedError(\"Writer implementation must override '_get_import_script_name'\")\n\n    def write_nodes(self, nodes, batch_size: int = int(1e6), force: bool = False):\n        \"\"\"Wrapper for writing nodes.\n\n        Args:\n        ----\n            nodes (BioCypherNode): a list or generator of nodes in\n                :py:class:`BioCypherNode` format\n            batch_size (int): The batch size for writing nodes.\n            force (bool): Whether to force writing nodes even if their type is\n                not present in the schema.\n\n        Returns:\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        \"\"\"\n        passed = self._write_node_data(nodes)\n        if not passed:\n            logger.error(\"Error while writing node data.\")\n            return False\n        return True\n\n    def write_edges(self, edges, batch_size: int = int(1e6), force: bool = False):\n        \"\"\"Wrapper for writing edges.\n\n        Args:\n        ----\n            nodes (BioCypherNode): a list or generator of nodes in\n                :py:class:`BioCypherNode` format\n            batch_size (int): The batch size for writing nodes.\n            force (bool): Whether to force writing nodes even if their type is\n                not present in the schema.\n\n        Returns:\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        \"\"\"\n        passed = self._write_edge_data(edges)\n        if not passed:\n            logger.error(\"Error while writing edge data.\")\n            return False\n        return True\n\n    def write_import_call(self):\n        \"\"\"Function to output.write the import call detailing folder and\n        individual node and edge headers and data files, as well as\n        delimiters and database name, to the export folder as txt.\n\n        Returns\n        -------\n            str: The path of the file holding the import call.\n\n        \"\"\"\n        file_path = os.path.join(self.output_directory, self._get_import_script_name())\n        logger.info(f\"Writing {self.__class__.__name__} import call to `{file_path}`.\")\n\n        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n            f.write(self._construct_import_call())\n\n        return file_path\n</code></pre>"},{"location":"reference/source/output-write/#biocypher.output.write._writer._Writer.__init__","title":"<code>__init__(translator, deduplicator, output_directory=None, strict_mode=False, *args, **kwargs)</code>","text":"<p>Abstract class for writing node and edge representations to disk.</p> <pre><code>translator (Translator): Instance of :py:class:`Translator` to enable translation of\n    nodes and manipulation of properties.\ndeduplicator (Deduplicator): Instance of :py:class:`Deduplicator` to enable deduplication\n    of nodes and edges.\noutput_directory (str, optional): Path for exporting CSV files. Defaults to None.\nstrict_mode (bool, optional): Whether to enforce source, version, and license properties. Defaults to False.\n</code></pre> Source code in <code>biocypher/output/write/_writer.py</code> <pre><code>def __init__(\n    self,\n    translator: Translator,\n    deduplicator: Deduplicator,\n    output_directory: str | None = None,\n    strict_mode: bool = False,\n    *args,\n    **kwargs,\n):\n    \"\"\"Abstract class for writing node and edge representations to disk.\n\n    Args:\n    ----\n        translator (Translator): Instance of :py:class:`Translator` to enable translation of\n            nodes and manipulation of properties.\n        deduplicator (Deduplicator): Instance of :py:class:`Deduplicator` to enable deduplication\n            of nodes and edges.\n        output_directory (str, optional): Path for exporting CSV files. Defaults to None.\n        strict_mode (bool, optional): Whether to enforce source, version, and license properties. Defaults to False.\n\n    \"\"\"\n    self.translator = translator\n    self.deduplicator = deduplicator\n    self.strict_mode = strict_mode\n    self.output_directory = output_directory\n\n    if os.path.exists(self.output_directory):\n        if kwargs.get(\"write_to_file\", True):\n            logger.warning(\n                f\"Output directory `{self.output_directory}` already exists. \"\n                \"If this is not planned, file consistency may be compromised.\",\n            )\n    else:\n        logger.info(f\"Creating output directory `{self.output_directory}`.\")\n        os.makedirs(self.output_directory)\n</code></pre>"},{"location":"reference/source/output-write/#biocypher.output.write._writer._Writer.write_edges","title":"<code>write_edges(edges, batch_size=int(1000000.0), force=False)</code>","text":"<p>Wrapper for writing edges.</p> <pre><code>nodes (BioCypherNode): a list or generator of nodes in\n    :py:class:`BioCypherNode` format\nbatch_size (int): The batch size for writing nodes.\nforce (bool): Whether to force writing nodes even if their type is\n    not present in the schema.\n</code></pre> <pre><code>bool: The return value. True for success, False otherwise.\n</code></pre> Source code in <code>biocypher/output/write/_writer.py</code> <pre><code>def write_edges(self, edges, batch_size: int = int(1e6), force: bool = False):\n    \"\"\"Wrapper for writing edges.\n\n    Args:\n    ----\n        nodes (BioCypherNode): a list or generator of nodes in\n            :py:class:`BioCypherNode` format\n        batch_size (int): The batch size for writing nodes.\n        force (bool): Whether to force writing nodes even if their type is\n            not present in the schema.\n\n    Returns:\n    -------\n        bool: The return value. True for success, False otherwise.\n\n    \"\"\"\n    passed = self._write_edge_data(edges)\n    if not passed:\n        logger.error(\"Error while writing edge data.\")\n        return False\n    return True\n</code></pre>"},{"location":"reference/source/output-write/#biocypher.output.write._writer._Writer.write_import_call","title":"<code>write_import_call()</code>","text":"<p>Function to output.write the import call detailing folder and individual node and edge headers and data files, as well as delimiters and database name, to the export folder as txt.</p>"},{"location":"reference/source/output-write/#biocypher.output.write._writer._Writer.write_import_call--returns","title":"Returns","text":"<pre><code>str: The path of the file holding the import call.\n</code></pre> Source code in <code>biocypher/output/write/_writer.py</code> <pre><code>def write_import_call(self):\n    \"\"\"Function to output.write the import call detailing folder and\n    individual node and edge headers and data files, as well as\n    delimiters and database name, to the export folder as txt.\n\n    Returns\n    -------\n        str: The path of the file holding the import call.\n\n    \"\"\"\n    file_path = os.path.join(self.output_directory, self._get_import_script_name())\n    logger.info(f\"Writing {self.__class__.__name__} import call to `{file_path}`.\")\n\n    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(self._construct_import_call())\n\n    return file_path\n</code></pre>"},{"location":"reference/source/output-write/#biocypher.output.write._writer._Writer.write_nodes","title":"<code>write_nodes(nodes, batch_size=int(1000000.0), force=False)</code>","text":"<p>Wrapper for writing nodes.</p> <pre><code>nodes (BioCypherNode): a list or generator of nodes in\n    :py:class:`BioCypherNode` format\nbatch_size (int): The batch size for writing nodes.\nforce (bool): Whether to force writing nodes even if their type is\n    not present in the schema.\n</code></pre> <pre><code>bool: The return value. True for success, False otherwise.\n</code></pre> Source code in <code>biocypher/output/write/_writer.py</code> <pre><code>def write_nodes(self, nodes, batch_size: int = int(1e6), force: bool = False):\n    \"\"\"Wrapper for writing nodes.\n\n    Args:\n    ----\n        nodes (BioCypherNode): a list or generator of nodes in\n            :py:class:`BioCypherNode` format\n        batch_size (int): The batch size for writing nodes.\n        force (bool): Whether to force writing nodes even if their type is\n            not present in the schema.\n\n    Returns:\n    -------\n        bool: The return value. True for success, False otherwise.\n\n    \"\"\"\n    passed = self._write_node_data(nodes)\n    if not passed:\n        logger.error(\"Error while writing node data.\")\n        return False\n    return True\n</code></pre>"},{"location":"reference/source/output-write/#batch-writer-base-class","title":"Batch Writer Base Class","text":"<p>               Bases: <code>_Writer</code>, <code>ABC</code></p> <p>Abstract batch writer class.</p> Source code in <code>biocypher/output/write/_batch_writer.py</code> <pre><code>class _BatchWriter(_Writer, ABC):\n    \"\"\"Abstract batch writer class.\"\"\"\n\n    @abstractmethod\n    def _quote_string(self, value: str) -&gt; str:\n        \"\"\"Quote a string.\n\n        Escaping is handled by the database-specific writer.\n        \"\"\"\n        msg = \"Database writer must override '_quote_string'\"\n        logger.error(msg)\n        raise NotImplementedError(msg)\n\n    @abstractmethod\n    def _get_default_import_call_bin_prefix(self):\n        \"\"\"Provide the default string for the import call bin prefix.\n\n        Returns\n        -------\n            str: The database-specific string for the path to the import call bin prefix\n\n        \"\"\"\n        msg = \"Database writer must override '_get_default_import_call_bin_prefix'\"\n        logger.error(msg)\n        raise NotImplementedError(msg)\n\n    @abstractmethod\n    def _write_array_string(self, string_list):\n        \"\"\"Write the string representation of an array into a .csv file.\n\n        Different databases require different formats of array to optimize\n        import speed.\n\n        Args:\n        ----\n            string_list (list): list of ontology strings\n\n        Returns:\n        -------\n            str: The database-specific string representation of an array\n\n        \"\"\"\n        msg = \"Database writer must override '_write_array_string'\"\n        logger.error(msg)\n        raise NotImplementedError(msg)\n\n    @abstractmethod\n    def _write_node_headers(self):\n        \"\"\"Write header files for nodes.\n\n        Write header files (node properties) for nodes as per the\n        definition in the `schema_config.yaml`.\n\n        Returns\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        \"\"\"\n        msg = \"Database writer must override '_write_node_headers'\"\n        logger.error(msg)\n        raise NotImplementedError(msg)\n\n    @abstractmethod\n    def _write_edge_headers(self):\n        \"\"\"Write a database import-file for an edge.\n\n        Write a database import-file for an edge as per the definition in\n        the `schema_config.yaml`, containing only the header for this type\n        of edge.\n\n        Returns\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        \"\"\"\n        msg = \"Database writer must override '_write_edge_headers'\"\n        logger.error(msg)\n        raise NotImplementedError(msg)\n\n    @abstractmethod\n    def _construct_import_call(self) -&gt; str:\n        \"\"\"Construct the import call.\n\n        Construct the import call detailing folder and individual node and\n        edge headers and data files, as well as delimiters and database name.\n        Built after all data has been processed to ensure that nodes are\n        called before any edges.\n\n        Returns\n        -------\n            str: A bash command for csv import.\n\n        \"\"\"\n        msg = \"Database writer must override '_construct_import_call'\"\n        logger.error(msg)\n        raise NotImplementedError(msg)\n\n    @abstractmethod\n    def _get_import_script_name(self) -&gt; str:\n        \"\"\"Return the name of the import script.\n\n        The name will be chosen based on the used database.\n\n        Returns\n        -------\n            str: The name of the import script (ending in .sh)\n\n        \"\"\"\n        msg = \"Database writer must override '_get_import_script_name'\"\n        logger.error(msg)\n        raise NotImplementedError(msg)\n\n    def __init__(\n        self,\n        translator: \"Translator\",\n        deduplicator: \"Deduplicator\",\n        delimiter: str,\n        array_delimiter: str = \",\",\n        quote: str = '\"',\n        output_directory: str | None = None,\n        db_name: str = \"neo4j\",\n        import_call_bin_prefix: str | None = None,\n        import_call_file_prefix: str | None = None,\n        wipe: bool = True,\n        strict_mode: bool = False,\n        skip_bad_relationships: bool = False,\n        skip_duplicate_nodes: bool = False,\n        db_user: str = None,\n        db_password: str = None,\n        db_host: str = None,\n        db_port: str = None,\n        file_format: str = None,\n        rdf_namespaces: dict = {},\n        labels_order: str = \"Ascending\",\n        **kwargs,\n    ):\n        \"\"\"Write node and edge representations to disk.\n\n        Abstract parent class for writing node and edge representations to disk\n        using the format specified by each database type. The database-specific\n        functions are implemented by the respective child-classes. This abstract\n        class contains all methods expected by a bach writer instance, some of\n        which need to be overwritten by the child classes.\n\n        Each batch writer instance has a fixed representation that needs to be\n        passed at instantiation via the :py:attr:`schema` argument. The instance\n        also expects an ontology adapter via :py:attr:`ontology_adapter` to be\n        able to convert and extend the hierarchy.\n\n        Requires the following methods to be overwritten by database-specific\n        writer classes:\n\n            - _write_node_headers\n            - _write_edge_headers\n            - _construct_import_call\n            - _write_array_string\n            - _get_import_script_name\n\n        Args:\n        ----\n            translator:\n                Instance of :py:class:`Translator` to enable translation of\n                nodes and manipulation of properties.\n\n            deduplicator:\n                Instance of :py:class:`Deduplicator` to enable deduplication\n                of nodes and edges.\n\n            delimiter:\n                The delimiter to use for the CSV files.\n\n            array_delimiter:\n                The delimiter to use for array properties.\n\n            quote:\n                The quote character to use for the CSV files.\n\n            output_directory:\n                Path for exporting CSV files.\n\n            db_name:\n                Name of the database that will be used in the generated\n                commands.\n\n            import_call_bin_prefix:\n                Path prefix for the admin import call binary.\n\n            import_call_file_prefix:\n                Path prefix for the data files (headers and parts) in the import\n                call.\n\n            wipe:\n                Whether to force import (removing existing DB content).\n                    (Specific to Neo4j.)\n\n            strict_mode:\n                Whether to enforce source, version, and license properties.\n\n            skip_bad_relationships:\n                Whether to skip relationships that do not have a valid\n                start and end node. (Specific to Neo4j.)\n\n            skip_duplicate_nodes:\n                Whether to skip duplicate nodes. (Specific to Neo4j.)\n\n            db_user:\n                The database user.\n\n            db_password:\n                The database password.\n\n            db_host:\n                The database host. Defaults to localhost.\n\n            db_port:\n                The database port.\n\n            file_format:\n                The format of RDF.\n\n            rdf_namespaces:\n                The namespaces for RDF.\n\n            labels_order:\n                The order of labels, to reflect the hierarchy (or not).\n                Default: \"Ascending\" (from more specific to more generic).\n\n        \"\"\"\n        super().__init__(\n            translator=translator,\n            deduplicator=deduplicator,\n            output_directory=output_directory,\n            strict_mode=strict_mode,\n        )\n        self.db_name = db_name\n        self.db_user = db_user\n        self.db_password = db_password\n        self.db_host = db_host or \"localhost\"\n        self.db_port = db_port\n        self.file_format = file_format\n        self.rdf_namespaces = rdf_namespaces\n\n        self.delim, self.escaped_delim = self._process_delimiter(delimiter)\n        self.adelim, self.escaped_adelim = self._process_delimiter(array_delimiter)\n        self.quote = quote\n        self.skip_bad_relationships = skip_bad_relationships\n        self.skip_duplicate_nodes = skip_duplicate_nodes\n\n        if import_call_bin_prefix is None:\n            self.import_call_bin_prefix = self._get_default_import_call_bin_prefix()\n        else:\n            self.import_call_bin_prefix = import_call_bin_prefix\n\n        self.wipe = wipe\n        self.strict_mode = strict_mode\n\n        self.translator = translator\n        self.deduplicator = deduplicator\n        self.node_property_dict = {}\n        self.edge_property_dict = {}\n        self.import_call_nodes = set()\n        self.import_call_edges = set()\n\n        self.outdir = output_directory\n\n        self._import_call_file_prefix = import_call_file_prefix\n\n        self.parts = {}  # dict to store the paths of part files for each label\n\n        self._labels_orders = [\"Alphabetical\", \"Ascending\", \"Descending\", \"Leaves\"]\n        if labels_order not in self._labels_orders:\n            msg = (\n                f\"neo4j's 'labels_order' parameter cannot be '{labels_order}',\"\n                \"must be one of: {' ,'.join(self._labels_orders)}\",\n            )\n            raise ValueError(msg)\n        self.labels_order = labels_order\n\n        # TODO not memory efficient, but should be fine for most cases; is\n        # there a more elegant solution?\n\n    @property\n    def import_call_file_prefix(self):\n        \"\"\"Property for output directory path.\"\"\"\n        if self._import_call_file_prefix is None:\n            return self.outdir\n        else:\n            return self._import_call_file_prefix\n\n    def _process_delimiter(self, delimiter: str) -&gt; str:\n        \"\"\"Process a delimited to escape correctly.\n\n        Args:\n        ----\n            delimiter (str): The delimiter to process.\n\n        Returns:\n        -------\n            tuple: The delimiter and its escaped representation.\n\n        \"\"\"\n        if delimiter == \"\\\\t\":\n            return \"\\t\", \"\\\\t\"\n\n        else:\n            return delimiter, delimiter\n\n    def write_nodes(self, nodes, batch_size: int = int(1e6), force: bool = False):\n        \"\"\"Write nodes and their headers.\n\n        Args:\n        ----\n            nodes (BioCypherNode): a list or generator of nodes in\n                :py:class:`BioCypherNode` format\n\n            batch_size (int): The batch size for writing nodes.\n\n            force (bool): Whether to force writing nodes even if their type is\n                not present in the schema.\n\n\n        Returns:\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        \"\"\"\n        # TODO check represented_as\n\n        # write node data\n        passed = self._write_node_data(nodes, batch_size, force)\n        if not passed:\n            logger.error(\"Error while writing node data.\")\n            return False\n        # pass property data to header writer per node type written\n        passed = self._write_node_headers()\n        if not passed:\n            logger.error(\"Error while writing node headers.\")\n            return False\n\n        return True\n\n    def write_edges(\n        self,\n        edges: list | GeneratorType,\n        batch_size: int = int(1e6),\n    ) -&gt; bool:\n        \"\"\"Write edges and their headers.\n\n        Args:\n        ----\n            edges (BioCypherEdge): a list or generator of edges in\n                :py:class:`BioCypherEdge` or :py:class:`BioCypherRelAsNode`\n                format\n\n        Returns:\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        \"\"\"\n        passed = False\n        edges = list(edges)  # force evaluation to handle empty generator\n        if edges:\n            nodes_flat = []\n            edges_flat = []\n            for edge in edges:\n                if isinstance(edge, BioCypherRelAsNode):\n                    # check if relationship has already been written, if so skip\n                    if self.deduplicator.rel_as_node_seen(edge):\n                        continue\n\n                    nodes_flat.append(edge.get_node())\n                    edges_flat.append(edge.get_source_edge())\n                    edges_flat.append(edge.get_target_edge())\n\n                else:\n                    # check if relationship has already been written, if so skip\n                    if self.deduplicator.edge_seen(edge):\n                        continue\n\n                    edges_flat.append(edge)\n\n            if nodes_flat and edges_flat:\n                passed = self.write_nodes(nodes_flat) and self._write_edge_data(\n                    edges_flat,\n                    batch_size,\n                )\n            else:\n                passed = self._write_edge_data(edges_flat, batch_size)\n\n        else:\n            # is this a problem? if the generator or list is empty, we\n            # don't write anything.\n            logger.debug(\n                \"No edges to write, possibly due to no matched Biolink classes.\",\n            )\n\n        if not passed:\n            logger.error(\"Error while writing edge data.\")\n            return False\n        # pass property data to header writer per edge type written\n        passed = self._write_edge_headers()\n        if not passed:\n            logger.error(\"Error while writing edge headers.\")\n            return False\n\n        return True\n\n    def _write_node_data(self, nodes, batch_size, force: bool = False):\n        \"\"\"Write biocypher nodes to CSV.\n\n        Conforms to the headers created with `_write_node_headers()`, and\n        is actually required to be run before calling `_write_node_headers()`\n        to set the :py:attr:`self.node_property_dict` for passing the node\n        properties to the instance. Expects list or generator of nodes from\n        the :py:class:`BioCypherNode` class.\n\n        Args:\n        ----\n            nodes (BioCypherNode): a list or generator of nodes in\n                :py:class:`BioCypherNode` format\n\n        Returns:\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        \"\"\"\n        if isinstance(nodes, GeneratorType | peekable):\n            logger.debug(\"Writing node CSV from generator.\")\n\n            bins = defaultdict(list)  # dict to store a list for each\n            # label that is passed in\n            bin_l = {}  # dict to store the length of each list for\n            # batching cutoff\n            reference_props = defaultdict(\n                dict,\n            )  # dict to store a dict of properties\n            # for each label to check for consistency and their type\n            # for now, relevant for `int`\n            labels = {}  # dict to store the additional labels for each\n            # primary graph constituent from biolink hierarchy\n            for node in nodes:\n                # check if node has already been written, if so skip\n                if self.deduplicator.node_seen(node):\n                    continue\n\n                _id = node.get_id()\n                label = node.get_label()\n\n                # check for non-id\n                if not _id:\n                    logger.warning(f\"Node {label} has no id; skipping.\")\n                    continue\n\n                if label not in bins.keys():\n                    # start new list\n                    all_labels = None\n                    bins[label].append(node)\n                    bin_l[label] = 1\n\n                    # get properties from config if present\n                    if label in self.translator.ontology.mapping.extended_schema:\n                        cprops = self.translator.ontology.mapping.extended_schema.get(label).get(\n                            \"properties\",\n                        )\n                    else:\n                        cprops = None\n                    if cprops:\n                        d = dict(cprops)\n\n                        # add id and preferred id to properties; these are\n                        # created in node creation (`_create.BioCypherNode`)\n                        d[\"id\"] = \"str\"\n                        d[\"preferred_id\"] = \"str\"\n\n                        # add strict mode properties\n                        if self.strict_mode:\n                            d[\"source\"] = \"str\"\n                            d[\"version\"] = \"str\"\n                            d[\"licence\"] = \"str\"\n\n                    else:\n                        d = dict(node.get_properties())\n                        # encode property type\n                        for k, v in d.items():\n                            if d[k] is not None:\n                                d[k] = type(v).__name__\n                    # else use first encountered node to define properties for\n                    # checking; could later be by checking all nodes but much\n                    # more complicated, particularly involving batch writing\n                    # (would require \"do-overs\"). for now, we output a warning\n                    # if node properties diverge from reference properties (in\n                    # write_single_node_list_to_file) TODO if it occurs, ask\n                    # user to select desired properties and restart the process\n\n                    reference_props[label] = d\n\n                    # get label hierarchy\n                    # multiple labels:\n                    if not force:\n                        all_labels = self.translator.ontology.get_ancestors(label)\n                    else:\n                        all_labels = None\n\n                    if all_labels:\n                        # convert to pascal case\n                        all_labels = [self.translator.name_sentence_to_pascal(label) for label in all_labels]\n                        # remove duplicates\n                        all_labels = list(OrderedDict.fromkeys(all_labels))\n                        match self.labels_order:\n                            case \"Ascending\":\n                                pass  # Default from get_ancestors.\n                            case \"Alphabetical\":\n                                all_labels.sort()\n                            case \"Descending\":\n                                all_labels.reverse()\n                            case \"Leaves\":\n                                if len(all_labels) &lt; 1:\n                                    msg = \"Labels list cannot be empty when using 'Leaves' order.\"\n                                    raise ValueError(msg)\n                                all_labels = [all_labels[0]]\n                            case _:\n                                # In case someone touched _label_orders after constructor.\n                                if self.labels_order not in self._labels_orders:\n                                    msg = (\n                                        f\"Invalid labels_order: {self.labels_order}. \"\n                                        f\"Must be one of {self._labels_orders}\"\n                                    )\n                                    raise ValueError(msg)\n                        # concatenate with array delimiter\n                        all_labels = self._write_array_string(all_labels)\n                    else:\n                        all_labels = self.translator.name_sentence_to_pascal(label)\n\n                    labels[label] = all_labels\n\n                else:\n                    # add to list\n                    bins[label].append(node)\n                    bin_l[label] += 1\n                    if not bin_l[label] &lt; batch_size:\n                        # batch size controlled here\n                        passed = self._write_single_node_list_to_file(\n                            bins[label],\n                            label,\n                            reference_props[label],\n                            labels[label],\n                        )\n\n                        if not passed:\n                            return False\n\n                        bins[label] = []\n                        bin_l[label] = 0\n\n            # after generator depleted, write remainder of bins\n            for label, nl in bins.items():\n                passed = self._write_single_node_list_to_file(\n                    nl,\n                    label,\n                    reference_props[label],\n                    labels[label],\n                )\n\n                if not passed:\n                    return False\n\n            # use complete bin list to write header files\n            # TODO if a node type has varying properties\n            # (ie missingness), we'd need to collect all possible\n            # properties in the generator pass\n\n            # save config or first-node properties to instance attribute\n            for label in reference_props.keys():\n                self.node_property_dict[label] = reference_props[label]\n\n            return True\n        elif not isinstance(nodes, list):\n            logger.error(\"Nodes must be passed as list or generator.\")\n            return False\n        else:\n\n            def gen(nodes):\n                yield from nodes\n\n            return self._write_node_data(gen(nodes), batch_size=batch_size)\n\n    def _write_single_node_list_to_file(\n        self,\n        node_list: list,\n        label: str,\n        prop_dict: dict,\n        labels: str,\n    ):\n        \"\"\"Write a list of biocypher nodes to a CSV file.\n\n        This function takes one list of biocypher nodes and writes them\n        to a Neo4j admin import compatible CSV file.\n\n        Args:\n        ----\n            node_list (list): list of BioCypherNodes to be written\n            label (str): the primary label of the node\n            prop_dict (dict): properties of node class passed from parsing\n                function and their types\n            labels (str): string of one or several concatenated labels\n                for the node class\n\n        Returns:\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        \"\"\"\n        if not all(isinstance(n, BioCypherNode) for n in node_list):\n            logger.error(\"Nodes must be passed as type BioCypherNode.\")\n            return False\n\n        # from list of nodes to list of strings\n        lines = []\n\n        for n in node_list:\n            # check for deviations in properties\n            # node properties\n            n_props = n.get_properties()\n            n_keys = list(n_props.keys())\n            # reference properties\n            ref_props = list(prop_dict.keys())\n\n            # compare lists order invariant\n            if set(ref_props) != set(n_keys):\n                onode = n.get_id()\n                oprop1 = set(ref_props).difference(n_keys)\n                oprop2 = set(n_keys).difference(ref_props)\n                logger.error(\n                    f\"At least one node of the class {n.get_label()} \"\n                    f\"has more or fewer properties than another. \"\n                    f\"Offending node: {onode!r}, offending property: \"\n                    f\"{max([oprop1, oprop2])}. \"\n                    f\"All reference properties: {ref_props}, \"\n                    f\"All node properties: {n_keys}.\",\n                )\n                return False\n\n            line = [n.get_id()]\n\n            if ref_props:\n                plist = []\n                # make all into strings, put actual strings in quotes\n                for k, v in prop_dict.items():\n                    p = n_props.get(k)\n                    if p is None:  # TODO make field empty instead of \"\"?\n                        plist.append(\"\")\n                    elif v in [\n                        \"int\",\n                        \"integer\",\n                        \"long\",\n                        \"float\",\n                        \"double\",\n                        \"dbl\",\n                        \"bool\",\n                        \"boolean\",\n                    ]:\n                        plist.append(str(p))\n                    elif isinstance(p, list):\n                        plist.append(self._write_array_string(p))\n                    else:\n                        plist.append(f\"{self.quote}{p!s}{self.quote}\")\n\n                line.append(self.delim.join(plist))\n            line.append(labels)\n\n            lines.append(self.delim.join(line) + \"\\n\")\n\n        # avoid writing empty files\n        if lines:\n            self._write_next_part(label, lines)\n\n        return True\n\n    def _write_edge_data(self, edges, batch_size):\n        \"\"\"Write biocypher edges to CSV.\n\n        Writes biocypher edges to CSV conforming to the headers created\n        with `_write_edge_headers()`, and is actually required to be run\n        before calling `_write_node_headers()` to set the\n        :py:attr:`self.edge_property_dict` for passing the edge\n        properties to the instance. Expects list or generator of edges\n        from the :py:class:`BioCypherEdge` class.\n\n        Args:\n        ----\n            edges (BioCypherEdge): a list or generator of edges in\n                :py:class:`BioCypherEdge` format\n\n        Returns:\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        Todo:\n        ----\n            - currently works for mixed edges but in practice often is\n              called on one iterable containing one type of edge only\n\n        \"\"\"\n        if isinstance(edges, GeneratorType):\n            logger.debug(\"Writing edge CSV from generator.\")\n\n            bins = defaultdict(list)  # dict to store a list for each\n            # label that is passed in\n            bin_l = {}  # dict to store the length of each list for\n            # batching cutoff\n            reference_props = defaultdict(\n                dict,\n            )  # dict to store a dict of properties\n            # for each label to check for consistency and their type\n            # for now, relevant for `int`\n            for edge in edges:\n                if not (edge.get_source_id() and edge.get_target_id()):\n                    logger.error(\n                        f\"Edge must have source and target node. Caused by: {edge}\",\n                    )\n                    continue\n\n                label = edge.get_label()\n\n                if label not in bins.keys():\n                    # start new list\n                    bins[label].append(edge)\n                    bin_l[label] = 1\n\n                    # get properties from config if present\n\n                    # check whether label is in ontology_adapter.leaves\n                    # (may not be if it is an edge that carries the\n                    # \"label_as_edge\" property)\n                    cprops = None\n                    if label in self.translator.ontology.mapping.extended_schema:\n                        cprops = self.translator.ontology.mapping.extended_schema.get(label).get(\n                            \"properties\",\n                        )\n                    else:\n                        # try via \"label_as_edge\"\n                        for (\n                            k,\n                            v,\n                        ) in self.translator.ontology.mapping.extended_schema.items():\n                            if isinstance(v, dict):\n                                if v.get(\"label_as_edge\") == label:\n                                    cprops = v.get(\"properties\")\n                                    break\n                    if cprops:\n                        d = cprops\n\n                        # add strict mode properties\n                        if self.strict_mode:\n                            d[\"source\"] = \"str\"\n                            d[\"version\"] = \"str\"\n                            d[\"licence\"] = \"str\"\n\n                    else:\n                        d = dict(edge.get_properties())\n                        # encode property type\n                        for k, v in d.items():\n                            if d[k] is not None:\n                                d[k] = type(v).__name__\n                    # else use first encountered edge to define\n                    # properties for checking; could later be by\n                    # checking all edges but much more complicated,\n                    # particularly involving batch writing (would\n                    # require \"do-overs\"). for now, we output a warning\n                    # if edge properties diverge from reference\n                    # properties (in write_single_edge_list_to_file)\n                    # TODO\n\n                    reference_props[label] = d\n\n                else:\n                    # add to list\n                    bins[label].append(edge)\n                    bin_l[label] += 1\n                    if not bin_l[label] &lt; batch_size:\n                        # batch size controlled here\n                        passed = self._write_single_edge_list_to_file(\n                            bins[label],\n                            label,\n                            reference_props[label],\n                        )\n\n                        if not passed:\n                            return False\n\n                        bins[label] = []\n                        bin_l[label] = 0\n\n            # after generator depleted, write remainder of bins\n            for label, nl in bins.items():\n                passed = self._write_single_edge_list_to_file(\n                    nl,\n                    label,\n                    reference_props[label],\n                )\n\n                if not passed:\n                    return False\n\n            # use complete bin list to write header files\n            # TODO if a edge type has varying properties\n            # (ie missingness), we'd need to collect all possible\n            # properties in the generator pass\n\n            # save first-edge properties to instance attribute\n            for label in reference_props.keys():\n                self.edge_property_dict[label] = reference_props[label]\n\n            return True\n        elif not isinstance(edges, list):\n            logger.error(\"Edges must be passed as list or generator.\")\n            return False\n        else:\n\n            def gen(edges):\n                yield from edges\n\n            return self._write_edge_data(gen(edges), batch_size=batch_size)\n\n    def _write_single_edge_list_to_file(\n        self,\n        edge_list: list,\n        label: str,\n        prop_dict: dict,\n    ):\n        \"\"\"Write a list of biocypher edges to a CSV file.\n\n        This function takes one list of biocypher edges and writes them\n        to a Neo4j admin import compatible CSV file.\n\n        Args:\n        ----\n            edge_list (list): list of BioCypherEdges to be written\n\n            label (str): the label (type) of the edge\n\n            prop_dict (dict): properties of node class passed from parsing\n                function and their types\n\n        Returns:\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        \"\"\"\n        if not all(isinstance(n, BioCypherEdge) for n in edge_list):\n            logger.error(\"Edges must be passed as type BioCypherEdge.\")\n            return False\n\n        # from list of edges to list of strings\n        lines = []\n        for e in edge_list:\n            # check for deviations in properties\n            # edge properties\n            e_props = e.get_properties()\n            e_keys = list(e_props.keys())\n            ref_props = list(prop_dict.keys())\n\n            # compare list order invariant\n            if set(ref_props) != set(e_keys):\n                oedge = f\"{e.get_source_id()}-{e.get_target_id()}\"\n                oprop1 = set(ref_props).difference(e_keys)\n                oprop2 = set(e_keys).difference(ref_props)\n                logger.error(\n                    f\"At least one edge of the class {e.get_label()} \"\n                    f\"has more or fewer properties than another. \"\n                    f\"Offending edge: {oedge!r}, offending property: \"\n                    f\"{max([oprop1, oprop2])}. \"\n                    f\"All reference properties: {ref_props}, \"\n                    f\"All edge properties: {e_keys}.\",\n                )\n                return False\n\n            plist = []\n            # make all into strings, put actual strings in quotes\n            for k, v in prop_dict.items():\n                p = e_props.get(k)\n                if p is None:  # TODO make field empty instead of \"\"?\n                    plist.append(\"\")\n                elif v in [\n                    \"int\",\n                    \"integer\",\n                    \"long\",\n                    \"float\",\n                    \"double\",\n                    \"dbl\",\n                    \"bool\",\n                    \"boolean\",\n                ]:\n                    plist.append(str(p))\n                elif isinstance(p, list):\n                    plist.append(self._write_array_string(p))\n                else:\n                    plist.append(self.quote + str(p) + self.quote)\n\n            entries = [e.get_source_id()]\n\n            skip_id = False\n            schema_label = None\n\n            if label in [\"IS_SOURCE_OF\", \"IS_TARGET_OF\", \"IS_PART_OF\"]:\n                skip_id = True\n            elif not self.translator.ontology.mapping.extended_schema.get(label):\n                # find label in schema by label_as_edge\n                for (\n                    k,\n                    v,\n                ) in self.translator.ontology.mapping.extended_schema.items():\n                    if v.get(\"label_as_edge\") == label:\n                        schema_label = k\n                        break\n            else:\n                schema_label = label\n\n            if schema_label:\n                if (\n                    self.translator.ontology.mapping.extended_schema.get(\n                        schema_label,\n                    ).get(\"use_id\")\n                    == False  # noqa: E712 (seems to not work with 'not')\n                ):\n                    skip_id = True\n\n            if not skip_id:\n                entries.append(e.get_id() or \"\")\n\n            if ref_props:\n                entries.append(self.delim.join(plist))\n\n            entries.append(e.get_target_id())\n            entries.append(\n                self.translator.name_sentence_to_pascal(\n                    e.get_label(),\n                ),\n            )\n\n            lines.append(\n                self.delim.join(entries) + \"\\n\",\n            )\n\n        # avoid writing empty files\n        if lines:\n            self._write_next_part(label, lines)\n\n        return True\n\n    def _write_next_part(self, label: str, lines: list):\n        \"\"\"Write a list of strings to a new part file.\n\n        Args:\n        ----\n            label (str): the label (type) of the edge; internal\n            representation sentence case -&gt; needs to become PascalCase\n            for disk representation\n\n            lines (list): list of strings to be written\n\n        Returns:\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        \"\"\"\n        # translate label to PascalCase\n        label_pascal = self.translator.name_sentence_to_pascal(parse_label(label))\n\n        # list files in self.outdir\n        files = glob.glob(os.path.join(self.outdir, f\"{label_pascal}-part*.csv\"))\n        # find file with highest part number\n        if not files:\n            next_part = 0\n\n        else:\n            next_part = (\n                max(\n                    [int(f.split(\".\")[-2].split(\"-\")[-1].replace(\"part\", \"\")) for f in files],\n                )\n                + 1\n            )\n\n        # write to file\n        padded_part = str(next_part).zfill(3)\n        logger.info(\n            f\"Writing {len(lines)} entries to {label_pascal}-part{padded_part}.csv\",\n        )\n\n        # store name only in case import_call_file_prefix is set\n        part = f\"{label_pascal}-part{padded_part}.csv\"\n        file_path = os.path.join(self.outdir, part)\n\n        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n            # concatenate with delimiter\n            f.writelines(lines)\n\n        if not self.parts.get(label):\n            self.parts[label] = [part]\n        else:\n            self.parts[label].append(part)\n\n    def get_import_call(self) -&gt; str:\n        \"\"\"Eeturn the import call.\n\n        Return the import call detailing folder and individual node and\n        edge headers and data files, as well as delimiters and database name.\n\n        Returns\n        -------\n            str: a bash command for the database import\n\n        \"\"\"\n        return self._construct_import_call()\n\n    def write_import_call(self) -&gt; str:\n        \"\"\"Write the import call.\n\n        Function to write the import call detailing folder and\n        individual node and edge headers and data files, as well as\n        delimiters and database name, to the export folder as txt.\n\n        Returns\n        -------\n            str: The path of the file holding the import call.\n\n        \"\"\"\n        file_path = os.path.join(self.outdir, self._get_import_script_name())\n        logger.info(f\"Writing {self.db_name + ' ' if self.db_name else ''}import call to `{file_path}`.\")\n\n        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n            f.write(self._construct_import_call())\n\n        return file_path\n</code></pre>"},{"location":"reference/source/output-write/#biocypher.output.write._batch_writer._BatchWriter.import_call_file_prefix","title":"<code>import_call_file_prefix</code>  <code>property</code>","text":"<p>Property for output directory path.</p>"},{"location":"reference/source/output-write/#biocypher.output.write._batch_writer._BatchWriter.__init__","title":"<code>__init__(translator, deduplicator, delimiter, array_delimiter=',', quote='\"', output_directory=None, db_name='neo4j', import_call_bin_prefix=None, import_call_file_prefix=None, wipe=True, strict_mode=False, skip_bad_relationships=False, skip_duplicate_nodes=False, db_user=None, db_password=None, db_host=None, db_port=None, file_format=None, rdf_namespaces={}, labels_order='Ascending', **kwargs)</code>","text":"<p>Write node and edge representations to disk.</p> <p>Abstract parent class for writing node and edge representations to disk using the format specified by each database type. The database-specific functions are implemented by the respective child-classes. This abstract class contains all methods expected by a bach writer instance, some of which need to be overwritten by the child classes.</p> <p>Each batch writer instance has a fixed representation that needs to be passed at instantiation via the attr:<code>schema</code> argument. The instance also expects an ontology adapter via attr:<code>ontology_adapter</code> to be able to convert and extend the hierarchy.</p> <p>Requires the following methods to be overwritten by database-specific writer classes:</p> <pre><code>- _write_node_headers\n- _write_edge_headers\n- _construct_import_call\n- _write_array_string\n- _get_import_script_name\n</code></pre> <pre><code>translator:\n    Instance of :py:class:`Translator` to enable translation of\n    nodes and manipulation of properties.\n\ndeduplicator:\n    Instance of :py:class:`Deduplicator` to enable deduplication\n    of nodes and edges.\n\ndelimiter:\n    The delimiter to use for the CSV files.\n\narray_delimiter:\n    The delimiter to use for array properties.\n\nquote:\n    The quote character to use for the CSV files.\n\noutput_directory:\n    Path for exporting CSV files.\n\ndb_name:\n    Name of the database that will be used in the generated\n    commands.\n\nimport_call_bin_prefix:\n    Path prefix for the admin import call binary.\n\nimport_call_file_prefix:\n    Path prefix for the data files (headers and parts) in the import\n    call.\n\nwipe:\n    Whether to force import (removing existing DB content).\n        (Specific to Neo4j.)\n\nstrict_mode:\n    Whether to enforce source, version, and license properties.\n\nskip_bad_relationships:\n    Whether to skip relationships that do not have a valid\n    start and end node. (Specific to Neo4j.)\n\nskip_duplicate_nodes:\n    Whether to skip duplicate nodes. (Specific to Neo4j.)\n\ndb_user:\n    The database user.\n\ndb_password:\n    The database password.\n\ndb_host:\n    The database host. Defaults to localhost.\n\ndb_port:\n    The database port.\n\nfile_format:\n    The format of RDF.\n\nrdf_namespaces:\n    The namespaces for RDF.\n\nlabels_order:\n    The order of labels, to reflect the hierarchy (or not).\n    Default: \"Ascending\" (from more specific to more generic).\n</code></pre> Source code in <code>biocypher/output/write/_batch_writer.py</code> <pre><code>def __init__(\n    self,\n    translator: \"Translator\",\n    deduplicator: \"Deduplicator\",\n    delimiter: str,\n    array_delimiter: str = \",\",\n    quote: str = '\"',\n    output_directory: str | None = None,\n    db_name: str = \"neo4j\",\n    import_call_bin_prefix: str | None = None,\n    import_call_file_prefix: str | None = None,\n    wipe: bool = True,\n    strict_mode: bool = False,\n    skip_bad_relationships: bool = False,\n    skip_duplicate_nodes: bool = False,\n    db_user: str = None,\n    db_password: str = None,\n    db_host: str = None,\n    db_port: str = None,\n    file_format: str = None,\n    rdf_namespaces: dict = {},\n    labels_order: str = \"Ascending\",\n    **kwargs,\n):\n    \"\"\"Write node and edge representations to disk.\n\n    Abstract parent class for writing node and edge representations to disk\n    using the format specified by each database type. The database-specific\n    functions are implemented by the respective child-classes. This abstract\n    class contains all methods expected by a bach writer instance, some of\n    which need to be overwritten by the child classes.\n\n    Each batch writer instance has a fixed representation that needs to be\n    passed at instantiation via the :py:attr:`schema` argument. The instance\n    also expects an ontology adapter via :py:attr:`ontology_adapter` to be\n    able to convert and extend the hierarchy.\n\n    Requires the following methods to be overwritten by database-specific\n    writer classes:\n\n        - _write_node_headers\n        - _write_edge_headers\n        - _construct_import_call\n        - _write_array_string\n        - _get_import_script_name\n\n    Args:\n    ----\n        translator:\n            Instance of :py:class:`Translator` to enable translation of\n            nodes and manipulation of properties.\n\n        deduplicator:\n            Instance of :py:class:`Deduplicator` to enable deduplication\n            of nodes and edges.\n\n        delimiter:\n            The delimiter to use for the CSV files.\n\n        array_delimiter:\n            The delimiter to use for array properties.\n\n        quote:\n            The quote character to use for the CSV files.\n\n        output_directory:\n            Path for exporting CSV files.\n\n        db_name:\n            Name of the database that will be used in the generated\n            commands.\n\n        import_call_bin_prefix:\n            Path prefix for the admin import call binary.\n\n        import_call_file_prefix:\n            Path prefix for the data files (headers and parts) in the import\n            call.\n\n        wipe:\n            Whether to force import (removing existing DB content).\n                (Specific to Neo4j.)\n\n        strict_mode:\n            Whether to enforce source, version, and license properties.\n\n        skip_bad_relationships:\n            Whether to skip relationships that do not have a valid\n            start and end node. (Specific to Neo4j.)\n\n        skip_duplicate_nodes:\n            Whether to skip duplicate nodes. (Specific to Neo4j.)\n\n        db_user:\n            The database user.\n\n        db_password:\n            The database password.\n\n        db_host:\n            The database host. Defaults to localhost.\n\n        db_port:\n            The database port.\n\n        file_format:\n            The format of RDF.\n\n        rdf_namespaces:\n            The namespaces for RDF.\n\n        labels_order:\n            The order of labels, to reflect the hierarchy (or not).\n            Default: \"Ascending\" (from more specific to more generic).\n\n    \"\"\"\n    super().__init__(\n        translator=translator,\n        deduplicator=deduplicator,\n        output_directory=output_directory,\n        strict_mode=strict_mode,\n    )\n    self.db_name = db_name\n    self.db_user = db_user\n    self.db_password = db_password\n    self.db_host = db_host or \"localhost\"\n    self.db_port = db_port\n    self.file_format = file_format\n    self.rdf_namespaces = rdf_namespaces\n\n    self.delim, self.escaped_delim = self._process_delimiter(delimiter)\n    self.adelim, self.escaped_adelim = self._process_delimiter(array_delimiter)\n    self.quote = quote\n    self.skip_bad_relationships = skip_bad_relationships\n    self.skip_duplicate_nodes = skip_duplicate_nodes\n\n    if import_call_bin_prefix is None:\n        self.import_call_bin_prefix = self._get_default_import_call_bin_prefix()\n    else:\n        self.import_call_bin_prefix = import_call_bin_prefix\n\n    self.wipe = wipe\n    self.strict_mode = strict_mode\n\n    self.translator = translator\n    self.deduplicator = deduplicator\n    self.node_property_dict = {}\n    self.edge_property_dict = {}\n    self.import_call_nodes = set()\n    self.import_call_edges = set()\n\n    self.outdir = output_directory\n\n    self._import_call_file_prefix = import_call_file_prefix\n\n    self.parts = {}  # dict to store the paths of part files for each label\n\n    self._labels_orders = [\"Alphabetical\", \"Ascending\", \"Descending\", \"Leaves\"]\n    if labels_order not in self._labels_orders:\n        msg = (\n            f\"neo4j's 'labels_order' parameter cannot be '{labels_order}',\"\n            \"must be one of: {' ,'.join(self._labels_orders)}\",\n        )\n        raise ValueError(msg)\n    self.labels_order = labels_order\n</code></pre>"},{"location":"reference/source/output-write/#biocypher.output.write._batch_writer._BatchWriter.get_import_call","title":"<code>get_import_call()</code>","text":"<p>Eeturn the import call.</p> <p>Return the import call detailing folder and individual node and edge headers and data files, as well as delimiters and database name.</p>"},{"location":"reference/source/output-write/#biocypher.output.write._batch_writer._BatchWriter.get_import_call--returns","title":"Returns","text":"<pre><code>str: a bash command for the database import\n</code></pre> Source code in <code>biocypher/output/write/_batch_writer.py</code> <pre><code>def get_import_call(self) -&gt; str:\n    \"\"\"Eeturn the import call.\n\n    Return the import call detailing folder and individual node and\n    edge headers and data files, as well as delimiters and database name.\n\n    Returns\n    -------\n        str: a bash command for the database import\n\n    \"\"\"\n    return self._construct_import_call()\n</code></pre>"},{"location":"reference/source/output-write/#biocypher.output.write._batch_writer._BatchWriter.write_edges","title":"<code>write_edges(edges, batch_size=int(1000000.0))</code>","text":"<p>Write edges and their headers.</p> <pre><code>edges (BioCypherEdge): a list or generator of edges in\n    :py:class:`BioCypherEdge` or :py:class:`BioCypherRelAsNode`\n    format\n</code></pre> <pre><code>bool: The return value. True for success, False otherwise.\n</code></pre> Source code in <code>biocypher/output/write/_batch_writer.py</code> <pre><code>def write_edges(\n    self,\n    edges: list | GeneratorType,\n    batch_size: int = int(1e6),\n) -&gt; bool:\n    \"\"\"Write edges and their headers.\n\n    Args:\n    ----\n        edges (BioCypherEdge): a list or generator of edges in\n            :py:class:`BioCypherEdge` or :py:class:`BioCypherRelAsNode`\n            format\n\n    Returns:\n    -------\n        bool: The return value. True for success, False otherwise.\n\n    \"\"\"\n    passed = False\n    edges = list(edges)  # force evaluation to handle empty generator\n    if edges:\n        nodes_flat = []\n        edges_flat = []\n        for edge in edges:\n            if isinstance(edge, BioCypherRelAsNode):\n                # check if relationship has already been written, if so skip\n                if self.deduplicator.rel_as_node_seen(edge):\n                    continue\n\n                nodes_flat.append(edge.get_node())\n                edges_flat.append(edge.get_source_edge())\n                edges_flat.append(edge.get_target_edge())\n\n            else:\n                # check if relationship has already been written, if so skip\n                if self.deduplicator.edge_seen(edge):\n                    continue\n\n                edges_flat.append(edge)\n\n        if nodes_flat and edges_flat:\n            passed = self.write_nodes(nodes_flat) and self._write_edge_data(\n                edges_flat,\n                batch_size,\n            )\n        else:\n            passed = self._write_edge_data(edges_flat, batch_size)\n\n    else:\n        # is this a problem? if the generator or list is empty, we\n        # don't write anything.\n        logger.debug(\n            \"No edges to write, possibly due to no matched Biolink classes.\",\n        )\n\n    if not passed:\n        logger.error(\"Error while writing edge data.\")\n        return False\n    # pass property data to header writer per edge type written\n    passed = self._write_edge_headers()\n    if not passed:\n        logger.error(\"Error while writing edge headers.\")\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/source/output-write/#biocypher.output.write._batch_writer._BatchWriter.write_import_call","title":"<code>write_import_call()</code>","text":"<p>Write the import call.</p> <p>Function to write the import call detailing folder and individual node and edge headers and data files, as well as delimiters and database name, to the export folder as txt.</p>"},{"location":"reference/source/output-write/#biocypher.output.write._batch_writer._BatchWriter.write_import_call--returns","title":"Returns","text":"<pre><code>str: The path of the file holding the import call.\n</code></pre> Source code in <code>biocypher/output/write/_batch_writer.py</code> <pre><code>def write_import_call(self) -&gt; str:\n    \"\"\"Write the import call.\n\n    Function to write the import call detailing folder and\n    individual node and edge headers and data files, as well as\n    delimiters and database name, to the export folder as txt.\n\n    Returns\n    -------\n        str: The path of the file holding the import call.\n\n    \"\"\"\n    file_path = os.path.join(self.outdir, self._get_import_script_name())\n    logger.info(f\"Writing {self.db_name + ' ' if self.db_name else ''}import call to `{file_path}`.\")\n\n    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(self._construct_import_call())\n\n    return file_path\n</code></pre>"},{"location":"reference/source/output-write/#biocypher.output.write._batch_writer._BatchWriter.write_nodes","title":"<code>write_nodes(nodes, batch_size=int(1000000.0), force=False)</code>","text":"<p>Write nodes and their headers.</p> <pre><code>nodes (BioCypherNode): a list or generator of nodes in\n    :py:class:`BioCypherNode` format\n\nbatch_size (int): The batch size for writing nodes.\n\nforce (bool): Whether to force writing nodes even if their type is\n    not present in the schema.\n</code></pre> <pre><code>bool: The return value. True for success, False otherwise.\n</code></pre> Source code in <code>biocypher/output/write/_batch_writer.py</code> <pre><code>def write_nodes(self, nodes, batch_size: int = int(1e6), force: bool = False):\n    \"\"\"Write nodes and their headers.\n\n    Args:\n    ----\n        nodes (BioCypherNode): a list or generator of nodes in\n            :py:class:`BioCypherNode` format\n\n        batch_size (int): The batch size for writing nodes.\n\n        force (bool): Whether to force writing nodes even if their type is\n            not present in the schema.\n\n\n    Returns:\n    -------\n        bool: The return value. True for success, False otherwise.\n\n    \"\"\"\n    # TODO check represented_as\n\n    # write node data\n    passed = self._write_node_data(nodes, batch_size, force)\n    if not passed:\n        logger.error(\"Error while writing node data.\")\n        return False\n    # pass property data to header writer per node type written\n    passed = self._write_node_headers()\n    if not passed:\n        logger.error(\"Error while writing node headers.\")\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/source/output-write/#neo4j-batch-writer","title":"Neo4j Batch Writer","text":"<p>               Bases: <code>_BatchWriter</code></p> <p>Class for writing node and edge representations to disk using the format specified by Neo4j for the use of admin import. Each batch writer instance has a fixed representation that needs to be passed at instantiation via the attr:<code>schema</code> argument. The instance also expects an ontology adapter via attr:<code>ontology_adapter</code> to be able to convert and extend the hierarchy.</p> <p>This class inherits from the abstract class \"_BatchWriter\" and implements the Neo4j-specific methods:</p> <pre><code>- _write_node_headers\n- _write_edge_headers\n- _construct_import_call\n- _write_array_string\n</code></pre> Source code in <code>biocypher/output/write/graph/_neo4j.py</code> <pre><code>class _Neo4jBatchWriter(_BatchWriter):\n    \"\"\"Class for writing node and edge representations to disk using the\n    format specified by Neo4j for the use of admin import. Each batch\n    writer instance has a fixed representation that needs to be passed\n    at instantiation via the :py:attr:`schema` argument. The instance\n    also expects an ontology adapter via :py:attr:`ontology_adapter` to be able\n    to convert and extend the hierarchy.\n\n    This class inherits from the abstract class \"_BatchWriter\" and implements the\n    Neo4j-specific methods:\n\n        - _write_node_headers\n        - _write_edge_headers\n        - _construct_import_call\n        - _write_array_string\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Constructor.\n\n        Check the version of Neo4j and adds a command scope if version &gt;= 5.\n\n        Returns\n        -------\n            _Neo4jBatchWriter: An instance of the writer.\n\n        \"\"\"\n        # Should read the configuration and setup import_call_bin_prefix.\n        super().__init__(*args, **kwargs)\n\n    def _get_default_import_call_bin_prefix(self):\n        \"\"\"Method to provide the default string for the import call bin prefix.\n\n        Returns\n        -------\n            str: The default location for the neo4j admin import location\n\n        \"\"\"\n        return \"bin/\"\n\n    def _quote_string(self, value: str) -&gt; str:\n        \"\"\"Quote a string. Quote character is escaped by doubling it.\"\"\"\n        return f\"{self.quote}{value.replace(self.quote, self.quote * 2)}{self.quote}\"\n\n    def _write_array_string(self, string_list):\n        \"\"\"Abstract method to output.write the string representation of an array into a .csv file\n        as required by the neo4j admin-import.\n\n        Args:\n        ----\n            string_list (list): list of ontology strings\n\n        Returns:\n        -------\n            str: The string representation of an array for the neo4j admin import\n\n        \"\"\"\n        string = self.adelim.join(string_list)\n        return self._quote_string(string)\n\n    def _write_node_headers(self):\n        \"\"\"Writes single CSV file for a graph entity that is represented\n        as a node as per the definition in the `schema_config.yaml`,\n        containing only the header for this type of node.\n\n        Returns\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        \"\"\"\n        # load headers from data parse\n        if not self.node_property_dict:\n            logger.error(\n                \"Header information not found. Was the data parsed first?\",\n            )\n            return False\n\n        for label, props in self.node_property_dict.items():\n            _id = \":ID\"\n\n            # translate label to PascalCase\n            pascal_label = self.translator.name_sentence_to_pascal(parse_label(label))\n\n            header = f\"{pascal_label}-header.csv\"\n            header_path = os.path.join(\n                self.outdir,\n                header,\n            )\n            parts = f\"{pascal_label}-part.*\"\n\n            # check if file already exists\n            if os.path.exists(header_path):\n                logger.warning(\n                    f\"Header file `{header_path}` already exists. Overwriting.\",\n                )\n\n            # concatenate key:value in props\n            props_list = []\n            for k, v in props.items():\n                if v in [\"int\", \"long\", \"integer\"]:\n                    props_list.append(f\"{k}:long\")\n                elif v in [\"int[]\", \"long[]\", \"integer[]\"]:\n                    props_list.append(f\"{k}:long[]\")\n                elif v in [\"float\", \"double\", \"dbl\"]:\n                    props_list.append(f\"{k}:double\")\n                elif v in [\"float[]\", \"double[]\"]:\n                    props_list.append(f\"{k}:double[]\")\n                elif v in [\"bool\", \"boolean\"]:\n                    # TODO Neo4j boolean support / spelling?\n                    props_list.append(f\"{k}:boolean\")\n                elif v in [\"bool[]\", \"boolean[]\"]:\n                    props_list.append(f\"{k}:boolean[]\")\n                elif v in [\"str[]\", \"string[]\"]:\n                    props_list.append(f\"{k}:string[]\")\n                else:\n                    props_list.append(f\"{k}\")\n\n            # create list of lists and flatten\n            out_list = [[_id], props_list, [\":LABEL\"]]\n            out_list = [val for sublist in out_list for val in sublist]\n\n            with open(header_path, \"w\", encoding=\"utf-8\") as f:\n                # concatenate with delimiter\n                row = self.delim.join(out_list)\n                f.write(row)\n\n            # add file path to neo4 admin import statement (import call file\n            # path may be different from actual file path)\n            import_call_header_path = os.path.join(\n                self.import_call_file_prefix,\n                header,\n            )\n            import_call_parts_path = os.path.join(\n                self.import_call_file_prefix,\n                parts,\n            )\n            self.import_call_nodes.add((import_call_header_path, import_call_parts_path))\n\n        return True\n\n    def _write_edge_headers(self):\n        \"\"\"Writes single CSV file for a graph entity that is represented\n        as an edge as per the definition in the `schema_config.yaml`,\n        containing only the header for this type of edge.\n\n        Returns\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        \"\"\"\n        # load headers from data parse\n        if not self.edge_property_dict:\n            logger.error(\n                \"Header information not found. Was the data parsed first?\",\n            )\n            return False\n\n        for label, props in self.edge_property_dict.items():\n            # translate label to PascalCase\n            pascal_label = self.translator.name_sentence_to_pascal(parse_label(label))\n\n            # paths\n            header = f\"{pascal_label}-header.csv\"\n            header_path = os.path.join(\n                self.outdir,\n                header,\n            )\n            parts = f\"{pascal_label}-part.*\"\n\n            # check for file exists\n            if os.path.exists(header_path):\n                logger.warning(f\"File {header_path} already exists. Overwriting.\")\n\n            # concatenate key:value in props\n            props_list = []\n            for k, v in props.items():\n                if v in [\"int\", \"long\", \"integer\"]:\n                    props_list.append(f\"{k}:long\")\n                elif v in [\"int[]\", \"long[]\", \"integer[]\"]:\n                    props_list.append(f\"{k}:long[]\")\n                elif v in [\"float\", \"double\"]:\n                    props_list.append(f\"{k}:double\")\n                elif v in [\"float[]\", \"double[]\"]:\n                    props_list.append(f\"{k}:double[]\")\n                elif v in [\n                    \"bool\",\n                    \"boolean\",\n                ]:  # TODO does Neo4j support bool?\n                    props_list.append(f\"{k}:boolean\")\n                elif v in [\"bool[]\", \"boolean[]\"]:\n                    props_list.append(f\"{k}:boolean[]\")\n                elif v in [\"str[]\", \"string[]\"]:\n                    props_list.append(f\"{k}:string[]\")\n                else:\n                    props_list.append(f\"{k}\")\n\n            skip_id = False\n            schema_label = None\n\n            if label in [\"IS_SOURCE_OF\", \"IS_TARGET_OF\", \"IS_PART_OF\"]:\n                skip_id = True\n            elif not self.translator.ontology.mapping.extended_schema.get(label):\n                # find label in schema by label_as_edge\n                for (\n                    k,\n                    v,\n                ) in self.translator.ontology.mapping.extended_schema.items():\n                    if v.get(\"label_as_edge\") == label:\n                        schema_label = k\n                        break\n            else:\n                schema_label = label\n\n            out_list = [\":START_ID\"]\n\n            if schema_label:\n                if (\n                    self.translator.ontology.mapping.extended_schema.get(  # (seems to not work with 'not')\n                        schema_label,\n                    ).get(\"use_id\")\n                    == False  # noqa: E712 (seems to not work with 'not')\n                ):\n                    skip_id = True\n\n            if not skip_id:\n                out_list.append(\"id\")\n\n            out_list.extend(props_list)\n            out_list.extend([\":END_ID\", \":TYPE\"])\n\n            with open(header_path, \"w\", encoding=\"utf-8\") as f:\n                # concatenate with delimiter\n                row = self.delim.join(out_list)\n                f.write(row)\n\n            # add file path to neo4 admin import statement (import call file\n            # path may be different from actual file path)\n            import_call_header_path = os.path.join(\n                self.import_call_file_prefix,\n                header,\n            )\n            import_call_parts_path = os.path.join(\n                self.import_call_file_prefix,\n                parts,\n            )\n            self.import_call_edges.add((import_call_header_path, import_call_parts_path))\n\n        return True\n\n    def _get_import_script_name(self) -&gt; str:\n        \"\"\"Returns the name of the neo4j admin import script\n\n        Returns\n        -------\n            str: The name of the import script (ending in .sh)\n\n        \"\"\"\n        return \"neo4j-admin-import-call.sh\"\n\n    def _construct_import_call(self) -&gt; str:\n        \"\"\"Function to construct the import call detailing folder and\n        individual node and edge headers and data files, as well as\n        delimiters and database name. Built after all data has been\n        processed to ensure that nodes are called before any edges.\n\n        Returns\n        -------\n            str: a bash command for neo4j-admin import\n\n        \"\"\"\n        import_call_neo4j_v4 = self._get_import_call(\"import\", \"--database=\", \"--force=\")\n        import_call_neo4j_v5 = self._get_import_call(\"database import full\", \"\", \"--overwrite-destination=\")\n        neo4j_version_check = f\"version=$({self.import_call_bin_prefix}neo4j-admin --version | cut -d '.' -f 1)\"\n\n        import_script = (\n            f\"#!/bin/bash\\n{neo4j_version_check}\\nif [[ $version -ge 5 ]]; \"\n            f\"then\\n\\t{import_call_neo4j_v5}\\nelse\\n\\t{import_call_neo4j_v4}\\nfi\"\n        )\n        return import_script\n\n    def _get_import_call(self, import_cmd: str, database_cmd: str, wipe_cmd: str) -&gt; str:\n        \"\"\"Get parametrized import call for Neo4j 4 or 5+.\n\n        Args:\n        ----\n            import_cmd (str): The import command to use.\n            database_cmd (str): The database command to use.\n            wipe_cmd (str): The wipe command to use.\n\n        Returns:\n        -------\n            str: The import call.\n\n        \"\"\"\n        import_call = f\"{self.import_call_bin_prefix}neo4j-admin {import_cmd} \"\n\n        import_call += f\"{database_cmd}{self.db_name} \"\n\n        import_call += f'--delimiter=\"{self.escaped_delim}\" '\n\n        import_call += f'--array-delimiter=\"{self.escaped_adelim}\" '\n\n        if self.quote == \"'\":\n            import_call += f'--quote=\"{self.quote}\" '\n        else:\n            import_call += f\"--quote='{self.quote}' \"\n\n        if self.wipe:\n            import_call += f\"{wipe_cmd}true \"\n        if self.skip_bad_relationships:\n            import_call += \"--skip-bad-relationships=true \"\n        if self.skip_duplicate_nodes:\n            import_call += \"--skip-duplicate-nodes=true \"\n\n        # append node import calls\n        for header_path, parts_path in self.import_call_nodes:\n            import_call += f'--nodes=\"{header_path},{parts_path}\" '\n\n        # append edge import calls\n        for header_path, parts_path in self.import_call_edges:\n            import_call += f'--relationships=\"{header_path},{parts_path}\" '\n\n        return import_call\n</code></pre>"},{"location":"reference/source/output-write/#biocypher.output.write.graph._neo4j._Neo4jBatchWriter.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Constructor.</p> <p>Check the version of Neo4j and adds a command scope if version &gt;= 5.</p>"},{"location":"reference/source/output-write/#biocypher.output.write.graph._neo4j._Neo4jBatchWriter.__init__--returns","title":"Returns","text":"<pre><code>_Neo4jBatchWriter: An instance of the writer.\n</code></pre> Source code in <code>biocypher/output/write/graph/_neo4j.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Constructor.\n\n    Check the version of Neo4j and adds a command scope if version &gt;= 5.\n\n    Returns\n    -------\n        _Neo4jBatchWriter: An instance of the writer.\n\n    \"\"\"\n    # Should read the configuration and setup import_call_bin_prefix.\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/source/output-write/#arangodb-batch-writer","title":"ArangoDB Batch Writer","text":"<p>               Bases: <code>_Neo4jBatchWriter</code></p> <p>Class for writing node and edge representations to disk.</p> <p>Uses the format specified by ArangoDB for the use of \"arangoimport\". Output files are similar to Neo4j, but with a different header format.</p> Source code in <code>biocypher/output/write/graph/_arangodb.py</code> <pre><code>class _ArangoDBBatchWriter(_Neo4jBatchWriter):\n    \"\"\"Class for writing node and edge representations to disk.\n\n    Uses the format specified by ArangoDB for the use of \"arangoimport\".\n    Output files are similar to Neo4j, but with a different header format.\n    \"\"\"\n\n    def _get_default_import_call_bin_prefix(self):\n        \"\"\"Provide the default string for the import call bin prefix.\n\n        Returns\n        -------\n            str: The default location for the neo4j admin import location\n\n        \"\"\"\n        return \"\"\n\n    def _get_import_script_name(self) -&gt; str:\n        \"\"\"Return the name of the neo4j admin import script.\n\n        Returns\n        -------\n            str: The name of the import script (ending in .sh)\n\n        \"\"\"\n        return \"arangodb-import-call.sh\"\n\n    def _write_node_headers(self):\n        \"\"\"Write single CSV file for a graph entity.\n\n        The graph entity is represented as a node as per the definition\n        in the `schema_config.yaml`, containing only the header for this type\n        of node.\n\n        Returns\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        \"\"\"\n        # load headers from data parse\n        if not self.node_property_dict:\n            logger.error(\n                \"Header information not found. Was the data parsed first?\",\n            )\n            return False\n\n        for label, props in self.node_property_dict.items():\n            # create header CSV with ID, properties, labels\n\n            _id = \"_key\"\n\n            # translate label to PascalCase\n            pascal_label = self.translator.name_sentence_to_pascal(label)\n\n            header = f\"{pascal_label}-header.csv\"\n            header_path = os.path.join(\n                self.outdir,\n                header,\n            )\n\n            # check if file already exists\n            if os.path.exists(header_path):\n                logger.warning(f\"File {header_path} already exists. Overwriting.\")\n\n            # concatenate key:value in props\n            props_list = []\n            for k in props.keys():\n                props_list.append(f\"{k}\")\n\n            # create list of lists and flatten\n            # removes need for empty check of property list\n            out_list = [[_id], props_list]\n            out_list = [val for sublist in out_list for val in sublist]\n\n            with open(header_path, \"w\", encoding=\"utf-8\") as f:\n                # concatenate with delimiter\n                row = self.delim.join(out_list)\n                f.write(row)\n\n            # add collection from schema config\n            collection = self.translator.ontology.mapping.extended_schema[label].get(\"db_collection_name\", None)\n\n            # add file path to neo4 admin import statement\n            # do once for each part file\n            parts = self.parts.get(label, [])\n\n            if not parts:\n                msg = f\"No parts found for node label {label}. Check that the data was parsed first.\"\n                logger.error(msg)\n                raise ValueError(msg)\n\n            for part in parts:\n                import_call_header_path = os.path.join(\n                    self.import_call_file_prefix,\n                    header,\n                )\n                import_call_parts_path = os.path.join(\n                    self.import_call_file_prefix,\n                    part,\n                )\n\n                self.import_call_nodes.add(\n                    (\n                        import_call_header_path,\n                        import_call_parts_path,\n                        collection,\n                    ),\n                )\n\n        return True\n\n    def _write_edge_headers(self):\n        \"\"\"Write single CSV file for a graph entity.\n\n        The graph entity is represented as an edge as per the definition\n        in the `schema_config.yaml`, containing only the header for this type\n        of edge.\n\n        Returns\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        \"\"\"\n        # load headers from data parse\n        if not self.edge_property_dict:\n            logger.error(\n                \"Header information not found. Was the data parsed first?\",\n            )\n            return False\n\n        for label, props in self.edge_property_dict.items():\n            # translate label to PascalCase\n            pascal_label = self.translator.name_sentence_to_pascal(label)\n\n            # paths\n            header = f\"{pascal_label}-header.csv\"\n            header_path = os.path.join(\n                self.outdir,\n                header,\n            )\n            parts = f\"{pascal_label}-part.*\"\n\n            # check for file exists\n            if os.path.exists(header_path):\n                logger.warning(f\"Header file {header_path} already exists. Overwriting.\")\n\n            # concatenate key:value in props\n            props_list = []\n            for k in props.keys():\n                props_list.append(f\"{k}\")\n\n            out_list = [\"_from\", \"_key\", *props_list, \"_to\"]\n\n            with open(header_path, \"w\", encoding=\"utf-8\") as f:\n                # concatenate with delimiter\n                row = self.delim.join(out_list)\n                f.write(row)\n\n            # add collection from schema config\n            if not self.translator.ontology.mapping.extended_schema.get(label):\n                for (\n                    _,\n                    v,\n                ) in self.translator.ontology.mapping.extended_schema.items():\n                    if v.get(\"label_as_edge\") == label:\n                        collection = v.get(\"db_collection_name\", None)\n                        break\n\n            else:\n                collection = self.translator.ontology.mapping.extended_schema[label].get(\"db_collection_name\", None)\n\n            # add file path to neo4 admin import statement (import call path\n            # may be different from actual output path)\n            header_import_call_path = os.path.join(\n                self.import_call_file_prefix,\n                header,\n            )\n            parts_import_call_path = os.path.join(\n                self.import_call_file_prefix,\n                parts,\n            )\n            self.import_call_edges.add(\n                (\n                    header_import_call_path,\n                    parts_import_call_path,\n                    collection,\n                ),\n            )\n\n        return True\n\n    def _construct_import_call(self) -&gt; str:\n        \"\"\"Construct the import call.\n\n        Details folder and individual node and edge headers and data files,\n        as well as delimiters and database name. Built after all data has been\n        processed to ensure that nodes are called before any edges.\n\n        Returns\n        -------\n            str: a bash command for arangoimport\n\n        \"\"\"\n        import_call = f\"{self.import_call_bin_prefix}arangoimp --type csv \" f'--separator=\"{self.escaped_delim}\" '\n\n        if self.quote == \"'\":\n            import_call += f'--quote=\"{self.quote}\" '\n        else:\n            import_call += f\"--quote='{self.quote}' \"\n\n        node_lines = \"\"\n\n        # node import calls: one line per node type\n        for header_path, parts_path, collection in self.import_call_nodes:\n            line = f\"{import_call} --headers-file {header_path} --file= {parts_path} \"\n\n            if collection:\n                line += f\"--create-collection --collection {collection} \"\n\n            node_lines += f\"{line}\\n\"\n\n        edge_lines = \"\"\n\n        # edge import calls: one line per edge type\n        for header_path, parts_path, collection in self.import_call_edges:\n            import_call += f'--relationships=\"{header_path},{parts_path}\" '\n\n        return node_lines + edge_lines\n</code></pre>"},{"location":"reference/source/output-write/#rdf-writer","title":"RDF Writer","text":"<p>               Bases: <code>_BatchWriter</code></p> <p>Write BioCypher's property graph into an RDF format.</p> <p>Uses <code>rdflib</code> and all the extensions it supports (RDF/XML, N3, NTriples, N-Quads, Turtle, TriX, Trig and JSON-LD). By default, the conversion is done keeping only the minimum information about node and edges, skipping all properties.</p> Source code in <code>biocypher/output/write/graph/_rdf.py</code> <pre><code>class _RDFWriter(_BatchWriter):\n    \"\"\"Write BioCypher's property graph into an RDF format.\n\n    Uses `rdflib` and all the extensions it supports (RDF/XML, N3, NTriples,\n    N-Quads, Turtle, TriX, Trig and JSON-LD). By default, the conversion\n    is done keeping only the minimum information about node and edges,\n    skipping all properties.\n    \"\"\"\n\n    def __init__(\n        self,\n        translator: Translator,\n        deduplicator: Deduplicator,\n        delimiter: str,\n        array_delimiter: str = \",\",\n        quote: str = '\"',\n        output_directory: str | None = None,\n        db_name: str = \"neo4j\",\n        import_call_bin_prefix: str | None = None,\n        import_call_file_prefix: str | None = None,\n        wipe: bool = True,\n        strict_mode: bool = False,\n        skip_bad_relationships: bool = False,\n        skip_duplicate_nodes: bool = False,\n        db_user: str = None,\n        db_password: str = None,\n        db_host: str = None,\n        db_port: str = None,\n        file_format: str = None,\n        rdf_namespaces: dict = {},\n        labels_order: str = \"Ascending\",\n        **kwargs,\n    ):\n        super().__init__(\n            translator=translator,\n            deduplicator=deduplicator,\n            delimiter=delimiter,\n            array_delimiter=array_delimiter,\n            quote=quote,\n            output_directory=output_directory,\n            db_name=db_name,\n            import_call_bin_prefix=import_call_bin_prefix,\n            import_call_file_prefix=import_call_file_prefix,\n            wipe=wipe,\n            strict_mode=strict_mode,\n            skip_bad_relationships=skip_bad_relationships,\n            skip_duplicate_nodes=skip_duplicate_nodes,\n            db_user=db_user,\n            db_password=db_password,\n            db_host=db_host,\n            db_port=db_port,\n            file_format=file_format,\n            rdf_namespaces=rdf_namespaces,\n            labels_order=labels_order,\n            **kwargs,\n        )\n        if not self.rdf_namespaces:\n            # For some reason, the config can pass\n            # the None object.\n            self.rdf_namespaces = {}\n\n        if \"rdf_format\" in kwargs:\n            logger.warning(\"The 'rdf_format' config option is deprecated, use 'file_format' instead.\")\n            if not file_format:\n                format = kwargs[\"rdf_format\"]\n                logger.warning(f\"I will set 'file_format: {format}' for you.\")\n                self.file_format = format\n                kwargs.pop(\"rdf_format\")\n            logger.warning(\"NOTE: this warning will become an error in next versions.\")\n\n        if not file_format:\n            msg = \"You need to indicate a 'file_format'.\"\n            logger.error(msg)\n            raise RuntimeError(msg)\n\n        self.namespaces = {}\n\n    def _get_import_script_name(self) -&gt; str:\n        \"\"\"Return the name of the RDF admin import script.\n\n        This function is used for RDF export.\n\n        Returns\n        -------\n            str: The name of the import script (ending in .sh)\n\n        \"\"\"\n        return \"rdf-import-call.sh\"\n\n    def _get_default_import_call_bin_prefix(self):\n        \"\"\"Provide the default string for the import call bin prefix.\n\n        Returns\n        -------\n            str: The default location for the RDF admin import location\n\n        \"\"\"\n        return \"bin/\"\n\n    def _is_rdf_format_supported(self, file_format: str) -&gt; bool:\n        \"\"\"Check if the specified RDF format is supported.\n\n        Args:\n        ----\n            file_format (str): The RDF format to check.\n\n        Returns:\n        -------\n            bool: Returns True if rdf format supported, False otherwise.\n\n        \"\"\"\n        supported_formats = [\n            \"xml\",\n            \"n3\",\n            \"turtle\",\n            \"ttl\",\n            \"nt\",\n            \"pretty-xml\",\n            \"trix\",\n            \"trig\",\n            \"nquads\",\n            \"json-ld\",\n        ]\n        if file_format not in supported_formats:\n            logger.error(\n                f\"Incorrect or unsupported RDF format: '{file_format}',\"\n                f\"use one of the following: {', '.join(supported_formats)}.\",\n            )\n            return False\n        else:\n            # Set the file extension to match the format\n            if self.file_format == \"turtle\":\n                self.extension = \"ttl\"\n            else:\n                self.extension = self.file_format\n            return True\n\n    def _write_single_edge_list_to_file(\n        self,\n        edge_list: list,\n        label: str,\n        prop_dict: dict,\n    ):\n        \"\"\"Write a list of BioCypherEdges to an RDF file.\n\n        Args:\n        ----\n            edge_list (list): list of BioCypherEdges to be written\n\n            label (str): the label (type) of the edge\n\n            prop_dict (dict): properties of node class passed from parsing\n                function and their types\n\n        Returns:\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        \"\"\"\n        # NOTE: prop_dict is not used. Remove in next refactor.\n\n        if not all(isinstance(n, BioCypherEdge) for n in edge_list):\n            logger.error(\"Edges must be passed as type BioCypherEdge.\")\n            return False\n\n        # translate label to PascalCase\n        label_pascal = self.translator.name_sentence_to_pascal(label)\n\n        # create file name\n        file_name = os.path.join(self.outdir, f\"{label_pascal}.{self.extension}\")\n\n        # write data in graph\n        graph = Graph()\n        self._init_namespaces(graph)\n\n        for edge in edge_list:\n            rdf_subject = edge.get_source_id()\n            rdf_object = edge.get_target_id()\n            rdf_predicate = edge.get_id()\n            rdf_properties = edge.get_properties()\n            if rdf_predicate is None:\n                rdf_predicate = rdf_subject + rdf_object\n\n            edge_label = self.translator.name_sentence_to_pascal(edge.get_label())\n            edge_uri = self.as_uri(edge_label, \"biocypher\")\n            graph.add((edge_uri, RDF.type, RDFS.Class))\n            graph.add(\n                (\n                    self.as_uri(rdf_predicate, \"biocypher\"),\n                    RDF.type,\n                    edge_uri,\n                ),\n            )\n            graph.add(\n                (\n                    self.as_uri(rdf_predicate, \"biocypher\"),\n                    self.as_uri(\"subject\", \"biocypher\"),\n                    self.to_uri(rdf_subject),\n                ),\n            )\n            graph.add(\n                (\n                    self.as_uri(rdf_predicate, \"biocypher\"),\n                    self.as_uri(\"object\", \"biocypher\"),\n                    self.to_uri(rdf_object),\n                ),\n            )\n\n            # add properties to the transformed edge --&gt; node\n            for key, value in rdf_properties.items():\n                # only write value if it exists.\n                if value:\n                    self.add_property_to_graph(graph, rdf_predicate, value, key)\n\n        graph.serialize(destination=file_name, format=self.file_format)\n\n        logger.info(\n            f\"Writing {len(edge_list)} entries to {label_pascal}.{self.file_format}\",\n        )\n\n        return True\n\n    def add_property_to_graph(\n        self,\n        graph: Graph,\n        rdf_subject: str,\n        rdf_object: str,\n        rdf_predicate: str,\n    ):\n        \"\"\"Add the properties to an RDF node.\n\n        It takes the graph, the subject, object, and predicate of the RDF\n        triple. It checks if the property is a list and adds it to the graph\n        accordingly. Otherwise it checks if the string represents a list. If it\n        does, it transforms it to a list and adds it to the graph. If not, it\n        adds the property to the graph as a literal. If the property is neither\n        a list or string, it will also be added as a literal.\n\n        Args:\n        ----\n            graph (RDFLib.Graph): The RDF graph to add the nodes to.\n\n            rdf_subject (str): The subject of the RDF triple.\n\n            rdf_object (str): The object of the RDF triple.\n\n            rdf_predicate (str): The predicate of the RDF triple.\n\n        Returns:\n        -------\n            None\n\n        \"\"\"\n        if isinstance(rdf_object, list):\n            for obj in rdf_object:\n                graph.add(\n                    (\n                        self.to_uri(rdf_subject),\n                        self.property_to_uri(rdf_predicate),\n                        Literal(obj),\n                    ),\n                )\n        elif isinstance(rdf_object, str):\n            if rdf_object.startswith(\"[\") and rdf_object.endswith(\"]\"):\n                self.add_property_to_graph(\n                    graph,\n                    rdf_subject,\n                    self.transform_string_to_list(rdf_object),\n                    rdf_predicate,\n                )\n            else:\n                graph.add(\n                    (\n                        self.to_uri(rdf_subject),\n                        self.property_to_uri(rdf_predicate),\n                        Literal(rdf_object),\n                    ),\n                )\n        else:\n            graph.add(\n                (\n                    self.to_uri(rdf_subject),\n                    self.property_to_uri(rdf_predicate),\n                    Literal(rdf_object),\n                ),\n            )\n\n    def transform_string_to_list(self, string_list: str) -&gt; list:\n        \"\"\"Transform a string representation of a list into a list.\n\n        Args:\n        ----\n            string_list (str): The string representation of the list.\n\n        Returns:\n        -------\n            list: The list representation of the input string.\n\n        \"\"\"\n        return string_list.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").split(\", \")\n\n    def _write_single_node_list_to_file(\n        self,\n        node_list: list,\n        label: str,\n        prop_dict: dict,\n        labels: str,\n    ):\n        \"\"\"Write a list of BioCypherNodes to an RDF file.\n\n        Args:\n        ----\n            node_list (list): A list of BioCypherNodes to be written.\n\n            label (str): The label (type) of the nodes.\n\n            prop_dict (dict): A dictionary of properties and their types for the node class.\n\n            labels (str): string of one or several concatenated labels\n\n        Returns:\n        -------\n            bool: True if the writing is successful, False otherwise.\n\n        \"\"\"\n        # NOTE: labels and prop_dict are not used.\n\n        if not all(isinstance(n, BioCypherNode) for n in node_list):\n            logger.error(\"Nodes must be passed as type BioCypherNode.\")\n            return False\n\n        # translate label to PascalCase\n        label_pascal = self.translator.name_sentence_to_pascal(label)\n\n        # create file name\n        file_name = os.path.join(self.outdir, f\"{label_pascal}.{self.extension}\")\n\n        # write data in graph\n        graph = Graph()\n        self._init_namespaces(graph)\n\n        for n in node_list:\n            rdf_subject = n.get_id()\n            rdf_object = n.get_label()\n            properties = n.get_properties()\n            class_name = self.translator.name_sentence_to_pascal(rdf_object)\n            graph.add(\n                (\n                    self.as_uri(class_name, \"biocypher\"),\n                    RDF.type,\n                    RDFS.Class,\n                ),\n            )\n            graph.add(\n                (\n                    self.to_uri(rdf_subject),\n                    RDF.type,\n                    self.as_uri(class_name, \"biocypher\"),\n                ),\n            )\n            for key, value in properties.items():\n                # only write value if it exists.\n                if value:\n                    self.add_property_to_graph(graph, rdf_subject, value, key)\n\n        graph.serialize(destination=file_name, format=self.file_format)\n\n        logger.info(\n            f\"Writing {len(node_list)} entries to {label_pascal}.{self.file_format}\",\n        )\n\n        return True\n\n    def write_nodes(self, nodes, batch_size: int = int(1e6), force: bool = False) -&gt; bool:\n        \"\"\"Write nodes in RDF format.\n\n        Args:\n        ----\n            nodes (list or generator): A list or generator of nodes in\n                BioCypherNode format.\n            batch_size (int): The number of nodes to write in each batch.\n            force (bool): Flag to force the writing even if the output file\n                already exists.\n\n        Returns:\n        -------\n            bool: True if the writing is successful, False otherwise.\n\n        \"\"\"\n        # check if specified output format is correct\n        passed = self._is_rdf_format_supported(self.file_format)\n        if not passed:\n            logger.error(\"Error while writing node data, wrong RDF format\")\n            return False\n        # write node data using _write_node_data method\n        passed = self._write_node_data(nodes, batch_size, force)\n        if not passed:\n            logger.error(\"Error while writing node data.\")\n            return False\n        return True\n\n    def write_edges(\n        self,\n        edges: list | GeneratorType,\n        batch_size: int = int(1e6),\n    ) -&gt; bool:\n        \"\"\"Write edges in RDF format.\n\n        Args:\n        ----\n            edges (BioCypherEdge): a list or generator of edges in\n                :py:class:`BioCypherEdge` format\n            batch_size (int): The number of edges to write in each batch.\n\n        Returns:\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        \"\"\"\n        # check if specified output format is correct\n        passed = self._is_rdf_format_supported(self.file_format)\n        if not passed:\n            logger.error(\"Error while writing edge data, wrong RDF format\")\n            return False\n        # write edge data using _write_edge_data method\n        passed = self._write_edge_data(edges, batch_size=batch_size)\n        if not passed:\n            logger.error(\"Error while writing edge data.\")\n            return False\n\n        return True\n\n    def _construct_import_call(self) -&gt; bool:\n        \"\"\"Write the import call.\n\n        This function is not applicable for RDF.\n\n        Returns\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        \"\"\"\n        return \"\"\n\n    def _quote_string(self, value: str) -&gt; str:\n        \"\"\"Quote a string.\"\"\"\n        return f\"{self.quote}{value}{self.quote}\"\n\n    def _write_array_string(self, string_list):\n        \"\"\"Write the string representation of an array into a .csv file.\n\n        This function is not applicable for RDF.\n\n        Args:\n        ----\n            string_list (list): list of ontology strings\n\n        Returns:\n        -------\n            str: The string representation of an array for the neo4j admin import\n\n        \"\"\"\n        return True\n\n    def _write_node_headers(self):\n        \"\"\"Import properties of a graph entity.\n\n        This function is not applicable for RDF.\n\n        Returns\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        \"\"\"\n        return True\n\n    def _write_edge_headers(self):\n        \"\"\"Write a database import-file for a graph entity.\n\n        This function is not applicable for RDF.\n\n        Returns\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        \"\"\"\n        return True\n\n    def as_uri(self, name: str, namespace: str = \"\") -&gt; str:\n        \"\"\"Return an RDFlib object with the given namespace as a URI.\n\n        There is often a default for empty namespaces, which would have been\n        loaded with the ontology, and put in `self.namespace` by\n        `self._init_namespaces`.\n\n        Args:\n        ----\n            name (str): The name to be transformed.\n            namespace (str): The namespace to be used.\n\n        Returns:\n        -------\n            str: The URI for the given name and namespace.\n\n        \"\"\"\n        if namespace in self.namespaces:\n            return URIRef(self.namespaces[namespace][name])\n        else:\n            assert \"biocypher\" in self.namespaces\n            # If no default empty NS, use the biocypher one,\n            # which is always there.\n            logger.debug(f\"I'll consider '{name}' as part of 'biocypher' namespace.\")\n            return URIRef(self.namespaces[\"biocypher\"][name])\n\n    def to_uri(self, subject: str) -&gt; str:\n        \"\"\"Extract the namespace from the given subject.\n\n        Split the subject's string on \":\". Then convert the subject to a\n        proper URI, if the namespace is known. If namespace is unknown,\n        defaults to the default prefix of the ontology.\n\n        Args:\n        ----\n            subject (str): The subject to be converted to a URI.\n\n        Returns:\n        -------\n            str: The corresponding URI for the subject.\n\n        \"\"\"\n        pref_id = subject.split(\":\")\n        if len(pref_id) == 2:\n            pref, id = pref_id\n            return self.as_uri(id, pref)\n        else:\n            return self.as_uri(subject)\n\n    def find_uri(self, regexp: str) -&gt; str:\n        query = f'SELECT DISTINCT ?s WHERE {{ ?s ?p ?o . FILTER regex(str(?s), \"{regexp}\")}}'\n        gen = self.graph.query(query)\n        uris = list(gen)\n        if len(uris) &gt; 1:\n            logger.warning(\n                f\"Found several terms matching `{regexp}`, I will consider only the first one: `{uris[0][0]}`\",\n            )\n            logger.debug(\"\\tothers:\")\n            for u in uris[1:]:\n                logger.debug(f\"\\t{u[0]}\")\n        if uris:\n            logger.debug(f\"Found {len(uris)} terms, returning: `{uris[0][0]}`\")\n            return uris[0][0]\n        else:\n            logger.debug(f\"Found no term matching: `{query}`\")\n            return None\n\n    def property_to_uri(self, property_name: str) -&gt; dict[str, str]:\n        \"\"\"Convert a property name to its corresponding URI.\n\n        This function takes a property name and searches for its corresponding\n        URI in various namespaces. It first checks the core namespaces for\n        rdflib, including owl, rdf, rdfs, xsd, and xml.\n\n        Args:\n        ----\n            property_name (str): The property name to be converted to a URI.\n\n        Returns:\n        -------\n            str: The corresponding URI for the input property name.\n\n        \"\"\"\n        # These namespaces are core for rdflib; owl, rdf, rdfs, xsd and xml\n        for namespace in _NAMESPACE_PREFIXES_CORE.values():\n            if property_name in namespace:\n                return namespace[property_name]\n\n        # If the property name is not found in the core namespaces, search in\n        # the SKOS, DC, and DCTERMS namespaces\n        for namespace in [SKOS, DC, DCTERMS]:\n            if property_name in namespace:\n                return namespace[property_name]\n\n        # If the property name is still not found, try other namespaces from\n        # rdflib.\n        for namespace in _NAMESPACE_PREFIXES_RDFLIB.values():\n            if property_name in namespace:\n                return namespace[property_name]\n\n        # If the property name is \"licence\", it recursively calls the function\n        # with \"license\" as the input.\n        if property_name == \"licence\":\n            return self.property_to_uri(\"license\")\n\n        # TODO: add an option to search trough manually implemented namespaces\n\n        # If the input is not found in any of the namespaces, it returns\n        # the corresponding URI from the biocypher namespace.\n        # TODO: give a warning and try to prevent this option altogether\n        return self.as_uri(property_name, \"biocypher\")\n\n    def _init_namespaces(self, graph: Graph):\n        \"\"\"Initialise the namespaces for the RDF graph.\n\n        This function adds the biocypher standard namespace to the `namespaces`\n        attribute of the class. If `namespaces` is empty, it sets it to the\n        biocypher standard namespace. Otherwise, it merges the biocypher\n        standard namespace with the namespaces defined in the\n        biocypher_config.yaml.\n\n        Args:\n        ----\n            graph (RDFLib.Graph): The RDF graph to bind the namespaces to.\n\n        Returns:\n        -------\n            None\n\n        \"\"\"\n        # Bind and keep the biocypher namespace.\n        bcns = Namespace(\"https://biocypher.org/biocypher#\")\n        bck = \"biocypher\"\n        self.namespaces = {bck: bcns}\n        graph.bind(bck, bcns)\n\n        # Keep track of namespaces loaded with the ontologies in the given graph.\n        logger.debug(\"Bind namespaces:\")\n        for prefix, ns in graph.namespaces():\n            if prefix in self.namespaces and str(ns) != str(self.namespaces[prefix]):\n                logger.warning(\n                    f\"Namespace '{prefix}' was already loaded\"\n                    f\"as '{self.namespaces[prefix]}',\"\n                    f\"I will overwrite it with '{ns}'.\",\n                )\n            logger.debug(f\"\\t'{prefix}'\\t=&gt;\\t'{ns}'\")\n            self.namespaces[prefix] = Namespace(ns)\n\n        # Bind and keep the namespaces given in the config.\n        for prefix, ns in self.rdf_namespaces.items():\n            assert prefix not in self.namespaces\n            self.namespaces[prefix] = Namespace(ns)\n            logger.debug(f\"\\t'{prefix}'\\t-&gt;\\t{ns}\")\n            graph.bind(prefix, self.namespaces[prefix])\n</code></pre>"},{"location":"reference/source/output-write/#biocypher.output.write.graph._rdf._RDFWriter.add_property_to_graph","title":"<code>add_property_to_graph(graph, rdf_subject, rdf_object, rdf_predicate)</code>","text":"<p>Add the properties to an RDF node.</p> <p>It takes the graph, the subject, object, and predicate of the RDF triple. It checks if the property is a list and adds it to the graph accordingly. Otherwise it checks if the string represents a list. If it does, it transforms it to a list and adds it to the graph. If not, it adds the property to the graph as a literal. If the property is neither a list or string, it will also be added as a literal.</p> <pre><code>graph (RDFLib.Graph): The RDF graph to add the nodes to.\n\nrdf_subject (str): The subject of the RDF triple.\n\nrdf_object (str): The object of the RDF triple.\n\nrdf_predicate (str): The predicate of the RDF triple.\n</code></pre> <pre><code>None\n</code></pre> Source code in <code>biocypher/output/write/graph/_rdf.py</code> <pre><code>def add_property_to_graph(\n    self,\n    graph: Graph,\n    rdf_subject: str,\n    rdf_object: str,\n    rdf_predicate: str,\n):\n    \"\"\"Add the properties to an RDF node.\n\n    It takes the graph, the subject, object, and predicate of the RDF\n    triple. It checks if the property is a list and adds it to the graph\n    accordingly. Otherwise it checks if the string represents a list. If it\n    does, it transforms it to a list and adds it to the graph. If not, it\n    adds the property to the graph as a literal. If the property is neither\n    a list or string, it will also be added as a literal.\n\n    Args:\n    ----\n        graph (RDFLib.Graph): The RDF graph to add the nodes to.\n\n        rdf_subject (str): The subject of the RDF triple.\n\n        rdf_object (str): The object of the RDF triple.\n\n        rdf_predicate (str): The predicate of the RDF triple.\n\n    Returns:\n    -------\n        None\n\n    \"\"\"\n    if isinstance(rdf_object, list):\n        for obj in rdf_object:\n            graph.add(\n                (\n                    self.to_uri(rdf_subject),\n                    self.property_to_uri(rdf_predicate),\n                    Literal(obj),\n                ),\n            )\n    elif isinstance(rdf_object, str):\n        if rdf_object.startswith(\"[\") and rdf_object.endswith(\"]\"):\n            self.add_property_to_graph(\n                graph,\n                rdf_subject,\n                self.transform_string_to_list(rdf_object),\n                rdf_predicate,\n            )\n        else:\n            graph.add(\n                (\n                    self.to_uri(rdf_subject),\n                    self.property_to_uri(rdf_predicate),\n                    Literal(rdf_object),\n                ),\n            )\n    else:\n        graph.add(\n            (\n                self.to_uri(rdf_subject),\n                self.property_to_uri(rdf_predicate),\n                Literal(rdf_object),\n            ),\n        )\n</code></pre>"},{"location":"reference/source/output-write/#biocypher.output.write.graph._rdf._RDFWriter.as_uri","title":"<code>as_uri(name, namespace='')</code>","text":"<p>Return an RDFlib object with the given namespace as a URI.</p> <p>There is often a default for empty namespaces, which would have been loaded with the ontology, and put in <code>self.namespace</code> by <code>self._init_namespaces</code>.</p> <pre><code>name (str): The name to be transformed.\nnamespace (str): The namespace to be used.\n</code></pre> <pre><code>str: The URI for the given name and namespace.\n</code></pre> Source code in <code>biocypher/output/write/graph/_rdf.py</code> <pre><code>def as_uri(self, name: str, namespace: str = \"\") -&gt; str:\n    \"\"\"Return an RDFlib object with the given namespace as a URI.\n\n    There is often a default for empty namespaces, which would have been\n    loaded with the ontology, and put in `self.namespace` by\n    `self._init_namespaces`.\n\n    Args:\n    ----\n        name (str): The name to be transformed.\n        namespace (str): The namespace to be used.\n\n    Returns:\n    -------\n        str: The URI for the given name and namespace.\n\n    \"\"\"\n    if namespace in self.namespaces:\n        return URIRef(self.namespaces[namespace][name])\n    else:\n        assert \"biocypher\" in self.namespaces\n        # If no default empty NS, use the biocypher one,\n        # which is always there.\n        logger.debug(f\"I'll consider '{name}' as part of 'biocypher' namespace.\")\n        return URIRef(self.namespaces[\"biocypher\"][name])\n</code></pre>"},{"location":"reference/source/output-write/#biocypher.output.write.graph._rdf._RDFWriter.property_to_uri","title":"<code>property_to_uri(property_name)</code>","text":"<p>Convert a property name to its corresponding URI.</p> <p>This function takes a property name and searches for its corresponding URI in various namespaces. It first checks the core namespaces for rdflib, including owl, rdf, rdfs, xsd, and xml.</p> <pre><code>property_name (str): The property name to be converted to a URI.\n</code></pre> <pre><code>str: The corresponding URI for the input property name.\n</code></pre> Source code in <code>biocypher/output/write/graph/_rdf.py</code> <pre><code>def property_to_uri(self, property_name: str) -&gt; dict[str, str]:\n    \"\"\"Convert a property name to its corresponding URI.\n\n    This function takes a property name and searches for its corresponding\n    URI in various namespaces. It first checks the core namespaces for\n    rdflib, including owl, rdf, rdfs, xsd, and xml.\n\n    Args:\n    ----\n        property_name (str): The property name to be converted to a URI.\n\n    Returns:\n    -------\n        str: The corresponding URI for the input property name.\n\n    \"\"\"\n    # These namespaces are core for rdflib; owl, rdf, rdfs, xsd and xml\n    for namespace in _NAMESPACE_PREFIXES_CORE.values():\n        if property_name in namespace:\n            return namespace[property_name]\n\n    # If the property name is not found in the core namespaces, search in\n    # the SKOS, DC, and DCTERMS namespaces\n    for namespace in [SKOS, DC, DCTERMS]:\n        if property_name in namespace:\n            return namespace[property_name]\n\n    # If the property name is still not found, try other namespaces from\n    # rdflib.\n    for namespace in _NAMESPACE_PREFIXES_RDFLIB.values():\n        if property_name in namespace:\n            return namespace[property_name]\n\n    # If the property name is \"licence\", it recursively calls the function\n    # with \"license\" as the input.\n    if property_name == \"licence\":\n        return self.property_to_uri(\"license\")\n\n    # TODO: add an option to search trough manually implemented namespaces\n\n    # If the input is not found in any of the namespaces, it returns\n    # the corresponding URI from the biocypher namespace.\n    # TODO: give a warning and try to prevent this option altogether\n    return self.as_uri(property_name, \"biocypher\")\n</code></pre>"},{"location":"reference/source/output-write/#biocypher.output.write.graph._rdf._RDFWriter.to_uri","title":"<code>to_uri(subject)</code>","text":"<p>Extract the namespace from the given subject.</p> <p>Split the subject's string on \":\". Then convert the subject to a proper URI, if the namespace is known. If namespace is unknown, defaults to the default prefix of the ontology.</p> <pre><code>subject (str): The subject to be converted to a URI.\n</code></pre> <pre><code>str: The corresponding URI for the subject.\n</code></pre> Source code in <code>biocypher/output/write/graph/_rdf.py</code> <pre><code>def to_uri(self, subject: str) -&gt; str:\n    \"\"\"Extract the namespace from the given subject.\n\n    Split the subject's string on \":\". Then convert the subject to a\n    proper URI, if the namespace is known. If namespace is unknown,\n    defaults to the default prefix of the ontology.\n\n    Args:\n    ----\n        subject (str): The subject to be converted to a URI.\n\n    Returns:\n    -------\n        str: The corresponding URI for the subject.\n\n    \"\"\"\n    pref_id = subject.split(\":\")\n    if len(pref_id) == 2:\n        pref, id = pref_id\n        return self.as_uri(id, pref)\n    else:\n        return self.as_uri(subject)\n</code></pre>"},{"location":"reference/source/output-write/#biocypher.output.write.graph._rdf._RDFWriter.transform_string_to_list","title":"<code>transform_string_to_list(string_list)</code>","text":"<p>Transform a string representation of a list into a list.</p> <pre><code>string_list (str): The string representation of the list.\n</code></pre> <pre><code>list: The list representation of the input string.\n</code></pre> Source code in <code>biocypher/output/write/graph/_rdf.py</code> <pre><code>def transform_string_to_list(self, string_list: str) -&gt; list:\n    \"\"\"Transform a string representation of a list into a list.\n\n    Args:\n    ----\n        string_list (str): The string representation of the list.\n\n    Returns:\n    -------\n        list: The list representation of the input string.\n\n    \"\"\"\n    return string_list.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").split(\", \")\n</code></pre>"},{"location":"reference/source/output-write/#biocypher.output.write.graph._rdf._RDFWriter.write_edges","title":"<code>write_edges(edges, batch_size=int(1000000.0))</code>","text":"<p>Write edges in RDF format.</p> <pre><code>edges (BioCypherEdge): a list or generator of edges in\n    :py:class:`BioCypherEdge` format\nbatch_size (int): The number of edges to write in each batch.\n</code></pre> <pre><code>bool: The return value. True for success, False otherwise.\n</code></pre> Source code in <code>biocypher/output/write/graph/_rdf.py</code> <pre><code>def write_edges(\n    self,\n    edges: list | GeneratorType,\n    batch_size: int = int(1e6),\n) -&gt; bool:\n    \"\"\"Write edges in RDF format.\n\n    Args:\n    ----\n        edges (BioCypherEdge): a list or generator of edges in\n            :py:class:`BioCypherEdge` format\n        batch_size (int): The number of edges to write in each batch.\n\n    Returns:\n    -------\n        bool: The return value. True for success, False otherwise.\n\n    \"\"\"\n    # check if specified output format is correct\n    passed = self._is_rdf_format_supported(self.file_format)\n    if not passed:\n        logger.error(\"Error while writing edge data, wrong RDF format\")\n        return False\n    # write edge data using _write_edge_data method\n    passed = self._write_edge_data(edges, batch_size=batch_size)\n    if not passed:\n        logger.error(\"Error while writing edge data.\")\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/source/output-write/#biocypher.output.write.graph._rdf._RDFWriter.write_nodes","title":"<code>write_nodes(nodes, batch_size=int(1000000.0), force=False)</code>","text":"<p>Write nodes in RDF format.</p> <pre><code>nodes (list or generator): A list or generator of nodes in\n    BioCypherNode format.\nbatch_size (int): The number of nodes to write in each batch.\nforce (bool): Flag to force the writing even if the output file\n    already exists.\n</code></pre> <pre><code>bool: True if the writing is successful, False otherwise.\n</code></pre> Source code in <code>biocypher/output/write/graph/_rdf.py</code> <pre><code>def write_nodes(self, nodes, batch_size: int = int(1e6), force: bool = False) -&gt; bool:\n    \"\"\"Write nodes in RDF format.\n\n    Args:\n    ----\n        nodes (list or generator): A list or generator of nodes in\n            BioCypherNode format.\n        batch_size (int): The number of nodes to write in each batch.\n        force (bool): Flag to force the writing even if the output file\n            already exists.\n\n    Returns:\n    -------\n        bool: True if the writing is successful, False otherwise.\n\n    \"\"\"\n    # check if specified output format is correct\n    passed = self._is_rdf_format_supported(self.file_format)\n    if not passed:\n        logger.error(\"Error while writing node data, wrong RDF format\")\n        return False\n    # write node data using _write_node_data method\n    passed = self._write_node_data(nodes, batch_size, force)\n    if not passed:\n        logger.error(\"Error while writing node data.\")\n        return False\n    return True\n</code></pre>"},{"location":"reference/source/output-write/#networkx-writer","title":"NetworkX Writer","text":"<p>               Bases: <code>_Writer</code></p> <p>Class for writing the in-memory networkx DiGraph to file.</p> <p>Call <code>_construct_import_call</code> to write the networkx DiGraph to a pickle file and return the Python call to load it.</p> <p>TODO: this is a non-intuitive name, should be adjusted.</p> Source code in <code>biocypher/output/write/graph/_networkx.py</code> <pre><code>class _NetworkXWriter(_Writer):\n    \"\"\"\n    Class for writing the in-memory networkx DiGraph to file.\n\n    Call `_construct_import_call` to write the networkx DiGraph to a pickle\n    file and return the Python call to load it.\n\n    TODO: this is a non-intuitive name, should be adjusted.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.in_memory_networkx_kg = NetworkxKG(\n            deduplicator=self.deduplicator,\n        )\n\n    def _construct_import_call(self) -&gt; str:\n        \"\"\"Dump networkx graph to a pickle file and return Python call.\n\n        Returns:\n            str: Python code to load the networkx graph from a pickle file.\n        \"\"\"\n        self.G = self.in_memory_networkx_kg._create_networkx_kg()\n        logger.info(f\"Writing networkx {self.G} to pickle file networkx_graph.pkl.\")\n        with open(f\"{self.output_directory}/networkx_graph.pkl\", \"wb\") as f:\n            pickle.dump(self.G, f)\n\n        import_call = \"import pickle\\n\"\n        import_call += \"with open('./networkx_graph.pkl', 'rb') as f:\\n\\tG_loaded = pickle.load(f)\"\n        return import_call\n\n    def _get_import_script_name(self) -&gt; str:\n        \"\"\"Function to return the name of the import script.\"\"\"\n        return \"import_networkx.py\"\n\n    def _write_node_data(self, nodes) -&gt; bool:\n        \"\"\"Add nodes to the networkx graph.\n\n        TODO: this is not strictly writing, should be refactored.\n\n        Args:\n            nodes (list): List of nodes to add to the networkx graph.\n\n        Returns:\n            bool: True if the nodes were added successfully, False otherwise.\n        \"\"\"\n        passed = self.in_memory_networkx_kg.add_nodes(nodes)\n        return passed\n\n    def _write_edge_data(self, edges) -&gt; bool:\n        \"\"\"Add edges to the networkx graph.\n\n        TODO: this is not strictly writing, should be refactored.\n\n        Args:\n            edges (list): List of edges to add to the networkx graph.\n\n        Returns:\n            bool: True if the edges were added successfully, False otherwise.\n        \"\"\"\n        passed = self.in_memory_networkx_kg.add_edges(edges)\n        return passed\n</code></pre>"},{"location":"reference/source/output-write/#postgresql-batch-writer","title":"PostgreSQL Batch Writer","text":"<p>               Bases: <code>_BatchWriter</code></p> <p>Write node and edge representations for PostgreSQL.</p> <p>Class for writing node and edge representations to disk using the format specified by PostgreSQL for the use of \"COPY FROM...\". Each batch writer instance has a fixed representation that needs to be passed at instantiation via the attr:<code>schema</code> argument. The instance also expects an ontology adapter via attr:<code>ontology_adapter</code> to be able to convert and extend the hierarchy.</p> <p>This class inherits from the abstract class \"_BatchWriter\" and implements the PostgreSQL-specific methods:</p> <pre><code>- _write_node_headers\n- _write_edge_headers\n- _construct_import_call\n- _write_array_string\n</code></pre> Source code in <code>biocypher/output/write/relational/_postgresql.py</code> <pre><code>class _PostgreSQLBatchWriter(_BatchWriter):\n    \"\"\"Write node and edge representations for PostgreSQL.\n\n    Class for writing node and edge representations to disk using the\n    format specified by PostgreSQL for the use of \"COPY FROM...\". Each batch\n    writer instance has a fixed representation that needs to be passed\n    at instantiation via the :py:attr:`schema` argument. The instance\n    also expects an ontology adapter via :py:attr:`ontology_adapter` to be able\n    to convert and extend the hierarchy.\n\n    This class inherits from the abstract class \"_BatchWriter\" and implements the\n    PostgreSQL-specific methods:\n\n        - _write_node_headers\n        - _write_edge_headers\n        - _construct_import_call\n        - _write_array_string\n    \"\"\"\n\n    DATA_TYPE_LOOKUP = {\n        \"str\": \"VARCHAR\",  # VARCHAR needs limit\n        \"int\": \"INTEGER\",\n        \"long\": \"BIGINT\",\n        \"float\": \"NUMERIC\",\n        \"double\": \"NUMERIC\",\n        \"dbl\": \"NUMERIC\",\n        \"boolean\": \"BOOLEAN\",\n        \"str[]\": \"VARCHAR[]\",\n        \"string[]\": \"VARCHAR[]\",\n    }\n\n    def __init__(self, *args, **kwargs):\n        self._copy_from_csv_commands = set()\n        super().__init__(*args, **kwargs)\n\n    def _get_default_import_call_bin_prefix(self) -&gt; str:\n        \"\"\"Provide the default string for the import call bin prefix.\n\n        Returns\n        -------\n            str: The default location for the psql command\n\n        \"\"\"\n        return \"\"\n\n    def _get_data_type(self, string) -&gt; str:\n        try:\n            return self.DATA_TYPE_LOOKUP[string]\n        except KeyError:\n            logger.info('Could not determine data type {string}. Using default \"VARCHAR\"')\n            return \"VARCHAR\"\n\n    def _quote_string(self, value: str) -&gt; str:\n        \"\"\"Quote a string.\"\"\"\n        return f\"{self.quote}{value}{self.quote}\"\n\n    def _write_array_string(self, string_list) -&gt; str:\n        \"\"\"Write the string representation of an array into a .csv file.\n\n        Abstract method to output.write the string representation of an array\n        into a .csv file as required by the postgresql COPY command, with\n        '{','}' brackets and ',' separation.\n\n        Args:\n        ----\n            string_list (list): list of ontology strings\n\n        Returns:\n        -------\n            str: The string representation of an array for postgres COPY\n\n        \"\"\"\n        string = \",\".join(string_list)\n        string = f'\"{{{string}}}\"'\n        return string\n\n    def _get_import_script_name(self) -&gt; str:\n        \"\"\"Return the name of the psql import script.\n\n        Returns\n        -------\n            str: The name of the import script (ending in .sh)\n\n        \"\"\"\n        return f\"{self.db_name}-import-call.sh\"\n\n    def _adjust_pascal_to_psql(self, string):\n        string = string.replace(\".\", \"_\")\n        string = string.lower()\n        return string\n\n    def _write_node_headers(self) -&gt; bool:\n        \"\"\"Write node header files for PostgreSQL.\n\n        Writes single CSV file for a graph entity that is represented\n        as a node as per the definition in the `schema_config.yaml`,\n        containing only the header for this type of node.\n\n        Returns\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        \"\"\"\n        # load headers from data parse\n        if not self.node_property_dict:\n            logger.error(\n                \"Header information not found. Was the data parsed first?\",\n            )\n            return False\n\n        for label, props in self.node_property_dict.items():\n            # create header CSV with ID, properties, labels\n\n            # translate label to PascalCase\n            pascal_label = self.translator.name_sentence_to_pascal(label)\n\n            parts = f\"{pascal_label}-part*.csv\"\n            parts_paths = os.path.join(self.outdir, parts)\n            parts_paths = glob.glob(parts_paths)\n            parts_paths.sort()\n\n            # adjust label for import to psql\n            pascal_label = self._adjust_pascal_to_psql(pascal_label)\n            table_create_command_path = os.path.join(\n                self.outdir,\n                f\"{pascal_label}-create_table.sql\",\n            )\n\n            # check if file already exists\n            if os.path.exists(table_create_command_path):\n                logger.warning(\n                    f\"File {table_create_command_path} already exists. Overwriting.\",\n                )\n\n            # concatenate key:value in props\n            columns = [\"_ID VARCHAR\"]\n            for col_name, col_type in props.items():\n                col_type = self._get_data_type(col_type)\n                col_name = self._adjust_pascal_to_psql(col_name)\n                columns.append(f\"{col_name} {col_type}\")\n            columns.append(\"_LABEL VARCHAR[]\")\n\n            with open(table_create_command_path, \"w\", encoding=\"utf-8\") as f:\n                command = \"\"\n                if self.wipe:\n                    command += f\"DROP TABLE IF EXISTS {pascal_label};\\n\"\n\n                # table creation requires comma separation\n                command += f\"CREATE TABLE {pascal_label}({','.join(columns)});\\n\"\n                f.write(command)\n\n                for parts_path in parts_paths:\n                    # if import_call_file_prefix is set, replace actual path\n                    # with prefix\n                    if self.import_call_file_prefix != self.outdir:\n                        parts_path = parts_path.replace(\n                            self.outdir,\n                            self.import_call_file_prefix,\n                        )\n\n                    self._copy_from_csv_commands.add(\n                        f\"\\\\copy {pascal_label} FROM '{parts_path}' DELIMITER E'{self.delim}' CSV;\",\n                    )\n\n            # add file path to import statement\n            # if import_call_file_prefix is set, replace actual path\n            # with prefix\n            if self.import_call_file_prefix != self.outdir:\n                table_create_command_path = table_create_command_path.replace(\n                    self.outdir,\n                    self.import_call_file_prefix,\n                )\n\n            self.import_call_nodes.add(table_create_command_path)\n\n        return True\n\n    def _write_edge_headers(self):\n        \"\"\"Writes single CSV file for a graph entity that is represented\n        as an edge as per the definition in the `schema_config.yaml`,\n        containing only the header for this type of edge.\n\n        Returns\n        -------\n            bool: The return value. True for success, False otherwise.\n\n        \"\"\"\n        # load headers from data parse\n        if not self.edge_property_dict:\n            logger.error(\n                \"Header information not found. Was the data parsed first?\",\n            )\n            return False\n\n        for label, props in self.edge_property_dict.items():\n            # translate label to PascalCase\n            pascal_label = self.translator.name_sentence_to_pascal(label)\n\n            parts_paths = os.path.join(self.outdir, f\"{pascal_label}-part*.csv\")\n            parts_paths = glob.glob(parts_paths)\n            parts_paths.sort()\n\n            # adjust label for import to psql\n            pascal_label = self._adjust_pascal_to_psql(pascal_label)\n            table_create_command_path = os.path.join(\n                self.outdir,\n                f\"{pascal_label}-create_table.sql\",\n            )\n\n            # check for file exists\n            if os.path.exists(table_create_command_path):\n                logger.warning(\n                    f\"File {table_create_command_path} already exists. Overwriting.\",\n                )\n\n            # concatenate key:value in props\n            columns = []\n            for col_name, col_type in props.items():\n                col_type = self._get_data_type(col_type)\n                col_name = self._adjust_pascal_to_psql(col_name)\n                if col_name == \"_ID\":\n                    # should ideally never happen\n                    raise ValueError(\n                        \"Column name '_ID' is reserved for internal use, \"\n                        \"denoting the relationship ID. Please choose a \"\n                        \"different name for your column.\",\n                    )\n\n                columns.append(f\"{col_name} {col_type}\")\n\n            # create list of lists and flatten\n            # removes need for empty check of property list\n            out_list = [\n                \"_START_ID VARCHAR\",\n                \"_ID VARCHAR\",\n                *columns,\n                \"_END_ID VARCHAR\",\n                \"_TYPE VARCHAR\",\n            ]\n\n            with open(table_create_command_path, \"w\", encoding=\"utf-8\") as f:\n                command = \"\"\n                if self.wipe:\n                    command += f\"DROP TABLE IF EXISTS {pascal_label};\\n\"\n\n                # table creation requires comma separation\n                command += f\"CREATE TABLE {pascal_label}({','.join(out_list)});\\n\"\n                f.write(command)\n\n                for parts_path in parts_paths:\n                    # if import_call_file_prefix is set, replace actual path\n                    # with prefix\n                    if self.import_call_file_prefix != self.outdir:\n                        parts_path = parts_path.replace(\n                            self.outdir,\n                            self.import_call_file_prefix,\n                        )\n\n                    self._copy_from_csv_commands.add(\n                        f\"\\\\copy {pascal_label} FROM '{parts_path}' DELIMITER E'{self.delim}' CSV;\",\n                    )\n\n            # add file path to import statement\n            # if import_call_file_prefix is set, replace actual path\n            # with prefix\n            if self.import_call_file_prefix != self.outdir:\n                table_create_command_path = table_create_command_path.replace(\n                    self.outdir,\n                    self.import_call_file_prefix,\n                )\n\n            self.import_call_edges.add(table_create_command_path)\n\n        return True\n\n    def _construct_import_call(self) -&gt; str:\n        \"\"\"Function to construct the import call detailing folder and\n        individual node and edge headers and data files, as well as\n        delimiters and database name. Built after all data has been\n        processed to ensure that nodes are called before any edges.\n\n        Returns\n        -------\n            str: a bash command for postgresql import\n\n        \"\"\"\n        import_call = \"\"\n\n        # create tables\n        # At this point, csv files of nodes and edges do not require differentiation\n        for import_file_path in [\n            *self.import_call_nodes,\n            *self.import_call_edges,\n        ]:\n            import_call += f'echo \"Setup {import_file_path}...\"\\n'\n            if {self.db_password}:\n                # set password variable inline\n                import_call += f\"PGPASSWORD={self.db_password} \"\n            import_call += f\"{self.import_call_bin_prefix}psql -f {import_file_path}\"\n            import_call += f\" --dbname {self.db_name}\"\n            import_call += f\" --host {self.db_host}\"\n            import_call += f\" --port {self.db_port}\"\n            import_call += f\" --user {self.db_user}\"\n            import_call += '\\necho \"Done!\"\\n'\n            import_call += \"\\n\"\n\n        # copy data to tables\n        for command in self._copy_from_csv_commands:\n            table_part = command.split(\" \")[3]\n            import_call += f'echo \"Importing {table_part}...\"\\n'\n            if {self.db_password}:\n                # set password variable inline\n                import_call += f\"PGPASSWORD={self.db_password} \"\n            import_call += f'{self.import_call_bin_prefix}psql -c \"{command}\"'\n            import_call += f\" --dbname {self.db_name}\"\n            import_call += f\" --host {self.db_host}\"\n            import_call += f\" --port {self.db_port}\"\n            import_call += f\" --user {self.db_user}\"\n            import_call += '\\necho \"Done!\"\\n'\n            import_call += \"\\n\"\n\n        return import_call\n</code></pre>"},{"location":"reference/source/output-write/#sqlite-batch-writer","title":"SQLite Batch Writer","text":"<p>               Bases: <code>_PostgreSQLBatchWriter</code></p> <p>Class for writing node and edge representations to a SQLite database. It uses the _PostgreSQLBatchWriter class under the hood, which already implements the logic to write the nodes/edges to a relational DBMS. Only the import bash script differs between PostgreSQL and SQLite and is therefore implemented in this class.</p> <ul> <li>_construct_import_call</li> </ul> Source code in <code>biocypher/output/write/relational/_sqlite.py</code> <pre><code>class _SQLiteBatchWriter(_PostgreSQLBatchWriter):\n    \"\"\"\n    Class for writing node and edge representations to a SQLite database.\n    It uses the _PostgreSQLBatchWriter class under the hood, which already\n    implements the logic to write the nodes/edges to a relational DBMS.\n    Only the import bash script differs between PostgreSQL and SQLite\n    and is therefore implemented in this class.\n\n    - _construct_import_call\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def _construct_import_call(self) -&gt; str:\n        \"\"\"\n        Function to construct the import call detailing folder and\n        individual node and edge headers and data files, as well as\n        delimiters and database name. Built after all data has been\n        processed to ensure that nodes are called before any edges.\n\n        Returns:\n            str: a bash command for sqlite import\n        \"\"\"\n        import_call = \"\"\n\n        # create tables\n        # At this point, csv files of nodes and edges do not require differentiation\n        for import_file_path in [\n            *self.import_call_nodes,\n            *self.import_call_edges,\n        ]:\n            import_call += f'echo \"Setup {import_file_path}...\"\\n'\n            import_call += f\"{self.import_call_bin_prefix}sqlite3 {self.db_name} &lt; {import_file_path}\"\n            import_call += '\\necho \"Done!\"\\n'\n            import_call += \"\\n\"\n\n        for command in self._copy_from_csv_commands:\n            table_name = command.split(\" \")[1]\n            table_part = command.split(\" \")[3].replace(\"'\", \"\")\n            import_call += f'echo \"Importing {table_part}...\"\\n'\n            separator = self.delim\n            import_part = f\".import {table_part} {table_name}\"\n            import_call += (\n                f\"{self.import_call_bin_prefix}sqlite3 -separator $'{separator}' {self.db_name} \\\"{import_part}\\\"\"\n            )\n            import_call += '\\necho \"Done!\"\\n'\n            import_call += \"\\n\"\n\n        return import_call\n</code></pre>"},{"location":"reference/source/output-write/#pandas-csv-writer","title":"Pandas CSV Writer","text":"<p>               Bases: <code>_Writer</code></p> <p>Class for writing node and edge representations to CSV files.</p> Source code in <code>biocypher/output/write/relational/_csv.py</code> <pre><code>class _PandasCSVWriter(_Writer):\n    \"\"\"\n    Class for writing node and edge representations to CSV files.\n    \"\"\"\n\n    def __init__(self, *args, write_to_file: bool = True, **kwargs):\n        kwargs[\"write_to_file\"] = write_to_file\n        super().__init__(*args, **kwargs)\n        self.in_memory_dfs = {}\n        self.stored_dfs = {}\n        self.pandas_in_memory = PandasKG(\n            deduplicator=self.deduplicator,\n        )\n        self.delimiter = kwargs.get(\"delimiter\")\n        if not self.delimiter:\n            self.delimiter = \",\"\n        self.write_to_file = write_to_file\n\n    def _construct_import_call(self) -&gt; str:\n        \"\"\"Function to construct the Python code to load all node and edge csv files again into Pandas dfs.\n\n        Returns:\n            str: Python code to load the csv files into Pandas dfs.\n        \"\"\"\n        import_call = \"import pandas as pd\\n\\n\"\n        for df_name in self.stored_dfs.keys():\n            import_call += f\"{df_name} = pd.read_csv('./{df_name}.csv', header=0, index_col=0)\\n\"\n        return import_call\n\n    def _get_import_script_name(self) -&gt; str:\n        \"\"\"Function to return the name of the import script.\"\"\"\n        return \"import_pandas_csv.py\"\n\n    def _write_node_data(self, nodes) -&gt; bool:\n        passed = self._write_entities_to_file(nodes)\n        return passed\n\n    def _write_edge_data(self, edges) -&gt; bool:\n        passed = self._write_entities_to_file(edges)\n        return passed\n\n    def _write_entities_to_file(self, entities: iter) -&gt; bool:\n        \"\"\"Function to write the entities to a CSV file.\n\n        Args:\n            entities (iterable): An iterable of BioCypherNode / BioCypherEdge / BioCypherRelAsNode objects.\n        \"\"\"\n        entities = peekable(entities)\n        entity_list = self.pandas_in_memory._separate_entity_types(entities)\n        for entity_type, entities in entity_list.items():\n            self.in_memory_dfs[entity_type] = self.pandas_in_memory._add_entity_df(entity_type, entities)\n        for entity_type in self.in_memory_dfs.keys():\n            entity_df = self.in_memory_dfs[entity_type]\n            if \" \" in entity_type or \".\" in entity_type:\n                entity_type = entity_type.replace(\" \", \"_\").replace(\".\", \"_\")\n            if self.write_to_file:\n                logger.info(f\"Writing {entity_df.shape[0]} entries to {entity_type}.csv.\")\n                entity_df.to_csv(\n                    f\"{self.output_directory}/{entity_type}.csv\",\n                    sep=self.delimiter,\n                )\n            self.stored_dfs[entity_type] = entity_df\n        self.in_memory_dfs = {}\n        return True\n</code></pre>"},{"location":"reference/source/translation/","title":"Mapping and Translation","text":"<p>Class responsible for exacting the translation process.</p> <p>Translation is configured in the schema_config.yaml file. Creates a mapping dictionary from that file, and, given nodes and edges, translates them into BioCypherNodes and BioCypherEdges. During this process, can also filter the properties of the entities if the schema_config.yaml file specifies a property whitelist or blacklist.</p> <p>Provides utility functions for translating between input and output labels and cypher queries.</p> Source code in <code>biocypher/_translate.py</code> <pre><code>class Translator:\n    \"\"\"Class responsible for exacting the translation process.\n\n    Translation is configured in the schema_config.yaml file. Creates a mapping\n    dictionary from that file, and, given nodes and edges, translates them into\n    BioCypherNodes and BioCypherEdges. During this process, can also filter the\n    properties of the entities if the schema_config.yaml file specifies a property\n    whitelist or blacklist.\n\n    Provides utility functions for translating between input and output labels\n    and cypher queries.\n    \"\"\"\n\n    def __init__(self, ontology: \"Ontology\", strict_mode: bool = False):\n        \"\"\"Initialise the translator.\n\n        Args:\n        ----\n            ontology (Ontology): An Ontology object providing schema and mapping details.\n            strict_mode:\n                strict_mode (bool, optional): If True, enforces that every node and edge carries\n                the required 'source', 'licence', and 'version' properties. Raises ValueError\n                if these are missing. Defaults to False.\n\n\n        \"\"\"\n        self.ontology = ontology\n        self.strict_mode = strict_mode\n\n        # record nodes without biolink type configured in schema_config.yaml\n        self.notype = {}\n\n        # mapping functionality for translating terms and queries\n        self.mappings = {}\n        self.reverse_mappings = {}\n\n        self._update_ontology_types()\n\n    def translate_entities(self, entities):\n        entities = peekable(entities)\n        if isinstance(entities.peek(), BioCypherEdge | BioCypherNode | BioCypherRelAsNode):\n            translated_entities = entities\n        elif len(entities.peek()) &lt; 4:\n            translated_entities = self.translate_nodes(entities)\n        else:\n            translated_entities = self.translate_edges(entities)\n        return translated_entities\n\n    def translate_nodes(\n        self,\n        node_tuples: Iterable,\n    ) -&gt; Generator[BioCypherNode, None, None]:\n        \"\"\"Translate input node representation.\n\n        Translate the node tuples to a representation that conforms to the\n        schema of the given BioCypher graph. For now requires explicit\n        statement of node type on pass.\n\n        Args:\n        ----\n            node_tuples (list of tuples): collection of tuples\n                representing individual nodes by their unique id and a type\n                that is translated from the original database notation to\n                the corresponding BioCypher notation.\n\n        \"\"\"\n        self._log_begin_translate(node_tuples, \"nodes\")\n\n        for _id, _type, _props in node_tuples:\n            # check for strict mode requirements\n            required_props = [\"source\", \"licence\", \"version\"]\n\n            if self.strict_mode:\n                # rename 'license' to 'licence' in _props\n                if _props.get(\"license\"):\n                    _props[\"licence\"] = _props.pop(\"license\")\n\n                for prop in required_props:\n                    if prop not in _props:\n                        msg = (\n                            f\"Property `{prop}` missing from node {_id}. \"\n                            \"Strict mode is enabled, so this is not allowed.\",\n                        )\n                        logger.error(msg)\n                        raise ValueError(msg)\n\n            # find the node in leaves that represents ontology node type\n            _ontology_class = self._get_ontology_mapping(_type)\n\n            if _ontology_class:\n                # filter properties for those specified in schema_config if any\n                _filtered_props = self._filter_props(_ontology_class, _props)\n\n                # preferred id\n                _preferred_id = self._get_preferred_id(_ontology_class)\n\n                yield BioCypherNode(\n                    node_id=_id,\n                    node_label=_ontology_class,\n                    preferred_id=_preferred_id,\n                    properties=_filtered_props,\n                )\n\n            else:\n                self._record_no_type(_type, _id)\n\n        self._log_finish_translate(\"nodes\")\n\n    def _get_preferred_id(self, _bl_type: str) -&gt; str:\n        \"\"\"Return the preferred id for the given Biolink type.\n\n        If the preferred id is not specified in the schema_config.yaml file,\n        return \"id\".\n        \"\"\"\n        return (\n            self.ontology.mapping.extended_schema[_bl_type][\"preferred_id\"]\n            if \"preferred_id\" in self.ontology.mapping.extended_schema.get(_bl_type, {})\n            else \"id\"\n        )\n\n    def _filter_props(self, bl_type: str, props: dict) -&gt; dict:\n        \"\"\"Filter properties for those specified in schema_config if any.\n\n        If the properties are not specified in the schema_config.yaml file,\n        return the original properties.\n        \"\"\"\n        filter_props = self.ontology.mapping.extended_schema[bl_type].get(\"properties\", {})\n\n        # strict mode: add required properties (only if there is a whitelist)\n        if self.strict_mode and filter_props:\n            filter_props.update(\n                {\"source\": \"str\", \"licence\": \"str\", \"version\": \"str\"},\n            )\n\n        exclude_props = self.ontology.mapping.extended_schema[bl_type].get(\"exclude_properties\", [])\n\n        if isinstance(exclude_props, str):\n            exclude_props = [exclude_props]\n\n        if filter_props and exclude_props:\n            filtered_props = {k: v for k, v in props.items() if (k in filter_props.keys() and k not in exclude_props)}\n\n        elif filter_props:\n            filtered_props = {k: v for k, v in props.items() if k in filter_props.keys()}\n\n        elif exclude_props:\n            filtered_props = {k: v for k, v in props.items() if k not in exclude_props}\n\n        else:\n            return props\n\n        missing_props = [k for k in filter_props.keys() if k not in filtered_props.keys()]\n        # add missing properties with default values\n        for k in missing_props:\n            filtered_props[k] = None\n\n        return filtered_props\n\n    def translate_edges(\n        self,\n        edge_tuples: Iterable,\n    ) -&gt; Generator[BioCypherEdge | BioCypherRelAsNode, None, None]:\n        \"\"\"Translate input edge representation.\n\n        Translate the edge tuples to a representation that conforms to the\n        schema of the given BioCypher graph. For now requires explicit\n        statement of edge type on pass.\n\n        Args:\n        ----\n            edge_tuples (list of tuples):\n\n                collection of tuples representing source and target of\n                an interaction via their unique ids as well as the type\n                of interaction in the original database notation, which\n                is translated to BioCypher notation using the `leaves`.\n                Can optionally possess its own ID.\n\n        \"\"\"\n        self._log_begin_translate(edge_tuples, \"edges\")\n\n        # legacy: deal with 4-tuples (no edge id)\n        # TODO remove for performance reasons once safe\n        edge_tuples = peekable(edge_tuples)\n        if len(edge_tuples.peek()) == 4:\n            edge_tuples = [(None, src, tar, typ, props) for src, tar, typ, props in edge_tuples]\n\n        for _id, _src, _tar, _type, _props in edge_tuples:\n            # check for strict mode requirements\n            if self.strict_mode:\n                if \"source\" not in _props:\n                    msg = (\n                        f\"Edge {_id if _id else (_src, _tar)} does not have a `source` property.\"\n                        \" This is required in strict mode.\",\n                    )\n                    logger.error(msg)\n                    raise ValueError(msg)\n                if \"licence\" not in _props:\n                    msg = (\n                        f\"Edge {_id if _id else (_src, _tar)} does not have a `licence` property.\"\n                        \" This is required in strict mode.\",\n                    )\n                    logger.error(msg)\n                    raise ValueError(msg)\n\n            # match the input label (_type) to\n            # an ontology label from schema_config\n            bl_type = self._get_ontology_mapping(_type)\n\n            if bl_type:\n                # filter properties for those specified in schema_config if any\n                _filtered_props = self._filter_props(bl_type, _props)\n\n                rep = self.ontology.mapping.extended_schema[bl_type][\"represented_as\"]\n\n                if rep == \"node\":\n                    if _id:\n                        # if it brings its own ID, use it\n                        node_id = _id\n\n                    else:\n                        # source target concat\n                        node_id = str(_src) + \"_\" + str(_tar) + \"_\" + \"_\".join(str(v) for v in _filtered_props.values())\n\n                    n = BioCypherNode(\n                        node_id=node_id,\n                        node_label=bl_type,\n                        properties=_filtered_props,\n                    )\n\n                    # directionality check TODO generalise to account for\n                    # different descriptions of directionality or find a\n                    # more consistent solution for indicating directionality\n                    if _filtered_props.get(\"directed\") == True:  # noqa: E712 (seems to not work without '== True')\n                        l1 = \"IS_SOURCE_OF\"\n                        l2 = \"IS_TARGET_OF\"\n\n                    elif _filtered_props.get(\n                        \"src_role\",\n                    ) and _filtered_props.get(\"tar_role\"):\n                        l1 = _filtered_props.get(\"src_role\")\n                        l2 = _filtered_props.get(\"tar_role\")\n\n                    else:\n                        l1 = l2 = \"IS_PART_OF\"\n\n                    e_s = BioCypherEdge(\n                        source_id=_src,\n                        target_id=node_id,\n                        relationship_label=l1,\n                        # additional here\n                    )\n\n                    e_t = BioCypherEdge(\n                        source_id=_tar,\n                        target_id=node_id,\n                        relationship_label=l2,\n                        # additional here\n                    )\n\n                    yield BioCypherRelAsNode(n, e_s, e_t)\n\n                else:\n                    edge_label = self.ontology.mapping.extended_schema[bl_type].get(\"label_as_edge\")\n\n                    if edge_label is None:\n                        edge_label = bl_type\n\n                    yield BioCypherEdge(\n                        relationship_id=_id,\n                        source_id=_src,\n                        target_id=_tar,\n                        relationship_label=edge_label,\n                        properties=_filtered_props,\n                    )\n\n            else:\n                self._record_no_type(_type, (_src, _tar))\n\n        self._log_finish_translate(\"edges\")\n\n    def _record_no_type(self, _type: Any, what: Any) -&gt; None:\n        \"\"\"Record the type of a non-represented node or edge.\n\n        In case of an entity that is not represented in the schema_config,\n        record the type and the entity.\n        \"\"\"\n        logger.error(f\"No ontology type defined for `{_type}`: {what}\")\n\n        if self.notype.get(_type, None):\n            self.notype[_type] += 1\n\n        else:\n            self.notype[_type] = 1\n\n    def get_missing_biolink_types(self) -&gt; dict:\n        \"\"\"Return a dictionary of non-represented types.\n\n        The dictionary contains the type as the key and the number of\n        occurrences as the value.\n        \"\"\"\n        return self.notype\n\n    @staticmethod\n    def _log_begin_translate(_input: Iterable, what: str):\n        n = f\"{len(_input)} \" if hasattr(_input, \"__len__\") else \"\"\n\n        logger.debug(f\"Translating {n}{what} to BioCypher\")\n\n    @staticmethod\n    def _log_finish_translate(what: str):\n        logger.debug(f\"Finished translating {what} to BioCypher.\")\n\n    def _update_ontology_types(self):\n        \"\"\"Create a dictionary to translate from input to ontology labels.\n\n        If multiple input labels, creates mapping for each.\n        \"\"\"\n        self._ontology_mapping = {}\n\n        for key, value in self.ontology.mapping.extended_schema.items():\n            labels = value.get(\"input_label\") or value.get(\"label_in_input\")\n\n            if isinstance(labels, str):\n                self._ontology_mapping[labels] = key\n\n            elif isinstance(labels, list):\n                for label in labels:\n                    self._ontology_mapping[label] = key\n\n            if value.get(\"label_as_edge\"):\n                self._add_translation_mappings(labels, value[\"label_as_edge\"])\n\n            else:\n                self._add_translation_mappings(labels, key)\n\n    def _get_ontology_mapping(self, label: str) -&gt; str | None:\n        \"\"\"Find the ontology class for the given input type.\n\n        For each given input type (\"input_label\" or \"label_in_input\"), find the\n        corresponding ontology class in the leaves dictionary (from the\n        `schema_config.yam`).\n\n        Args:\n        ----\n            label:\n                The input type to find (`input_label` or `label_in_input` in\n                `schema_config.yaml`).\n\n        \"\"\"\n        # FIXME does not seem like a necessary function.\n        # commented out until behaviour of _update_bl_types is fixed\n        return self._ontology_mapping.get(label, None)\n\n    def translate_term(self, term):\n        \"\"\"Translate a single term.\"\"\"\n        return self.mappings.get(term, None)\n\n    def reverse_translate_term(self, term):\n        \"\"\"Reverse translate a single term.\"\"\"\n        return self.reverse_mappings.get(term, None)\n\n    def translate(self, query):\n        \"\"\"Translate a cypher query.\n\n        Only translates labels as of now.\n        \"\"\"\n        for key in self.mappings:\n            query = query.replace(\":\" + key, \":\" + self.mappings[key])\n        return query\n\n    def reverse_translate(self, query):\n        \"\"\"Reverse translate a cypher query.\n\n        Only translates labels as of now.\n        \"\"\"\n        for key in self.reverse_mappings:\n            a = \":\" + key + \")\"\n            b = \":\" + key + \"]\"\n            # TODO this conditional probably does not cover all cases\n            if a in query or b in query:\n                if isinstance(self.reverse_mappings[key], list):\n                    msg = (\n                        \"Reverse translation of multiple inputs not \"\n                        \"implemented yet. Many-to-one mappings are \"\n                        \"not reversible. \"\n                        f\"({key} -&gt; {self.reverse_mappings[key]})\",\n                    )\n                    logger.error(msg)\n                    raise NotImplementedError(msg)\n                else:\n                    query = query.replace(\n                        a,\n                        \":\" + self.reverse_mappings[key] + \")\",\n                    ).replace(b, \":\" + self.reverse_mappings[key] + \"]\")\n        return query\n\n    def _add_translation_mappings(self, original_name, biocypher_name):\n        \"\"\"Add translation mappings for a label and name.\n\n        We use here the PascalCase version of the BioCypher name, since\n        sentence case is not useful for Cypher queries.\n        \"\"\"\n        if isinstance(original_name, list):\n            for on in original_name:\n                self.mappings[on] = self.name_sentence_to_pascal(\n                    biocypher_name,\n                )\n        else:\n            self.mappings[original_name] = self.name_sentence_to_pascal(\n                biocypher_name,\n            )\n\n        if isinstance(biocypher_name, list):\n            for bn in biocypher_name:\n                self.reverse_mappings[\n                    self.name_sentence_to_pascal(\n                        bn,\n                    )\n                ] = original_name\n        else:\n            self.reverse_mappings[\n                self.name_sentence_to_pascal(\n                    biocypher_name,\n                )\n            ] = original_name\n\n    @staticmethod\n    def name_sentence_to_pascal(name: str) -&gt; str:\n        \"\"\"Convert a name in sentence case to pascal case.\"\"\"\n        # split on dots if dot is present\n        if \".\" in name:\n            return \".\".join(\n                [_misc.sentencecase_to_pascalcase(n) for n in name.split(\".\")],\n            )\n        else:\n            return _misc.sentencecase_to_pascalcase(name)\n</code></pre>"},{"location":"reference/source/translation/#biocypher._translate.Translator.__init__","title":"<code>__init__(ontology, strict_mode=False)</code>","text":"<p>Initialise the translator.</p> <pre><code>ontology (Ontology): An Ontology object providing schema and mapping details.\nstrict_mode:\n    strict_mode (bool, optional): If True, enforces that every node and edge carries\n    the required 'source', 'licence', and 'version' properties. Raises ValueError\n    if these are missing. Defaults to False.\n</code></pre> Source code in <code>biocypher/_translate.py</code> <pre><code>def __init__(self, ontology: \"Ontology\", strict_mode: bool = False):\n    \"\"\"Initialise the translator.\n\n    Args:\n    ----\n        ontology (Ontology): An Ontology object providing schema and mapping details.\n        strict_mode:\n            strict_mode (bool, optional): If True, enforces that every node and edge carries\n            the required 'source', 'licence', and 'version' properties. Raises ValueError\n            if these are missing. Defaults to False.\n\n\n    \"\"\"\n    self.ontology = ontology\n    self.strict_mode = strict_mode\n\n    # record nodes without biolink type configured in schema_config.yaml\n    self.notype = {}\n\n    # mapping functionality for translating terms and queries\n    self.mappings = {}\n    self.reverse_mappings = {}\n\n    self._update_ontology_types()\n</code></pre>"},{"location":"reference/source/translation/#biocypher._translate.Translator.get_missing_biolink_types","title":"<code>get_missing_biolink_types()</code>","text":"<p>Return a dictionary of non-represented types.</p> <p>The dictionary contains the type as the key and the number of occurrences as the value.</p> Source code in <code>biocypher/_translate.py</code> <pre><code>def get_missing_biolink_types(self) -&gt; dict:\n    \"\"\"Return a dictionary of non-represented types.\n\n    The dictionary contains the type as the key and the number of\n    occurrences as the value.\n    \"\"\"\n    return self.notype\n</code></pre>"},{"location":"reference/source/translation/#biocypher._translate.Translator.name_sentence_to_pascal","title":"<code>name_sentence_to_pascal(name)</code>  <code>staticmethod</code>","text":"<p>Convert a name in sentence case to pascal case.</p> Source code in <code>biocypher/_translate.py</code> <pre><code>@staticmethod\ndef name_sentence_to_pascal(name: str) -&gt; str:\n    \"\"\"Convert a name in sentence case to pascal case.\"\"\"\n    # split on dots if dot is present\n    if \".\" in name:\n        return \".\".join(\n            [_misc.sentencecase_to_pascalcase(n) for n in name.split(\".\")],\n        )\n    else:\n        return _misc.sentencecase_to_pascalcase(name)\n</code></pre>"},{"location":"reference/source/translation/#biocypher._translate.Translator.reverse_translate","title":"<code>reverse_translate(query)</code>","text":"<p>Reverse translate a cypher query.</p> <p>Only translates labels as of now.</p> Source code in <code>biocypher/_translate.py</code> <pre><code>def reverse_translate(self, query):\n    \"\"\"Reverse translate a cypher query.\n\n    Only translates labels as of now.\n    \"\"\"\n    for key in self.reverse_mappings:\n        a = \":\" + key + \")\"\n        b = \":\" + key + \"]\"\n        # TODO this conditional probably does not cover all cases\n        if a in query or b in query:\n            if isinstance(self.reverse_mappings[key], list):\n                msg = (\n                    \"Reverse translation of multiple inputs not \"\n                    \"implemented yet. Many-to-one mappings are \"\n                    \"not reversible. \"\n                    f\"({key} -&gt; {self.reverse_mappings[key]})\",\n                )\n                logger.error(msg)\n                raise NotImplementedError(msg)\n            else:\n                query = query.replace(\n                    a,\n                    \":\" + self.reverse_mappings[key] + \")\",\n                ).replace(b, \":\" + self.reverse_mappings[key] + \"]\")\n    return query\n</code></pre>"},{"location":"reference/source/translation/#biocypher._translate.Translator.reverse_translate_term","title":"<code>reverse_translate_term(term)</code>","text":"<p>Reverse translate a single term.</p> Source code in <code>biocypher/_translate.py</code> <pre><code>def reverse_translate_term(self, term):\n    \"\"\"Reverse translate a single term.\"\"\"\n    return self.reverse_mappings.get(term, None)\n</code></pre>"},{"location":"reference/source/translation/#biocypher._translate.Translator.translate","title":"<code>translate(query)</code>","text":"<p>Translate a cypher query.</p> <p>Only translates labels as of now.</p> Source code in <code>biocypher/_translate.py</code> <pre><code>def translate(self, query):\n    \"\"\"Translate a cypher query.\n\n    Only translates labels as of now.\n    \"\"\"\n    for key in self.mappings:\n        query = query.replace(\":\" + key, \":\" + self.mappings[key])\n    return query\n</code></pre>"},{"location":"reference/source/translation/#biocypher._translate.Translator.translate_edges","title":"<code>translate_edges(edge_tuples)</code>","text":"<p>Translate input edge representation.</p> <p>Translate the edge tuples to a representation that conforms to the schema of the given BioCypher graph. For now requires explicit statement of edge type on pass.</p> <pre><code>edge_tuples (list of tuples):\n\n    collection of tuples representing source and target of\n    an interaction via their unique ids as well as the type\n    of interaction in the original database notation, which\n    is translated to BioCypher notation using the `leaves`.\n    Can optionally possess its own ID.\n</code></pre> Source code in <code>biocypher/_translate.py</code> <pre><code>def translate_edges(\n    self,\n    edge_tuples: Iterable,\n) -&gt; Generator[BioCypherEdge | BioCypherRelAsNode, None, None]:\n    \"\"\"Translate input edge representation.\n\n    Translate the edge tuples to a representation that conforms to the\n    schema of the given BioCypher graph. For now requires explicit\n    statement of edge type on pass.\n\n    Args:\n    ----\n        edge_tuples (list of tuples):\n\n            collection of tuples representing source and target of\n            an interaction via their unique ids as well as the type\n            of interaction in the original database notation, which\n            is translated to BioCypher notation using the `leaves`.\n            Can optionally possess its own ID.\n\n    \"\"\"\n    self._log_begin_translate(edge_tuples, \"edges\")\n\n    # legacy: deal with 4-tuples (no edge id)\n    # TODO remove for performance reasons once safe\n    edge_tuples = peekable(edge_tuples)\n    if len(edge_tuples.peek()) == 4:\n        edge_tuples = [(None, src, tar, typ, props) for src, tar, typ, props in edge_tuples]\n\n    for _id, _src, _tar, _type, _props in edge_tuples:\n        # check for strict mode requirements\n        if self.strict_mode:\n            if \"source\" not in _props:\n                msg = (\n                    f\"Edge {_id if _id else (_src, _tar)} does not have a `source` property.\"\n                    \" This is required in strict mode.\",\n                )\n                logger.error(msg)\n                raise ValueError(msg)\n            if \"licence\" not in _props:\n                msg = (\n                    f\"Edge {_id if _id else (_src, _tar)} does not have a `licence` property.\"\n                    \" This is required in strict mode.\",\n                )\n                logger.error(msg)\n                raise ValueError(msg)\n\n        # match the input label (_type) to\n        # an ontology label from schema_config\n        bl_type = self._get_ontology_mapping(_type)\n\n        if bl_type:\n            # filter properties for those specified in schema_config if any\n            _filtered_props = self._filter_props(bl_type, _props)\n\n            rep = self.ontology.mapping.extended_schema[bl_type][\"represented_as\"]\n\n            if rep == \"node\":\n                if _id:\n                    # if it brings its own ID, use it\n                    node_id = _id\n\n                else:\n                    # source target concat\n                    node_id = str(_src) + \"_\" + str(_tar) + \"_\" + \"_\".join(str(v) for v in _filtered_props.values())\n\n                n = BioCypherNode(\n                    node_id=node_id,\n                    node_label=bl_type,\n                    properties=_filtered_props,\n                )\n\n                # directionality check TODO generalise to account for\n                # different descriptions of directionality or find a\n                # more consistent solution for indicating directionality\n                if _filtered_props.get(\"directed\") == True:  # noqa: E712 (seems to not work without '== True')\n                    l1 = \"IS_SOURCE_OF\"\n                    l2 = \"IS_TARGET_OF\"\n\n                elif _filtered_props.get(\n                    \"src_role\",\n                ) and _filtered_props.get(\"tar_role\"):\n                    l1 = _filtered_props.get(\"src_role\")\n                    l2 = _filtered_props.get(\"tar_role\")\n\n                else:\n                    l1 = l2 = \"IS_PART_OF\"\n\n                e_s = BioCypherEdge(\n                    source_id=_src,\n                    target_id=node_id,\n                    relationship_label=l1,\n                    # additional here\n                )\n\n                e_t = BioCypherEdge(\n                    source_id=_tar,\n                    target_id=node_id,\n                    relationship_label=l2,\n                    # additional here\n                )\n\n                yield BioCypherRelAsNode(n, e_s, e_t)\n\n            else:\n                edge_label = self.ontology.mapping.extended_schema[bl_type].get(\"label_as_edge\")\n\n                if edge_label is None:\n                    edge_label = bl_type\n\n                yield BioCypherEdge(\n                    relationship_id=_id,\n                    source_id=_src,\n                    target_id=_tar,\n                    relationship_label=edge_label,\n                    properties=_filtered_props,\n                )\n\n        else:\n            self._record_no_type(_type, (_src, _tar))\n\n    self._log_finish_translate(\"edges\")\n</code></pre>"},{"location":"reference/source/translation/#biocypher._translate.Translator.translate_nodes","title":"<code>translate_nodes(node_tuples)</code>","text":"<p>Translate input node representation.</p> <p>Translate the node tuples to a representation that conforms to the schema of the given BioCypher graph. For now requires explicit statement of node type on pass.</p> <pre><code>node_tuples (list of tuples): collection of tuples\n    representing individual nodes by their unique id and a type\n    that is translated from the original database notation to\n    the corresponding BioCypher notation.\n</code></pre> Source code in <code>biocypher/_translate.py</code> <pre><code>def translate_nodes(\n    self,\n    node_tuples: Iterable,\n) -&gt; Generator[BioCypherNode, None, None]:\n    \"\"\"Translate input node representation.\n\n    Translate the node tuples to a representation that conforms to the\n    schema of the given BioCypher graph. For now requires explicit\n    statement of node type on pass.\n\n    Args:\n    ----\n        node_tuples (list of tuples): collection of tuples\n            representing individual nodes by their unique id and a type\n            that is translated from the original database notation to\n            the corresponding BioCypher notation.\n\n    \"\"\"\n    self._log_begin_translate(node_tuples, \"nodes\")\n\n    for _id, _type, _props in node_tuples:\n        # check for strict mode requirements\n        required_props = [\"source\", \"licence\", \"version\"]\n\n        if self.strict_mode:\n            # rename 'license' to 'licence' in _props\n            if _props.get(\"license\"):\n                _props[\"licence\"] = _props.pop(\"license\")\n\n            for prop in required_props:\n                if prop not in _props:\n                    msg = (\n                        f\"Property `{prop}` missing from node {_id}. \"\n                        \"Strict mode is enabled, so this is not allowed.\",\n                    )\n                    logger.error(msg)\n                    raise ValueError(msg)\n\n        # find the node in leaves that represents ontology node type\n        _ontology_class = self._get_ontology_mapping(_type)\n\n        if _ontology_class:\n            # filter properties for those specified in schema_config if any\n            _filtered_props = self._filter_props(_ontology_class, _props)\n\n            # preferred id\n            _preferred_id = self._get_preferred_id(_ontology_class)\n\n            yield BioCypherNode(\n                node_id=_id,\n                node_label=_ontology_class,\n                preferred_id=_preferred_id,\n                properties=_filtered_props,\n            )\n\n        else:\n            self._record_no_type(_type, _id)\n\n    self._log_finish_translate(\"nodes\")\n</code></pre>"},{"location":"reference/source/translation/#biocypher._translate.Translator.translate_term","title":"<code>translate_term(term)</code>","text":"<p>Translate a single term.</p> Source code in <code>biocypher/_translate.py</code> <pre><code>def translate_term(self, term):\n    \"\"\"Translate a single term.\"\"\"\n    return self.mappings.get(term, None)\n</code></pre>"},{"location":"reference/source/utils/","title":"Miscellaneous Utility Functions","text":"<p>Ensure that <code>value</code> is a list.</p> Source code in <code>biocypher/_misc.py</code> <pre><code>def to_list(value: Any) -&gt; list:\n    \"\"\"Ensure that ``value`` is a list.\"\"\"\n    if isinstance(value, LIST_LIKE):\n        value = list(value)\n\n    else:\n        value = [value]\n\n    return value\n</code></pre> <p>Return iterables, except strings, wrap simple types into tuple.</p> Source code in <code>biocypher/_misc.py</code> <pre><code>def ensure_iterable(value: Any) -&gt; Iterable:\n    \"\"\"Return iterables, except strings, wrap simple types into tuple.\"\"\"\n    return value if isinstance(value, LIST_LIKE) else (value,)\n</code></pre> <p>Create a visualisation of the inheritance tree using treelib.</p> Source code in <code>biocypher/_misc.py</code> <pre><code>def create_tree_visualisation(inheritance_graph: dict | nx.Graph) -&gt; Tree:\n    \"\"\"Create a visualisation of the inheritance tree using treelib.\"\"\"\n    inheritance_tree = _get_inheritance_tree(inheritance_graph)\n    classes, root = _find_root_node(inheritance_tree)\n\n    tree = Tree()\n    tree.create_node(root, root)\n    while classes:\n        for child in classes:\n            parent = inheritance_tree[child]\n            if parent in tree.nodes.keys() or parent == root:\n                tree.create_node(child, child, parent=parent)\n\n        for node in tree.nodes.keys():\n            if node in classes:\n                classes.remove(node)\n\n    return tree\n</code></pre> Source code in <code>biocypher/_misc.py</code> <pre><code>def from_pascal(s: str, sep: str = \" \") -&gt; str:\n    underscored = underscore_pattern.sub(sep, s)\n    lowercased = lowercase_pattern.sub(\n        lambda match: match.group(0).lower(),\n        underscored,\n    )\n    return lowercased\n</code></pre> <p>Convert PascalCase to sentence case.</p> <pre><code>s: Input string in PascalCase\n</code></pre> <pre><code>string in sentence case form\n</code></pre> Source code in <code>biocypher/_misc.py</code> <pre><code>def pascalcase_to_sentencecase(s: str) -&gt; str:\n    \"\"\"Convert PascalCase to sentence case.\n\n    Args:\n    ----\n        s: Input string in PascalCase\n\n    Returns:\n    -------\n        string in sentence case form\n\n    \"\"\"\n    return from_pascal(s, sep=\" \")\n</code></pre> <p>Convert snake_case to sentence case.</p> <pre><code>s: Input string in snake_case\n</code></pre> <pre><code>string in sentence case form\n</code></pre> Source code in <code>biocypher/_misc.py</code> <pre><code>def snakecase_to_sentencecase(s: str) -&gt; str:\n    \"\"\"Convert snake_case to sentence case.\n\n    Args:\n    ----\n        s: Input string in snake_case\n\n    Returns:\n    -------\n        string in sentence case form\n\n    \"\"\"\n    return \" \".join(word.lower() for word in s.split(\"_\"))\n</code></pre> <p>Convert sentence case to snake_case.</p> <pre><code>s: Input string in sentence case\n</code></pre> <pre><code>string in snake_case form\n</code></pre> Source code in <code>biocypher/_misc.py</code> <pre><code>def sentencecase_to_snakecase(s: str) -&gt; str:\n    \"\"\"Convert sentence case to snake_case.\n\n    Args:\n    ----\n        s: Input string in sentence case\n\n    Returns:\n    -------\n        string in snake_case form\n\n    \"\"\"\n    return \"_\".join(s.lower().split())\n</code></pre> <p>Convert sentence case to PascalCase.</p> <pre><code>s: Input string in sentence case\nsep: Separator for the words in the input string\n</code></pre> <pre><code>string in PascalCase form\n</code></pre> Source code in <code>biocypher/_misc.py</code> <pre><code>def sentencecase_to_pascalcase(s: str, sep: str = r\"\\s\") -&gt; str:\n    \"\"\"Convert sentence case to PascalCase.\n\n    Args:\n    ----\n        s: Input string in sentence case\n        sep: Separator for the words in the input string\n\n    Returns:\n    -------\n        string in PascalCase form\n\n    \"\"\"\n    return re.sub(\n        r\"(?:^|[\" + sep + \"])([a-zA-Z])\",\n        lambda match: match.group(1).upper(),\n        s,\n    )\n</code></pre>"}]}